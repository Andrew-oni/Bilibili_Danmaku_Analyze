{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-09-06T17:20:12.763227Z",
     "end_time": "2025-09-06T17:21:05.031422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "B站弹幕批量抓取（按 url.txt 列表；timestamp=视频发布日期）\n",
      "======================================================================\n",
      "\n",
      "=== 处理 BV1iR8MzkEpc ===\n",
      "[META] 标题: 问界/理想/小米/极氪/特斯拉 城市事故炼狱 26车都谁不能回家？ | 发布日期: 2025-07-24\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 问界/理想/小米/极氪/特斯拉 城市事故炼狱 26车都谁不能回家？ (cid=31247631170) …\n",
      "    问界/理想/小米/极氪/特斯拉 城市事故炼狱 26车都谁不能回家？ 抓到 28839 条\n",
      "[ERROR] 处理 BV1iR8MzkEpc 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1iR8MzkEpc_31247631170_问界/理想/小米/极氪/特斯拉 城市事故炼狱 26车都谁不能回家？.csv'\n",
      "\n",
      "=== 处理 BV1vz8FzDEyE ===\n",
      "[META] 标题: 全球首次 问界/理想/小米/特斯拉 36辆辅助驾驶高速事故搏命 你敢把命交给车吗？ | 发布日期: 2025-07-23\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 全球首次 问界/理想/小米/特斯拉 36辆辅助驾驶高速事故搏命 你敢把命交给车吗？ (cid=31226333587) …\n",
      "    全球首次 问界/理想/小米/特斯拉 36辆辅助驾驶高速事故搏命 你敢把命交给车吗？ 抓到 15717 条\n",
      "[ERROR] 处理 BV1vz8FzDEyE 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1vz8FzDEyE_31226333587_全球首次 问界/理想/小米/特斯拉 36辆辅助驾驶高速事故搏命 你敢把命交给车吗？.csv'\n",
      "\n",
      "=== 处理 BV19T8wznEHB ===\n",
      "[META] 标题: 复现高速施工驾驶辅助事故 问界/小米/理想等 谁会选择撞卡车自断A柱？ | 发布日期: 2025-07-23\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 复现高速施工驾驶辅助事故 问界/小米/理想等 谁会选择撞卡车自断A柱？ (cid=31234132454) …\n",
      "    复现高速施工驾驶辅助事故 问界/小米/理想等 谁会选择撞卡车自断A柱？ 抓到 18037 条\n",
      "[ERROR] 处理 BV19T8wznEHB 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV19T8wznEHB_31234132454_复现高速施工驾驶辅助事故 问界/小米/理想等 谁会选择撞卡车自断A柱？.csv'\n",
      "\n",
      "=== 处理 BV1tftdztEs9 ===\n",
      "[META] 标题: 小米YU7的1000万clips辅助驾驶，成长到什么地步了？ | 发布日期: 2025-08-12\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 小米YU7的1000万clips辅助驾驶，成长到什么地步了？ (cid=31796496131) …\n",
      "    小米YU7的1000万clips辅助驾驶，成长到什么地步了？ 抓到 1298 条\n",
      "[OK] 单视频合计 1298 条 → combined_raw_BV1tftdztEs9_202509061720.csv / combined_raw_BV1tftdztEs9_202509061720.txt\n",
      "\n",
      "=== 处理 BV1jkbDzZEbo ===\n",
      "[META] 标题: 高速挑战升级  问界/理想/小米/特斯拉 能否在绝境中获取生机？ | 发布日期: 2025-07-25\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 高速挑战升级  问界/理想/小米/特斯拉 能否在绝境中获取生机？ (cid=31265065994) …\n",
      "    高速挑战升级  问界/理想/小米/特斯拉 能否在绝境中获取生机？ 抓到 9204 条\n",
      "[ERROR] 处理 BV1jkbDzZEbo 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1jkbDzZEbo_31265065994_高速挑战升级  问界/理想/小米/特斯拉 能否在绝境中获取生机？.csv'\n",
      "\n",
      "=== 处理 BV1bK4y1P7Ba ===\n",
      "[META] 标题: 干翻特斯拉？百度Apollo自动驾驶辅助体验 | 发布日期: 2021-04-21\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 成片v2 (cid=327215289) …\n",
      "    成片v2 抓到 7777 条\n",
      "[OK] 单视频合计 7777 条 → combined_raw_BV1bK4y1P7Ba_202509061720.csv / combined_raw_BV1bK4y1P7Ba_202509061720.txt\n",
      "\n",
      "=== 处理 BV1xP8AzGEjc ===\n",
      "[META] 标题: “直到碰撞前最后一秒，我都是相信它的”，“智驾”事故的背后，有问题吗？ | 发布日期: 2025-07-24\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 “直到碰撞前最后一秒，我都是相信它的”，“智驾”事故的背后，有问题吗？ (cid=31231970391) …\n",
      "    “直到碰撞前最后一秒，我都是相信它的”，“智驾”事故的背后，有问题吗？ 抓到 6488 条\n",
      "[OK] 单视频合计 6488 条 → combined_raw_BV1xP8AzGEjc_202509061720.csv / combined_raw_BV1xP8AzGEjc_202509061720.txt\n",
      "\n",
      "=== 处理 BV1Xv4YeGED3 ===\n",
      "[META] 标题: 飞机工程师，帮我的帝豪改自动驾驶？ | 发布日期: 2024-09-11\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 飞机工程师，帮我的帝豪改自动驾驶？ (cid=25830033977) …\n",
      "    飞机工程师，帮我的帝豪改自动驾驶？ 抓到 7376 条\n",
      "[OK] 单视频合计 7376 条 → combined_raw_BV1Xv4YeGED3_202509061720.csv / combined_raw_BV1Xv4YeGED3_202509061720.txt\n",
      "\n",
      "=== 处理 BV1Sc9sYoEvG ===\n",
      "[META] 标题: 小伙开自动驾驶后放心睡着，结果撞上收费站翻滚2圈！【1671期】 | 发布日期: 2025-03-05\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 小伙开自动驾驶后放心睡着，结果撞上收费站翻滚2圈！【1671期】 (cid=28696579943) …\n",
      "    小伙开自动驾驶后放心睡着，结果撞上收费站翻滚2圈！【1671期】 抓到 6275 条\n",
      "[OK] 单视频合计 6275 条 → combined_raw_BV1Sc9sYoEvG_202509061720.csv / combined_raw_BV1Sc9sYoEvG_202509061720.txt\n",
      "\n",
      "=== 处理 BV1rb4y1R7Wd ===\n",
      "[META] 标题: 你真的会开特斯拉吗？特斯拉失控车评人不会告诉你的秘密【刀哥谈车】 | 发布日期: 2021-02-21\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 Produce_6 (cid=300739050) …\n",
      "    Produce_6 抓到 651 条\n",
      "[OK] 单视频合计 651 条 → combined_raw_BV1rb4y1R7Wd_202509061720.csv / combined_raw_BV1rb4y1R7Wd_202509061720.txt\n",
      "\n",
      "=== 处理 BV1D14y1r73p ===\n",
      "[META] 标题: 华为：比智驾开城是吧，年底我直接全国可用【问界 M7 智驾版】 | 发布日期: 2023-09-12\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 华为：比智驾开城是吧，年底我直接全国可用【问界 M7 智驾版】 (cid=1265055590) …\n",
      "    华为：比智驾开城是吧，年底我直接全国可用【问界 M7 智驾版】 抓到 3774 条\n",
      "[OK] 单视频合计 3774 条 → combined_raw_BV1D14y1r73p_202509061720.csv / combined_raw_BV1D14y1r73p_202509061720.txt\n",
      "\n",
      "=== 处理 BV1BTb1zjEps ===\n",
      "[META] 标题: 问界/理想/小米/极氪/特斯拉 面对高速施工 36车险些全军覆没？ | 发布日期: 2025-07-25\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 问界/理想/小米/极氪/特斯拉 面对高速施工 36车险些全军覆没？ (cid=31257460826) …\n",
      "    问界/理想/小米/极氪/特斯拉 面对高速施工 36车险些全军覆没？ 抓到 4268 条\n",
      "[ERROR] 处理 BV1BTb1zjEps 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1BTb1zjEps_31257460826_问界/理想/小米/极氪/特斯拉 面对高速施工 36车险些全军覆没？.csv'\n",
      "\n",
      "=== 处理 BV1fK4y1g71C ===\n",
      "[META] 标题: 为什么造车新势力们的领航辅助一个都没法用？ | 发布日期: 2021-06-03\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 为什么造车新势力们的领航辅助一个都没法用？ (cid=348049570) …\n",
      "    为什么造车新势力们的领航辅助一个都没法用？ 抓到 3814 条\n",
      "[OK] 单视频合计 3814 条 → combined_raw_BV1fK4y1g71C_202509061720.csv / combined_raw_BV1fK4y1g71C_202509061720.txt\n",
      "\n",
      "=== 处理 BV1JT411k77P ===\n",
      "[META] 标题: 易车横评 蔚来ES7/小鹏G9/阿维塔11横评！辅助驾驶神仙打架？全员续航打对折？ | 发布日期: 2023-03-17\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 易车横评 蔚来ES7/小鹏G9/阿维塔11横评！辅助驾驶神仙打架？全员续航打对折？ (cid=1055606894) …\n",
      "    易车横评 蔚来ES7/小鹏G9/阿维塔11横评！辅助驾驶神仙打架？全员续航打对折？ 抓到 3645 条\n",
      "[ERROR] 处理 BV1JT411k77P 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1JT411k77P_1055606894_易车横评 蔚来ES7/小鹏G9/阿维塔11横评！辅助驾驶神仙打架？全员续航打对折？.csv'\n",
      "\n",
      "=== 处理 BV191421m7zx ===\n",
      "[META] 标题: 骗过车机！纯视觉的特斯拉，眼神真的不太好！我自己是不敢用AP了 | 发布日期: 2024-04-08\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 骗过车机！纯视觉的特斯拉，眼神真的不太好！我自己是不敢用AP了 (cid=1498402504) …\n",
      "    骗过车机！纯视觉的特斯拉，眼神真的不太好！我自己是不敢用AP了 抓到 3592 条\n",
      "[OK] 单视频合计 3592 条 → combined_raw_BV191421m7zx_202509061720.csv / combined_raw_BV191421m7zx_202509061720.txt\n",
      "\n",
      "=== 处理 BV1xX4y1H7F3 ===\n",
      "[META] 标题: “华为”造车，遥遥领先？全方位实测辅助驾驶+AEB：问界M5 PK 小鹏G6 | 发布日期: 2023-07-06\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 “华为”造车，遥遥领先？全方位实测辅助驾驶+AEB：问界M5 PK 小鹏G6 (cid=1187210517) …\n",
      "    “华为”造车，遥遥领先？全方位实测辅助驾驶+AEB：问界M5 PK 小鹏G6 抓到 3509 条\n",
      "[OK] 单视频合计 3509 条 → combined_raw_BV1xX4y1H7F3_202509061720.csv / combined_raw_BV1xX4y1H7F3_202509061720.txt\n",
      "\n",
      "=== 处理 BV1Wg411k7Tp ===\n",
      "[META] 标题: 915期：小鹏P7高架80速失控撞死修车人员，驾驶员：我开启了辅助驾驶。 | 发布日期: 2022-08-13\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 序列 01_1 (cid=800361433) …\n",
      "    序列 01_1 抓到 2660 条\n",
      "[OK] 单视频合计 2660 条 → combined_raw_BV1Wg411k7Tp_202509061720.csv / combined_raw_BV1Wg411k7Tp_202509061720.txt\n",
      "\n",
      "=== 处理 BV1gD4y1g7iC ===\n",
      "[META] 标题: 【42Mark】我花几万块买的辅助驾驶能让我上下班通勤更轻松吗？ | 发布日期: 2023-03-03\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 【42Mark】我花几万块买的辅助驾驶能让我上下班通勤更轻松吗？ (cid=1036115057) …\n",
      "    【42Mark】我花几万块买的辅助驾驶能让我上下班通勤更轻松吗？ 抓到 2245 条\n",
      "[OK] 单视频合计 2245 条 → combined_raw_BV1gD4y1g7iC_202509061720.csv / combined_raw_BV1gD4y1g7iC_202509061720.txt\n",
      "\n",
      "=== 处理 BV1Y54y1o7LC ===\n",
      "[META] 标题: 【42Mark】一年过去了，导航辅助驾驶发展到什么水平了？（上） | 发布日期: 2022-05-26\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 42Mark 导航辅助驾驶上 4k (cid=729887454) …\n",
      "    42Mark 导航辅助驾驶上 4k 抓到 2271 条\n",
      "[OK] 单视频合计 2271 条 → combined_raw_BV1Y54y1o7LC_202509061720.csv / combined_raw_BV1Y54y1o7LC_202509061720.txt\n",
      "\n",
      "=== 处理 BV16y411i7kQ ===\n",
      "[META] 标题: 没有华为 ADS 3.0 不能开的路？！享界 S9 暴雨天辅助驾驶极限测试！ | 发布日期: 2024-08-02\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 没有华为 ADS 3.0 不能开的路？！享界 S9 暴雨天辅助驾驶极限测试！ (cid=1635069917) …\n",
      "    没有华为 ADS 3.0 不能开的路？！享界 S9 暴雨天辅助驾驶极限测试！ 抓到 1800 条\n",
      "[OK] 单视频合计 1800 条 → combined_raw_BV16y411i7kQ_202509061720.csv / combined_raw_BV16y411i7kQ_202509061720.txt\n",
      "\n",
      "=== 处理 BV1GB4y1e7LJ ===\n",
      "[META] 标题: 极狐阿尔法 S Hi 版辅助驾驶体验！华为又创造了一个天花板？！ | 发布日期: 2022-07-23\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 极狐阿尔法 S Hi 版辅助驾驶体验！华为又创造了一个天花板？！ (cid=781950203) …\n",
      "    极狐阿尔法 S Hi 版辅助驾驶体验！华为又创造了一个天花板？！ 抓到 1352 条\n",
      "[OK] 单视频合计 1352 条 → combined_raw_BV1GB4y1e7LJ_202509061720.csv / combined_raw_BV1GB4y1e7LJ_202509061720.txt\n",
      "\n",
      "=== 处理 BV1VQ5EzhEcm ===\n",
      "[META] 标题: 全网首发！8大热门车，100km城区辅助智驾横评，华为、特斯拉、小米、理想谁更靠谱？ | 发布日期: 2025-04-18\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 全网首发！8大热门车，100km城区辅助智驾横评，华为、特斯拉、小米、理想谁更靠谱？ (cid=29485828966) …\n",
      "    全网首发！8大热门车，100km城区辅助智驾横评，华为、特斯拉、小米、理想谁更靠谱？ 抓到 2179 条\n",
      "[OK] 单视频合计 2179 条 → combined_raw_BV1VQ5EzhEcm_202509061720.csv / combined_raw_BV1VQ5EzhEcm_202509061720.txt\n",
      "\n",
      "=== 处理 BV18Yt1zvEqu ===\n",
      "[META] 标题: 辅助驾驶大乱斗！纯视觉和激光雷达，谁更老司机？ | 发布日期: 2025-08-08\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 辅助驾驶大乱斗！纯视觉和激光雷达，谁更老司机？ (cid=31567449077) …\n",
      "    辅助驾驶大乱斗！纯视觉和激光雷达，谁更老司机？ 抓到 1269 条\n",
      "[OK] 单视频合计 1269 条 → combined_raw_BV18Yt1zvEqu_202509061720.csv / combined_raw_BV18Yt1zvEqu_202509061720.txt\n",
      "\n",
      "=== 处理 BV1hK4y1k7nx ===\n",
      "[META] 标题: 最强辅助驾驶之战！宝马特斯拉蔚来理想，电动车还是燃油车更胜一筹？ | 发布日期: 2020-04-29\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 ADAS 横评v4 (cid=184580836) …\n",
      "    ADAS 横评v4 抓到 2068 条\n",
      "[OK] 单视频合计 2068 条 → combined_raw_BV1hK4y1k7nx_202509061720.csv / combined_raw_BV1hK4y1k7nx_202509061720.txt\n",
      "\n",
      "=== 处理 BV1TNPpeCEQX ===\n",
      "[META] 标题: 特斯拉 FSD 进入中国，中美智驾大对决 | 发布日期: 2025-02-27\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 特斯拉 FSD 进入中国，中美智驾大对决 (cid=28598405743) …\n",
      "    特斯拉 FSD 进入中国，中美智驾大对决 抓到 1852 条\n",
      "[OK] 单视频合计 1852 条 → combined_raw_BV1TNPpeCEQX_202509061720.csv / combined_raw_BV1TNPpeCEQX_202509061720.txt\n",
      "\n",
      "=== 处理 BV1KuP3e3E5M ===\n",
      "[META] 标题: 特斯拉 FSD 上线首日，上海街头夜考，到底好不好使？ | 发布日期: 2025-02-26\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 特斯拉 FSD 上线首日，上海街头夜考，到底好不好使？ (cid=28578351159) …\n",
      "    特斯拉 FSD 上线首日，上海街头夜考，到底好不好使？ 抓到 1529 条\n",
      "[OK] 单视频合计 1529 条 → combined_raw_BV1KuP3e3E5M_202509061720.csv / combined_raw_BV1KuP3e3E5M_202509061720.txt\n",
      "\n",
      "=== 处理 BV1GD5DzHEJo ===\n",
      "[META] 标题: 2025年了，华为辅助驾驶有什么槽点？ | 发布日期: 2025-04-23\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 2025年了，华为辅助驾驶有什么槽点？ (cid=29546709885) …\n",
      "    2025年了，华为辅助驾驶有什么槽点？ 抓到 1193 条\n",
      "[OK] 单视频合计 1193 条 → combined_raw_BV1GD5DzHEJo_202509061720.csv / combined_raw_BV1GD5DzHEJo_202509061720.txt\n",
      "\n",
      "=== 处理 BV1NP411h7hm ===\n",
      "[META] 标题: 你是想取代驾驶员吗？体验问界新M7大五座  华为无图智驾上车！ | 发布日期: 2023-09-12\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 你是想取代驾驶员吗？体验问界新M7大五座  华为无图智驾上车！ (cid=1264963436) …\n",
      "    你是想取代驾驶员吗？体验问界新M7大五座  华为无图智驾上车！ 抓到 1176 条\n",
      "[OK] 单视频合计 1176 条 → combined_raw_BV1NP411h7hm_202509061720.csv / combined_raw_BV1NP411h7hm_202509061720.txt\n",
      "\n",
      "=== 处理 BV1A4421f7ek ===\n",
      "[META] 标题: 玩手机、睡觉！辅助驾驶到底管不管？8车横评！ | 发布日期: 2024-07-25\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 玩手机、睡觉！辅助驾驶到底管不管？8车横评！ (cid=1626545880) …\n",
      "    玩手机、睡觉！辅助驾驶到底管不管？8车横评！ 抓到 1183 条\n",
      "[OK] 单视频合计 1183 条 → combined_raw_BV1A4421f7ek_202509061720.csv / combined_raw_BV1A4421f7ek_202509061720.txt\n",
      "\n",
      "=== 处理 BV1PVPpeMEGz ===\n",
      "[META] 标题: 特斯拉fsd辅助驾驶重庆早高峰体验，江北区开往渝中区！一刀不剪 | 发布日期: 2025-02-27\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 特斯拉fsd辅助驾驶重庆早高峰体验，江北区开往渝中区！一刀不剪 (cid=28597028690) …\n",
      "    特斯拉fsd辅助驾驶重庆早高峰体验，江北区开往渝中区！一刀不剪 抓到 1481 条\n",
      "[OK] 单视频合计 1481 条 → combined_raw_BV1PVPpeMEGz_202509061720.csv / combined_raw_BV1PVPpeMEGz_202509061720.txt\n",
      "\n",
      "=== 处理 BV1gEvNzuEDq ===\n",
      "[META] 标题: 小米/小鹏/理想/问界 谁的辅助驾驶能让我上班不迟到？「快飞」 | 发布日期: 2025-08-26\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 小米/小鹏/理想/问界 谁的辅助驾驶能让我上班不迟到？「快飞」 (cid=31955617882) …\n",
      "    小米/小鹏/理想/问界 谁的辅助驾驶能让我上班不迟到？「快飞」 抓到 1037 条\n",
      "[ERROR] 处理 BV1gEvNzuEDq 失败：[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1gEvNzuEDq_31955617882_小米/小鹏/理想/问界 谁的辅助驾驶能让我上班不迟到？「快飞」.csv'\n",
      "\n",
      "=== 处理 BV12h411i73Y ===\n",
      "[META] 标题: 自动驾驶辅助系统真的比人类驾驶更安全吗？ | 发布日期: 2021-08-17\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 自动驾驶辅助系统真的比人类驾驶更安全吗？ (cid=390657546) …\n",
      "    自动驾驶辅助系统真的比人类驾驶更安全吗？ 抓到 1277 条\n",
      "[OK] 单视频合计 1277 条 → combined_raw_BV12h411i73Y_202509061720.csv / combined_raw_BV12h411i73Y_202509061720.txt\n",
      "\n",
      "=== 处理 BV1ZaACeDEXU ===\n",
      "[META] 标题: 自掏腰包全网首测，9.98万的比亚迪自动泊车好用吗？「秦L」 | 发布日期: 2025-02-23\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 自掏腰包全网首测，9.98万的比亚迪自动泊车好用吗？「秦L」 (cid=28548860733) …\n",
      "    自掏腰包全网首测，9.98万的比亚迪自动泊车好用吗？「秦L」 抓到 1523 条\n",
      "[OK] 单视频合计 1523 条 → combined_raw_BV1ZaACeDEXU_202509061720.csv / combined_raw_BV1ZaACeDEXU_202509061720.txt\n",
      "\n",
      "=== 处理 BV1WxGwz4EVg ===\n",
      "[META] 标题: 两分钟看完比亚迪L4级智能泊车重大官宣 #比亚迪率先实现媲美L4级智能泊车# #比亚迪承诺为智能泊车安全兜底# #比亚迪将迎来史上最大规模智驾OTA# | 发布日期: 2025-07-09\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 video1752033400968 (cid=30933650317) …\n",
      "    video1752033400968 抓到 1618 条\n",
      "[OK] 单视频合计 1618 条 → combined_raw_BV1WxGwz4EVg_202509061720.csv / combined_raw_BV1WxGwz4EVg_202509061720.txt\n",
      "\n",
      "=== 处理 BV1h5fnYcEyC ===\n",
      "[META] 标题: 危险危险危险！车辆“自动驾驶”，司机盖被睡觉？ | 发布日期: 2025-01-24\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 危险危险危险！车辆“自动驾驶”，司机盖被睡觉？ (cid=28054652505) …\n",
      "    危险危险危险！车辆“自动驾驶”，司机盖被睡觉？ 抓到 1608 条\n",
      "[OK] 单视频合计 1608 条 → combined_raw_BV1h5fnYcEyC_202509061720.csv / combined_raw_BV1h5fnYcEyC_202509061720.txt\n",
      "\n",
      "=== 处理 BV1hBuFzfEY5 ===\n",
      "[META] 标题: 【大虾沉浸式试驾】岚图FREE+👉智能驾驶·底盘·百公里加速全知道！ | 发布日期: 2025-07-13\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 【大虾沉浸式试驾】岚图FREE+👉智能驾驶·底盘·百公里加速全知道！ (cid=31011965200) …\n",
      "    【大虾沉浸式试驾】岚图FREE+👉智能驾驶·底盘·百公里加速全知道！ 抓到 1945 条\n",
      "[OK] 单视频合计 1945 条 → combined_raw_BV1hBuFzfEY5_202509061720.csv / combined_raw_BV1hBuFzfEY5_202509061720.txt\n",
      "\n",
      "=== 处理 BV1fcXQYUEik ===\n",
      "[META] 标题: 全网最全“特斯拉FSD对比华为ADS”，中美智驾有多大差距？【科技狐】 | 发布日期: 2025-03-01\n",
      "[META] 共 3 个分P\n",
      "  - 抓取 全网最全“特斯拉FSD对比华为ADS”，中美智驾有多大差距？【科技狐】 (cid=28632024035) …\n",
      "    全网最全“特斯拉FSD对比华为ADS”，中美智驾有多大差距？【科技狐】 抓到 2299 条\n",
      "  - 抓取 特斯拉FSD夜间广州郊区到城区一镜到底 (cid=28696839809) …\n",
      "    特斯拉FSD夜间广州郊区到城区一镜到底 抓到 8 条\n",
      "  - 抓取 特斯拉FSD广州城中村一镜到底 (cid=28696905721) …\n",
      "    特斯拉FSD广州城中村一镜到底 抓到 5 条\n",
      "[OK] 单视频合计 2312 条 → combined_raw_BV1fcXQYUEik_202509061720.csv / combined_raw_BV1fcXQYUEik_202509061720.txt\n",
      "\n",
      "=== 处理 BV1D197YCEsW ===\n",
      "[META] 标题: 全网唯一最真实对比，特斯拉、华为什么水平？「特斯拉VS华为」 | 发布日期: 2025-03-04\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 全网唯一最真实对比，特斯拉、华为什么水平？「特斯拉VS华为」 (cid=28692122756) …\n",
      "    全网唯一最真实对比，特斯拉、华为什么水平？「特斯拉VS华为」 抓到 2666 条\n",
      "[OK] 单视频合计 2666 条 → combined_raw_BV1D197YCEsW_202509061720.csv / combined_raw_BV1D197YCEsW_202509061720.txt\n",
      "\n",
      "=== 处理 BV17pZfYLEu6 ===\n",
      "[META] 标题: 小米su7事件，4月1号信息分析🧐 | 发布日期: 2025-04-02\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 小米su7事件，4月1号信息分析🧐 (cid=29199436543) …\n",
      "    小米su7事件，4月1号信息分析🧐 抓到 2582 条\n",
      "[OK] 单视频合计 2582 条 → combined_raw_BV17pZfYLEu6_202509061720.csv / combined_raw_BV17pZfYLEu6_202509061720.txt\n",
      "\n",
      "=== 处理 BV1ZDGYz6Eko ===\n",
      "[META] 标题: 长城汽车智能化，如何向前走？ | 发布日期: 2025-05-01\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 长城汽车智能化，如何向前走？ (cid=29727722007) …\n",
      "    长城汽车智能化，如何向前走？ 抓到 967 条\n",
      "[OK] 单视频合计 967 条 → combined_raw_BV1ZDGYz6Eko_202509061720.csv / combined_raw_BV1ZDGYz6Eko_202509061720.txt\n",
      "\n",
      "=== 处理 BV1TRLEzpE6k ===\n",
      "[META] 标题: 小米SU7事件，应该教会我们更多   【下尺报告】 | 发布日期: 2025-04-25\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 小米SU7事件，应该教会我们更多   【下尺报告】 (cid=29618405918) …\n",
      "    小米SU7事件，应该教会我们更多   【下尺报告】 抓到 4598 条\n",
      "[OK] 单视频合计 4598 条 → combined_raw_BV1TRLEzpE6k_202509061720.csv / combined_raw_BV1TRLEzpE6k_202509061720.txt\n",
      "\n",
      "=== 处理 BV1v7PjegENh ===\n",
      "[META] 标题: 一镜到底：特斯拉FSD挑战「智闯纽北」 | 发布日期: 2025-02-26\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 一镜到底：特斯拉FSD挑战「智闯纽北」 (cid=28605743916) …\n",
      "    一镜到底：特斯拉FSD挑战「智闯纽北」 抓到 4489 条\n",
      "[OK] 单视频合计 4489 条 → combined_raw_BV1v7PjegENh_202509061720.csv / combined_raw_BV1v7PjegENh_202509061720.txt\n",
      "\n",
      "=== 处理 BV1bN9PYeEBm ===\n",
      "[META] 标题: 52.99万，小米SU7智驾水平应该不怎么样吧？ | 发布日期: 2025-02-27\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 50多万1548匹小米SU7，智驾水平应该不怎么样吧？ (cid=28603977518) …\n",
      "    50多万1548匹小米SU7，智驾水平应该不怎么样吧？ 抓到 2851 条\n",
      "[OK] 单视频合计 2851 条 → combined_raw_BV1bN9PYeEBm_202509061720.csv / combined_raw_BV1bN9PYeEBm_202509061720.txt\n",
      "\n",
      "=== 处理 BV1kNZ1YJEPH ===\n",
      "[META] 标题: 智能驾驶≠自动驾驶！岚图梦想家“智驾”要如何更安全？ | 发布日期: 2025-04-03\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 智能驾驶≠自动驾驶！岚图梦想家“智驾”要如何更安全？ (cid=29212607774) …\n",
      "    智能驾驶≠自动驾驶！岚图梦想家“智驾”要如何更安全？ 抓到 734 条\n",
      "[OK] 单视频合计 734 条 → combined_raw_BV1kNZ1YJEPH_202509061720.csv / combined_raw_BV1kNZ1YJEPH_202509061720.txt\n",
      "\n",
      "=== 处理 BV1Bh3jzxE2q ===\n",
      "[META] 标题: 外国车评人来到武汉，感受无人驾驶，悬浮空轨，完全颠覆认知！ | 发布日期: 2025-07-04\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 外国车评人来到武汉，感受无人驾驶，悬浮空轨，完全颠覆认知！ (cid=30840851939) …\n",
      "    外国车评人来到武汉，感受无人驾驶，悬浮空轨，完全颠覆认知！ 抓到 3529 条\n",
      "[OK] 单视频合计 3529 条 → combined_raw_BV1Bh3jzxE2q_202509061721.csv / combined_raw_BV1Bh3jzxE2q_202509061721.txt\n",
      "\n",
      "=== 处理 BV1f7b6zPESj ===\n",
      "[META] 标题: 馆长8.14深圳行④，体验无人驾驶出租车：哎哟喂呀，第一次坐，真是吓死我了好不好？ | 发布日期: 2025-08-14\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 馆长8.14深圳行④，体验无人驾驶出租车：哎哟喂呀，第一次坐，真是吓死我了好不好？ (cid=31686987287) …\n",
      "    馆长8.14深圳行④，体验无人驾驶出租车：哎哟喂呀，第一次坐，真是吓死我了好不好？ 抓到 1962 条\n",
      "[OK] 单视频合计 1962 条 → combined_raw_BV1f7b6zPESj_202509061721.csv / combined_raw_BV1f7b6zPESj_202509061721.txt\n",
      "\n",
      "=== 处理 BV1bxb1zPEUJ ===\n",
      "[META] 标题: 首次体验萝卜快跑无人驾驶出租车 | 发布日期: 2025-07-25\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 首次体验萝卜快跑无人驾驶出租车 (cid=31257921527) …\n",
      "    首次体验萝卜快跑无人驾驶出租车 抓到 1724 条\n",
      "[OK] 单视频合计 1724 条 → combined_raw_BV1bxb1zPEUJ_202509061721.csv / combined_raw_BV1bxb1zPEUJ_202509061721.txt\n",
      "\n",
      "=== 处理 BV1Y1JezEELN ===\n",
      "[META] 标题: 本田思域你也敢无人驾驶？ | 发布日期: 2025-05-21\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 lv_0_20250520225935 (cid=30066738154) …\n",
      "    lv_0_20250520225935 抓到 535 条\n",
      "[OK] 单视频合计 535 条 → combined_raw_BV1Y1JezEELN_202509061721.csv / combined_raw_BV1Y1JezEELN_202509061721.txt\n",
      "\n",
      "=== 处理 BV1wWfDYiEUi ===\n",
      "[META] 标题: 【深度解析】智能驾驶烧了1000亿，发生了什么？ | 发布日期: 2025-01-27\n",
      "[META] 共 1 个分P\n",
      "  - 抓取 【深度解析】智能驾驶烧了1000亿，发生了什么？ (cid=28111012375) …\n",
      "    【深度解析】智能驾驶烧了1000亿，发生了什么？ 抓到 7443 条\n",
      "[OK] 单视频合计 7443 条 → combined_raw_BV1wWfDYiEUi_202509061721.csv / combined_raw_BV1wWfDYiEUi_202509061721.txt\n",
      "\n",
      "====== 汇总完成 ======\n",
      "总计抓到 113175 条弹幕\n",
      "全量 CSV：C:\\Users\\Andrew\\Desktop\\homework\\cc\\bili_danmu_results\\combined_all_202509061721.csv\n",
      "全量 TXT：C:\\Users\\Andrew\\Desktop\\homework\\cc\\bili_danmu_results\\combined_all_202509061721.txt\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "批量抓取 B 站弹幕（无需输入日期），从 url.txt 读取多个视频链接/BV，\n",
    "对每条弹幕的 timestamp 统一写入该视频的“发布日期”（pubdate），\n",
    "并在 bili_danmu_results/ 下保存分P CSV、每视频合并 CSV/TXT、以及全量汇总 CSV/TXT。\n",
    "\n",
    "用法：\n",
    "    python crawl_bili_danmu_batch.py\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, csv, datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "HEADERS_BASE = {\n",
    "    \"origin\": \"https://www.bilibili.com\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DM_RE = re.compile(rb':(.*?)@', re.S)            # 从 seg.so 提取纯文本弹幕\n",
    "BV_RE = re.compile(r'(BV[0-9A-Za-z]{10,})')\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "URL_LIST_FILE = \"url.txt\"  # 每行一个 BV 或视频链接，支持注释（以 # 开头）\n",
    "\n",
    "# 可选：如需登录态稳定性，可在此处粘贴你的 Cookie（也可以留空）\n",
    "DEFAULT_COOKIE = \"enable_web_push=DISABLE; buvid4=441674D3-73C6-EE18-D7F6-0A9AA6A8764B10929-024061616-tMB8uhs7bNpfYIQVaVKjtQ%3D%3D; DedeUserID=499303036; DedeUserID__ckMd5=7c2e754fb5285a0b; buvid_fp_plain=undefined; enable_feed_channel=ENABLE; hit-dyn-v2=1; fingerprint=d30d288e57446a6e2075d79545bcdece; buvid_fp=d30d288e57446a6e2075d79545bcdece; buvid3=D8E29309-F4DF-1535-6A27-C5ADFB04F3B208505infoc; b_nut=1750090608; _uuid=EE91029AA-C3ED-2D10D-A31B-8610DA6DC1BEE28428infoc; header_theme_version=OPEN; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; rpdid=|(J~R~uR))~u0J'u~lJkJl)Y|; LIVE_BUVID=AUTO2217563800546588; PVID=2; CURRENT_QUALITY=80; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTczMDE2NTEsImlhdCI6MTc1NzA0MjM5MSwicGx0IjotMX0.FWLzrbsTzZVO9pAvA2K-j8JpwDO4_00W2sqETwK91NA; bili_ticket_expires=1757301591; SESSDATA=fc10051c%2C1772610457%2C5b79d%2A91CjBHREFbDUZegrrFspSN0CJBiv6HiJF4vOUubDwDGrFJa2-yRA8gOqoqGpCwy29sjfwSVnVwNlBZR1hKb2VFaGI0NkZpcWZMYm5ubE5Kd2JFdkluaFp2Q1JfeFNLX1JqWWxxb19mUENfa193RkFUVDNPa3lZcUU4WmZrZ3ZlLXRpZ3RBRkQ5RnNRIIEC; bili_jct=e685ab8cae35933d20e223cb4b6a6d91; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; bsource=search_google; sid=7l86jgpi; b_lsid=D181510CA_1991E1E02C8; bp_t_offset_499303036=1109440247839588352; CURRENT_FNVAL=2000; home_feed_column=4; browser_resolution=1089-770\"\n",
    "# DEFAULT_COOKIE = \"SESSDATA=xxx; bili_jct=yyy; ...\"\n",
    "\n",
    "if not DEFAULT_COOKIE:\n",
    "    print(\"no cookie\")\n",
    "\n",
    "# ---------- 工具 ----------\n",
    "def build_session(cookie=None, referer=None):\n",
    "    s = requests.Session()\n",
    "    headers = HEADERS_BASE.copy()\n",
    "    if cookie:\n",
    "        headers[\"cookie\"] = cookie\n",
    "    if referer:\n",
    "        headers[\"referer\"] = referer\n",
    "    retries = Retry(\n",
    "        total=3, backoff_factor=0.6,\n",
    "        status_forcelist=[412, 429, 500, 502, 503, 504]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries, pool_connections=16, pool_maxsize=16))\n",
    "    s.headers.update(headers)\n",
    "    return s\n",
    "\n",
    "def extract_bvid(text: str) -> str:\n",
    "    m = BV_RE.search(text)\n",
    "    if not m:\n",
    "        raise ValueError(f\"未找到 BV 号：{text}\")\n",
    "    return m.group(1)\n",
    "\n",
    "def read_urls(path=URL_LIST_FILE):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"未找到 {path}，请创建并写入视频链接或 BV 号\")\n",
    "    bvids = []\n",
    "    seen = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):  # 跳过注释/空行\n",
    "                continue\n",
    "            try:\n",
    "                bvid = extract_bvid(line)\n",
    "                if bvid not in seen:\n",
    "                    seen.add(bvid)\n",
    "                    bvids.append(bvid)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] 跳过无法解析的行：{line} ({e})\")\n",
    "    return bvids\n",
    "\n",
    "def get_pagelist_by_bvid(session, bvid):\n",
    "    \"\"\"获取分P信息：[{cid, page, part, duration}, ...]\"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/player/pagelist?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0:\n",
    "        raise RuntimeError(f\"pagelist 接口失败：code={j.get('code')} msg={j.get('message')}\")\n",
    "    pages = j.get(\"data\") or []\n",
    "    if not pages:\n",
    "        raise RuntimeError(\"pagelist 返回空，可能 BV 无效或需要登录。\")\n",
    "    return pages\n",
    "\n",
    "def get_video_meta(session, bvid):\n",
    "    \"\"\"\n",
    "    取视频基础信息（标题、发布日期）。\n",
    "    x/web-interface/view 返回 data.pubdate（秒级 Unix 时间戳）\n",
    "    \"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/web-interface/view?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0 or not j.get(\"data\"):\n",
    "        raise RuntimeError(f\"view 接口失败：code={j.get('code')} msg={j.get('message')}\")\n",
    "    data = j[\"data\"]\n",
    "    title = data.get(\"title\", f\"{bvid}\")\n",
    "    pub_ts = data.get(\"pubdate\")  # int\n",
    "    if not pub_ts:\n",
    "        # 有些情况下可能返回 ctime；兜底一下\n",
    "        pub_ts = data.get(\"ctime\")\n",
    "    if not pub_ts:\n",
    "        # 再兜底为“今天”\n",
    "        pub_dt = datetime.datetime.now()\n",
    "    else:\n",
    "        pub_dt = datetime.datetime.fromtimestamp(int(pub_ts))\n",
    "    pubdate_str = pub_dt.strftime(\"%Y-%m-%d\")  # 统一写“日期”到 timestamp\n",
    "    return title, pubdate_str\n",
    "\n",
    "def fetch_seg_bytes(session, cid, seg_idx):\n",
    "    url = f\"https://api.bilibili.com/x/v2/dm/web/seg.so?type=1&oid={cid}&segment_index={seg_idx}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def parse_danmu_from_seg(seg_bytes):\n",
    "    # 仅提取“内容”，不解析更丰富字段（若需要可以改为 protobuf 解析）\n",
    "    return [m.decode('utf-8', errors='ignore') for m in DM_RE.findall(seg_bytes)]\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def save_csv(rows, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"content\", \"video_title\", \"timestamp\"])\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def save_txt(lines, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write((line or \"\").strip() + \"\\n\")\n",
    "\n",
    "# ---------- 抓取一个视频 ----------\n",
    "def crawl_one_bv(session, bvid, cookie=None, sleep_s=0.05):\n",
    "    referer = f\"https://www.bilibili.com/video/{bvid}\"\n",
    "    session.headers.update({\"referer\": referer})\n",
    "    print(f\"\\n=== 处理 {bvid} ===\")\n",
    "\n",
    "    # 1) 元数据：标题 & 发布日期（timestamp 统一用它）\n",
    "    video_title_main, pubdate_str = get_video_meta(session, bvid)\n",
    "    print(f\"[META] 标题: {video_title_main} | 发布日期: {pubdate_str}\")\n",
    "\n",
    "    # 2) 分 P 列表\n",
    "    pages = get_pagelist_by_bvid(session, bvid)\n",
    "    print(f\"[META] 共 {len(pages)} 个分P\")\n",
    "\n",
    "    all_rows_video = []\n",
    "    for p in pages:\n",
    "        cid = p[\"cid\"]\n",
    "        part = p.get(\"part\") or f\"P{p.get('page', '?')}\"\n",
    "        title = f\"{bvid} | {part} | {video_title_main}\"\n",
    "        print(f\"  - 抓取 {part} (cid={cid}) …\")\n",
    "\n",
    "        # 3) seg.so 逐段抓\n",
    "        all_items = []\n",
    "        seg_idx = 1\n",
    "        empty_runs = 0\n",
    "        while True:\n",
    "            try:\n",
    "                seg_bytes = fetch_seg_bytes(session, cid, seg_idx)\n",
    "            except requests.HTTPError:\n",
    "                time.sleep(1.0)\n",
    "                seg_bytes = fetch_seg_bytes(session, cid, seg_idx)\n",
    "\n",
    "            contents = parse_danmu_from_seg(seg_bytes)\n",
    "            if not contents:\n",
    "                empty_runs += 1\n",
    "                if empty_runs >= 2:\n",
    "                    break\n",
    "            else:\n",
    "                empty_runs = 0\n",
    "                for c in contents:\n",
    "                    all_items.append({\n",
    "                        \"content\": c,\n",
    "                        \"video_title\": title,\n",
    "                        # 统一写“视频发布日期”\n",
    "                        \"timestamp\": pubdate_str\n",
    "                    })\n",
    "            seg_idx += 1\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "        print(f\"    {part} 抓到 {len(all_items)} 条\")\n",
    "        all_rows_video.extend(all_items)\n",
    "\n",
    "        # 分P各自保存\n",
    "        csv_path = os.path.join(RAW_DIR, f\"{bvid}_{cid}_{part}.csv\")\n",
    "        save_csv(all_items, csv_path)\n",
    "\n",
    "    # 单视频合并输出\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    combined_csv = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.csv\")\n",
    "    combined_txt = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.txt\")\n",
    "    save_csv(all_rows_video, combined_csv)\n",
    "    save_txt([r[\"content\"] for r in all_rows_video], combined_txt)\n",
    "    print(f\"[OK] 单视频合计 {len(all_rows_video)} 条 → {os.path.basename(combined_csv)} / {os.path.basename(combined_txt)}\")\n",
    "\n",
    "    return all_rows_video  # 给总汇总用\n",
    "\n",
    "# ---------- 主流程 ----------\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"B站弹幕批量抓取（按 url.txt 列表；timestamp=视频发布日期）\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    bvids = read_urls(URL_LIST_FILE)\n",
    "    if not bvids:\n",
    "        print(\"未从 url.txt 读到任何 BV/链接。\"); return\n",
    "\n",
    "    sess = build_session(cookie=DEFAULT_COOKIE)\n",
    "\n",
    "    grand_total_rows = []\n",
    "    for bvid in bvids:\n",
    "        try:\n",
    "            rows = crawl_one_bv(sess, bvid, cookie=DEFAULT_COOKIE)\n",
    "            grand_total_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] 处理 {bvid} 失败：{e}\")\n",
    "\n",
    "    # 全量汇总\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    all_csv = os.path.join(RAW_DIR, f\"combined_all_{ts}.csv\")\n",
    "    all_txt = os.path.join(RAW_DIR, f\"combined_all_{ts}.txt\")\n",
    "    save_csv(grand_total_rows, all_csv)\n",
    "    save_txt([r[\"content\"] for r in grand_total_rows], all_txt)\n",
    "\n",
    "    print(\"\\n====== 汇总完成 ======\")\n",
    "    print(f\"总计抓到 {len(grand_total_rows)} 条弹幕\")\n",
    "    print(f\"全量 CSV：{all_csv}\")\n",
    "    print(f\"全量 TXT：{all_txt}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "B站弹幕清洗（仅处理 combined_all_* 汇总文件）\n",
      "============================================================\n",
      "将仅清洗：combined_all_202509061721.csv\n",
      "读取：combined_all_202509061721.csv\n",
      "读取记录数：113175\n",
      "\n",
      "保留 86961/113175 (76.8%)\n",
      "\n",
      "清洗后保存：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\combined_cleaned_danmu_202509061738.txt\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\combined_cleaned_danmu_202509061738.csv\n",
      "移除原因分布： {'noise': 13304, 'duplicate': 12910}\n",
      "移除明细：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\removed_reasons_202509061738.csv\n",
      "调试对照样本：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\debug_original_vs_cleaned_202509061738.csv\n",
      "\n",
      "完成。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "B站弹幕批量清洗工具（仅处理汇总文件 combined_all_*）\n",
    "- 只读取 bili_danmu_results/combined_all_*.csv（优先）或同名 .txt（兜底）\n",
    "- 抽取→去噪→规范化→去重，保留 video_title/timestamp\n",
    "- 归一化日期：YYYY/MM/DD、YYYY.M.D 等 → YYYY-MM-DD\n",
    "- 输出 cleaned_danmu_results/：\n",
    "    - combined_cleaned_danmu_*.txt / *.csv\n",
    "    - removed_reasons_*.csv （被删除条目与原因）\n",
    "    - debug_original_vs_cleaned_*.csv （抽样对照）\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "OUT_DIR = os.path.join(os.getcwd(), \"cleaned_danmu_results\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 预编译正则 ----------\n",
    "RE_CTRL = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')                    # 控制字符\n",
    "RE_HTML = re.compile(r'<[^>]+>')\n",
    "RE_URL  = re.compile(r'https?://\\S+')\n",
    "# 仅保留：中英文、数字、常见中文标点、连字符\n",
    "RE_KEEP = re.compile(r'[^\\w\\u4e00-\\u9fa5，。！？、；：\"\\'()【】《》·—…\\-]+')\n",
    "RE_SP   = re.compile(r'\\s+')\n",
    "RE_MULTI_PUNCT = re.compile(r'([，。！？…])\\1+')\n",
    "# 常见行首噪声块（括号/奇怪字节/短hex/孤立字母/标点）组合\n",
    "RE_NOISY_PREFIX = re.compile(\n",
    "    r'^\\s*(?:[\\(\\)\\[\\]{}<>\\|\\\\/\\-_=+~^`·•]+|[A-Za-z](?=\\s)|[0-9A-Fa-f]{6,10}|[,.:;，。；：!！?？\\s])+'\n",
    ")\n",
    "\n",
    "def _cut_after_colon(raw: str) -> str:\n",
    "    \"\"\"如含冒号，取最后一个冒号后的片段\"\"\"\n",
    "    if ':' in raw:\n",
    "        return raw.split(':')[-1]\n",
    "    return raw\n",
    "\n",
    "def _strip_to_first_textual_char(s: str) -> str:\n",
    "    \"\"\"剥离前缀噪声块，定位第一个有效字符（中英数）\"\"\"\n",
    "    s = RE_NOISY_PREFIX.sub('', s)\n",
    "    idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch.isalnum() or ('\\u4e00' <= ch <= '\\u9fa5'):\n",
    "            idx = i\n",
    "            break\n",
    "    return s[idx:] if idx is not None else ''\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    s = RE_CTRL.sub(' ', s)                    # 删控制字符（含你样例里的奇怪前导字节）\n",
    "    s = unicodedata.normalize('NFKC', s)       # 全角→半角\n",
    "    s = RE_HTML.sub('', s)                     # 去HTML\n",
    "    s = RE_URL.sub('', s)                      # 去URL\n",
    "    s = RE_KEEP.sub(' ', s)                    # 过滤到允许字符集\n",
    "    s = RE_MULTI_PUNCT.sub(r'\\1', s)           # 连续标点压缩\n",
    "    s = RE_SP.sub(' ', s).strip()              # 空白规整\n",
    "    return s\n",
    "\n",
    "def _looks_like_noise(s: str) -> bool:\n",
    "    \"\"\"噪声判定：纯数字/单字母/太短/文本占比低\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    if s.isdigit():\n",
    "        return True\n",
    "    if len(s) == 1 and s.isalnum():\n",
    "        return True\n",
    "    if len(s) < 2:\n",
    "        return True\n",
    "    total = len(s)\n",
    "    keep = sum(1 for ch in s if ch.isalnum() or '\\u4e00' <= ch <= '\\u9fa5')\n",
    "    if keep / max(1, total) < 0.3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _normalize_date(ts: str) -> str:\n",
    "    \"\"\"\n",
    "    归一化日期到 YYYY-MM-DD：\n",
    "    支持：YYYY/M/D, YYYY-M-D, YYYY.M.D, YYYY_MM_DD（都可1位月日）\n",
    "    其他情况原样返回（或空则返回空）\n",
    "    \"\"\"\n",
    "    if not ts or not str(ts).strip():\n",
    "        return \"\"\n",
    "    s = str(ts).strip()\n",
    "    s = RE_CTRL.sub(' ', s)         # 防止控制字符\n",
    "    s = s.replace('.', '-').replace('/', '-').replace('_', '-')\n",
    "    parts = [p for p in s.split('-') if p]\n",
    "    if len(parts) >= 3 and parts[0].isdigit():\n",
    "        y = parts[0]\n",
    "        m = parts[1].zfill(2) if parts[1].isdigit() else parts[1]\n",
    "        d = parts[2].zfill(2) if parts[2].isdigit() else parts[2]\n",
    "        try:\n",
    "            dt = datetime.date(int(y), int(m), int(d))\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "def clean_one(raw: str) -> str:\n",
    "    \"\"\"单条弹幕清洗：抽取→去噪→规范化→长度保护\"\"\"\n",
    "    s = str(raw or '')\n",
    "    s = _cut_after_colon(s)\n",
    "    s = RE_CTRL.sub(' ', s)              # 先去控制符，再定位文本起点\n",
    "    s = _strip_to_first_textual_char(s)\n",
    "    s = _normalize_text(s)\n",
    "    if len(s) > 100:\n",
    "        s = s[:100].rstrip()\n",
    "    return s\n",
    "\n",
    "# ---------- 文件选择：仅 combined_all_* ----------\n",
    "def pick_combined_all_files():\n",
    "    \"\"\"\n",
    "    优先 CSV：\n",
    "      - 若存在多个 combined_all_*.csv，按修改时间取“最新的1个”\n",
    "      - 若没有 CSV，则回退到最新的 combined_all_*.txt\n",
    "    返回：文件路径列表（长度=1），并标注类型\n",
    "    \"\"\"\n",
    "    csvs = glob.glob(os.path.join(RAW_DIR, \"combined_all_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(RAW_DIR, \"combined_all_*.txt\"))\n",
    "\n",
    "    pick = None\n",
    "    ftype = None\n",
    "\n",
    "    if csvs:\n",
    "        pick = max(csvs, key=os.path.getmtime)\n",
    "        ftype = \"csv\"\n",
    "    elif txts:\n",
    "        pick = max(txts, key=os.path.getmtime)\n",
    "        ftype = \"txt\"\n",
    "\n",
    "    if not pick:\n",
    "        return []\n",
    "    print(f\"将仅清洗：{os.path.basename(pick)}\")\n",
    "    return [(pick, ftype)]\n",
    "\n",
    "def read_records_from_csv(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in [\"content\", \"video_title\", \"timestamp\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df = df[[\"content\", \"video_title\", \"timestamp\"]]\n",
    "    # 归一化 timestamp\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].map(_normalize_date)\n",
    "    return df.to_dict(\"records\")\n",
    "\n",
    "def read_records_from_txt(path: str):\n",
    "    # 一行一条，仅有 content；video_title/timestamp 置空\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            rows.append({\n",
    "                \"content\": line,\n",
    "                \"video_title\": \"\",\n",
    "                \"timestamp\": \"\"\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def read_combined_all():\n",
    "    picks = pick_combined_all_files()\n",
    "    if not picks:\n",
    "        print(f\"未在 {RAW_DIR} 找到 combined_all_*.csv 或 .txt\")\n",
    "        return []\n",
    "    path, ftype = picks[0]\n",
    "    print(f\"读取：{os.path.basename(path)}\")\n",
    "    if ftype == \"csv\":\n",
    "        return read_records_from_csv(path)\n",
    "    else:\n",
    "        return read_records_from_txt(path)\n",
    "\n",
    "# ---------- 清洗主流程 ----------\n",
    "def clean_danmu_content(records):\n",
    "    \"\"\"\n",
    "    输入：记录列表（至少含 content）\n",
    "    输出：\n",
    "      - cleaned: [{original, cleaned, video_title, timestamp}]\n",
    "      - removed: [(original, reason)]\n",
    "    \"\"\"\n",
    "    cleaned, removed = [], []\n",
    "\n",
    "    for dm in records:\n",
    "        raw = str(dm.get(\"content\", \"\") or \"\")\n",
    "        if not raw:\n",
    "            removed.append((raw, \"empty\")); continue\n",
    "\n",
    "        out = clean_one(raw)\n",
    "        if _looks_like_noise(out):\n",
    "            removed.append((raw, \"noise\")); continue\n",
    "\n",
    "        cleaned.append({\n",
    "            \"original\": raw,\n",
    "            \"cleaned\": out,\n",
    "            \"video_title\": dm.get(\"video_title\", \"未知视频\"),\n",
    "            \"timestamp\": _normalize_date(dm.get(\"timestamp\", \"\"))  # 再保险\n",
    "        })\n",
    "\n",
    "    # 去重（按 cleaned）\n",
    "    seen, dedup = set(), []\n",
    "    for row in cleaned:\n",
    "        key = row[\"cleaned\"]\n",
    "        if key in seen:\n",
    "            removed.append((row[\"original\"], \"duplicate\")); continue\n",
    "        seen.add(key)\n",
    "        dedup.append(row)\n",
    "\n",
    "    return dedup, removed\n",
    "\n",
    "# ---------- 输出 ----------\n",
    "def save_outputs(cleaned, removed):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    txt_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.txt\")\n",
    "    csv_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.csv\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in cleaned:\n",
    "            f.write(r[\"cleaned\"] + \"\\n\")\n",
    "    pd.DataFrame(cleaned).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n清洗后保存：\\n- {txt_path}\\n- {csv_path}\")\n",
    "\n",
    "    if removed:\n",
    "        rm_path = os.path.join(OUT_DIR, f\"removed_reasons_{ts}.csv\")\n",
    "        pd.DataFrame(removed, columns=[\"original\", \"reason\"]).to_csv(\n",
    "            rm_path, index=False, encoding=\"utf-8-sig\")\n",
    "        cnt = Counter([r for _, r in removed])\n",
    "        print(\"移除原因分布：\", dict(cnt))\n",
    "        print(f\"移除明细：\\n- {rm_path}\")\n",
    "\n",
    "    dbg_path = os.path.join(OUT_DIR, f\"debug_original_vs_cleaned_{ts}.csv\")\n",
    "    pd.DataFrame(cleaned)[[\"original\", \"cleaned\"]].head(200)\\\n",
    "        .to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"调试对照样本：\\n- {dbg_path}\")\n",
    "\n",
    "# ---------- 入口 ----------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"B站弹幕清洗（仅处理 combined_all_* 汇总文件）\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    records = read_combined_all()\n",
    "    if not records:\n",
    "        return\n",
    "    print(f\"读取记录数：{len(records)}\")\n",
    "\n",
    "    cleaned, removed = clean_danmu_content(records)\n",
    "    keep_ratio = len(cleaned) / max(1, len(records)) * 100\n",
    "    print(f\"\\n保留 {len(cleaned)}/{len(records)} ({keep_ratio:.1f}%)\")\n",
    "\n",
    "    save_outputs(cleaned, removed)\n",
    "    print(\"\\n完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T17:38:10.832812Z",
     "end_time": "2025-09-06T17:38:18.410113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Danmu Analysis Pro (Word Frequency / Word Cloud / Adaptive Clustering / Sentiment / Monthly Trends)\n",
      "================================================================\n",
      "[INFO] Analyzing file: cleaned_danmu_results\\combined_cleaned_danmu_202509061738.csv\n",
      "[INFO] Number of sentences after cleaning: 86961\n",
      "[INFO] Number of documents after segmentation: 78047; total tokens: 219504\n",
      "[OK] Word frequency file: analysis_results\\word_frequency_202509061957.csv\n",
      "[OK] Word cloud: analysis_results\\wordcloud_202509061957.png\n",
      "[OK] Top bigram file: analysis_results\\top_bigram_202509061957.csv\n",
      "[INFO] Adaptive clustering …\n",
      "[INFO] Silhouette score: K=7, score=0.0194\n",
      "[OK] Cluster overview: analysis_results\\cluster_summary_202509062001.png\n",
      "[OK] Cluster keywords file: analysis_results\\cluster_keywords_202509061957.csv\n",
      "[OK] Cluster sentiment file: analysis_results\\cluster_sentiment_202509061957.csv\n",
      "[OK] Monthly sentiment file: analysis_results\\monthly_sentiment_202509062005.csv\n",
      "[OK] Monthly sentiment chart: analysis_results\\monthly_sentiment_202509062005.png\n",
      "\n",
      "Top 10 Words:\n",
      "1. 驾驶  3815\n",
      "2. 华为  2664\n",
      "3. 没有  2424\n",
      "4. 自动  2409\n",
      "5. 问题  1880\n",
      "6. 智驾  1728\n",
      "7. 辅助  1376\n",
      "8. 识别  1335\n",
      "9. 小米  1066\n",
      "10. 司机  1042\n",
      "\n",
      "Done. Output directory: analysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "B站弹幕分析 Pro（适配 cleaned_danmu_results）\n",
    "- 只读取最新 cleaned CSV（combined_cleaned_danmu_*.csv）\n",
    "- 分词：词性过滤 + 停用词（含口语补充） + 可选行业词典\n",
    "- 向量化：1-2gram TF-IDF（min_df/max_df 抑制口水词）\n",
    "- 自适应聚类：KMeans + 轮廓系数选 K（cosine）\n",
    "- 主题命名：非停用词关键词 Top2\n",
    "- 情感：SnowNLP（<4字判中性），汇总主题；并新增“按月情感走势”\n",
    "- 可视化：词云、主题分布饼图 + 主题情感堆叠柱状 + 关键词表 + 月度情感折线\n",
    "- 导出：词频、主题关键词/情感、Top n-gram、月度情感\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba, jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ===================== 参数 =====================\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"      # 可空\n",
    "USER_DICT   = \"user_dict.txt\"         # 可空\n",
    "\n",
    "# 词性保留（名词/动词/术语/英文等）\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# 弹幕口语停用词补充\n",
    "DANMU_STOP_EXTRA = {\n",
    "    \"这个\",\"那个\",\"就是\",\"什么\",\"还有\",\"然后\",\"但是\",\"所以\",\"还是\",\"已经\",\"真的\",\"感觉\",\"觉得\",\"知道\",\n",
    "    \"可以\",\"不可以\",\"不会\",\"不能\",\"应该\",\"可能\",\"有点\",\"有些\",\"怎么\",\"为啥\",\"为什么\",\n",
    "    \"啊\",\"呀\",\"呢\",\"吧\",\"哦\",\"哇\",\"诶\",\"嘛\",\"哈\",\"哈哈\",\"哈哈哈\",\"emm\",\"嗯\",\"啊啊\",\"呜呜\",\n",
    "    \"视频\",\"弹幕\",\"现在\",\"今天\",\"昨天\",\"明天\",\"这里\",\"那里\",\"这样\",\"那样\",\"很多\",\"非常\"\n",
    "}\n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 4000\n",
    "MIN_DF       = 5\n",
    "MAX_DF       = 0.6\n",
    "NGRAM        = (1,2)\n",
    "\n",
    "# K 范围\n",
    "K_MIN, K_MAX = 2, 8\n",
    "\n",
    "# 词云 TopN\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# 小句情感中性阈\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "# =================================================\n",
    "\n",
    "# ---------- 字体 ----------\n",
    "def setup_chinese_font():\n",
    "    candidates = ['SimHei','Microsoft YaHei','SimSun','KaiTi',\n",
    "                  'Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for name in candidates:\n",
    "        if any(name in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = name; break\n",
    "    if picked:\n",
    "        plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                font_path = f; break\n",
    "    if not font_path:\n",
    "        font_path = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return font_path\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "# ---------- 基础工具 ----------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    base |= set(list(\"，。、！？；：”“‘‘（）()[]【】—…- \"))\n",
    "    base |= DANMU_STOP_EXTRA\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] Loaded user dictionary: {USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_csv():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    if not csvs: return None\n",
    "    return max(csvs, key=os.path.getmtime)\n",
    "\n",
    "def normalize_date_str(s):\n",
    "    \"\"\"兼容 2025/8/12, 2025.8.12, 2025-8-12 等 → YYYY-MM-DD\"\"\"\n",
    "    if not isinstance(s, str): s = str(s or \"\")\n",
    "    s = CTRL_RE.sub(\" \", s).strip().replace(\"/\", \"-\").replace(\".\", \"-\").replace(\"_\", \"-\")\n",
    "    parts = [p for p in s.split(\"-\") if p]\n",
    "    if len(parts) >= 3 and parts[0].isdigit() and parts[1].isdigit() and parts[2].isdigit():\n",
    "        try:\n",
    "            dt = datetime.date(int(parts[0]), int(parts[1]), int(parts[2]))\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "def load_cleaned_df(latest_csv):\n",
    "    df = pd.read_csv(latest_csv)\n",
    "    # 兼容列名\n",
    "    if \"cleaned\" not in df.columns:\n",
    "        raise ValueError(\"Column 'cleaned' not found in the cleaned CSV\")\n",
    "    for col in [\"video_title\",\"timestamp\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    # 轻量去噪\n",
    "    df[\"cleaned\"] = df[\"cleaned\"].astype(str).map(lambda x: EMOJI_RE.sub(\"\", CTRL_RE.sub(\" \", x)).strip())\n",
    "    # 日期归一化\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].astype(str).map(normalize_date_str)\n",
    "    return df\n",
    "\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter, top_n=WORDCLOUD_TOPN):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(top_n)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('High-frequency Word Cloud', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------- 聚类 ----------\n",
    "def auto_kmeans(docs, min_k=K_MIN, max_k=K_MAX, max_features=MAX_FEATURES):\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if n < 60:  # 少样本保护\n",
    "        k = max(2, min(4, n // 15 or 2))\n",
    "        model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        return model, vec, None\n",
    "\n",
    "    X_eval = X\n",
    "    if n > 6000:  # 抽样评估\n",
    "        idx = np.random.RandomState(42).choice(n, 6000, replace=False)\n",
    "        X_eval = X[idx]\n",
    "\n",
    "    best_k, best_score, best_model = None, -1, None\n",
    "    for k in range(min_k, max_k+1):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        labels_eval = km.labels_ if X_eval is X else km.predict(X_eval)\n",
    "        score = silhouette_score(X_eval, labels_eval, metric='cosine')\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_model = k, score, km\n",
    "    return best_model, vec, best_score\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in range(topn*3):\n",
    "            idx = order[i, j]\n",
    "            if idx < len(terms):\n",
    "                t = terms[idx]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"Topic {i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # Pie: Topic distribution\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt} items ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"Topic Distribution\", fontsize=15)\n",
    "\n",
    "    # Stacked bars: Sentiment distribution\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='Positive')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='Neutral')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='Negative')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"Ratio\"); ax2.set_title(\"Sentiment Distribution\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # Keywords table\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"Keyword {i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"Cluster Keywords\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# ---------- 月度情感 ----------\n",
    "def monthly_sentiment(df):\n",
    "    \"\"\"\n",
    "    读取 cleaned（原句）和 timestamp（日期），按月汇总积极/中性/消极比例\n",
    "    \"\"\"\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        return None, None\n",
    "    tmp = df.copy()\n",
    "    tmp[\"date\"] = tmp[\"timestamp\"].map(normalize_date_str)\n",
    "    tmp = tmp[tmp[\"date\"].str.len() >= 8]\n",
    "    if tmp.empty:\n",
    "        return None, None\n",
    "\n",
    "    tmp[\"month\"] = tmp[\"date\"].map(lambda x: x[:7])  # YYYY-MM\n",
    "    recs = []\n",
    "    for m, sub in tmp.groupby(\"month\"):\n",
    "        pos = neu = neg = 0\n",
    "        total = 0\n",
    "        for t in sub[\"cleaned\"].astype(str):\n",
    "            t = t.strip()\n",
    "            if not t: continue\n",
    "            if len(t) < SHORT_NEUTRAL_LEN:\n",
    "                neu += 1\n",
    "            else:\n",
    "                s = SnowNLP(t).sentiments\n",
    "                if s > 0.6: pos += 1\n",
    "                elif s < 0.4: neg += 1\n",
    "                else: neu += 1\n",
    "            total += 1\n",
    "        if total > 0:\n",
    "            recs.append({\"month\": m,\n",
    "                         \"positive\": pos/total,\n",
    "                         \"neutral\": neu/total,\n",
    "                         \"negative\": neg/total,\n",
    "                         \"count\": total})\n",
    "    if not recs:\n",
    "        return None, None\n",
    "\n",
    "    ms_df = pd.DataFrame(sorted(recs, key=lambda x: x[\"month\"]))\n",
    "    # 可视化\n",
    "    plt.figure(figsize=(12,6))\n",
    "    x = np.arange(len(ms_df))\n",
    "    plt.plot(x, ms_df[\"positive\"], marker=\"o\", label=\"Positive\")\n",
    "    plt.plot(x, ms_df[\"neutral\"], marker=\"o\", label=\"Neutral\")\n",
    "    plt.plot(x, ms_df[\"negative\"], marker=\"o\", label=\"Negative\")\n",
    "    plt.xticks(x, ms_df[\"month\"], rotation=45, ha=\"right\")\n",
    "    plt.ylim(0,1); plt.ylabel(\"Ratio\"); plt.title(\"Monthly Sentiment Trends\")\n",
    "    plt.legend()\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"monthly_sentiment_{ts}.png\")\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=300); plt.close()\n",
    "    csv_out = os.path.join(OUT_DIR, f\"monthly_sentiment_{ts}.csv\")\n",
    "    ms_df.to_csv(csv_out, index=False, encoding=\"utf-8-sig\")\n",
    "    return csv_out, out\n",
    "\n",
    "# ---------- 主流程 ----------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"Danmu Analysis Pro (Word Frequency / Word Cloud / Adaptive Clustering / Sentiment / Monthly Trends)\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_csv()\n",
    "    if not latest:\n",
    "        print(\"No cleaned CSV found, please run preprocessing first.\"); return\n",
    "    print(f\"[INFO] Analyzing file: {latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "\n",
    "    df = load_cleaned_df(latest)\n",
    "    lines = df[\"cleaned\"].astype(str).tolist()\n",
    "    print(f\"[INFO] Number of sentences after cleaning: {len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] Empty after segmentation (stopwords too aggressive or texts too short).\"); return\n",
    "    print(f\"[INFO] Number of documents after segmentation: {len(docs)}; total tokens: {len(all_words)}\")\n",
    "\n",
    "    # 词频、词云、Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter); print(f\"[OK] Word frequency file: {freq_csv}\")\n",
    "    wc_path = draw_wordcloud(counter)\n",
    "    if wc_path: print(f\"[OK] Word cloud: {wc_path}\")\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram = top_ngrams(docs, n=2, topk=50)\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigram file: {bigram_csv}\")\n",
    "\n",
    "    # 聚类\n",
    "    if len(docs) >= 20:\n",
    "        print(\"[INFO] Adaptive clustering …\")\n",
    "        model, vec, s_score = auto_kmeans(docs)\n",
    "        if s_score is not None:\n",
    "            print(f\"[INFO] Silhouette score: K={model.n_clusters}, score={s_score:.4f}\")\n",
    "        labels = model.labels_\n",
    "\n",
    "        # 主题命名 + 关键词\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, DANMU_STOP_EXTRA, topn=10)\n",
    "\n",
    "        # 每簇原句（情感用）\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {}\n",
    "        for i, nm in enumerate(theme_names):\n",
    "            p,u,n = sentiment_ratio(groups[i])\n",
    "            sentiments[nm] = {\"positive\": p, \"neutral\": u, \"negative\": n}\n",
    "\n",
    "        # 图表\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] Cluster overview: {img}\")\n",
    "\n",
    "        # 导出 CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] Cluster keywords file: {kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] Cluster sentiment file: {sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] Documents < 20, skip clustering/sentiment.\")\n",
    "\n",
    "    # 月度情感走势\n",
    "    csv_out, img_out = monthly_sentiment(df)\n",
    "    if csv_out:\n",
    "        print(f\"[OK] Monthly sentiment file: {csv_out}\")\n",
    "        print(f\"[OK] Monthly sentiment chart: {img_out}\")\n",
    "    else:\n",
    "        print(\"[INFO] Monthly sentiment not generated (timestamp missing or <2 months of samples).\")\n",
    "\n",
    "    # 摘要\n",
    "    print(\"\\nTop 10 Words:\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\nDone. Output directory: analysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T19:55:48.248807Z",
     "end_time": "2025-09-06T20:05:36.351056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Danmaku Sentiment Analysis (SnowNLP, 5-level buckets)\n",
      "================================================================\n",
      "[INFO] Using cleaned CSV: combined_cleaned_danmu_202509061738.csv\n",
      "[INFO] Loaded 86959 cleaned danmaku lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|██████████| 86959/86959 [04:18<00:00, 336.73danmaku/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY]\n",
      "- N = 86959 | Average = 0.4837\n",
      "   Very Negative:  27755  (31.9%)\n",
      "        Negative:   8403  ( 9.7%)\n",
      "         Neutral:  19240  (22.1%)\n",
      "        Positive:   7003  ( 8.1%)\n",
      "   Very Positive:  24558  (28.2%)\n",
      "\n",
      "[FILES]\n",
      "- scored_csv: sentiment_results\\sentiment_scored_202509061806.csv\n",
      "- extremes_txt: sentiment_results\\sentiment_extremes_202509061806.txt\n",
      "- bar_html: sentiment_results\\sentiment_distribution_202509061806.html\n",
      "- hist_png: sentiment_results\\sentiment_histogram_202509061806.png\n",
      "- summary_json: sentiment_results\\sentiment_summary_202509061806.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sentiment Analysis for Cleaned Bilibili Danmaku\n",
    "- Input: latest cleaned file from cleaned_danmu_results (CSV preferred; TXT fallback)\n",
    "- Scoring: SnowNLP (0..1, higher = more positive)\n",
    "- Buckets (five-level):\n",
    "    Very Negative (<0.3) | Negative (0.3-0.4) | Neutral (0.4-0.6) | Positive (0.6-0.7) | Very Positive (>0.7)\n",
    "- Outputs (sentiment_results/):\n",
    "    * Interactive bar chart (HTML, pyecharts)\n",
    "    * Static histogram (PNG, matplotlib)\n",
    "    * Scored table CSV (text, score, label)\n",
    "    * Summary JSON\n",
    "    * Extreme-sample snippets (most negative / most positive)\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "from pyecharts.charts import Bar\n",
    "from pyecharts import options as opts\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------- Paths -----------------------\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"sentiment_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------- Buckets -----------------------\n",
    "# Boundaries reflect the spec precisely.\n",
    "BUCKETS = [\n",
    "    (\"Very Negative\", lambda s: s < 0.3),\n",
    "    (\"Negative\",      lambda s: 0.3 <= s < 0.4),\n",
    "    (\"Neutral\",       lambda s: 0.4 <= s <= 0.6),   # inclusive both ends\n",
    "    (\"Positive\",      lambda s: 0.6 <  s <= 0.7),\n",
    "    (\"Very Positive\", lambda s: s > 0.7),\n",
    "]\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    \"\"\"Prefer CSV, fallback to TXT.\"\"\"\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    if csvs:\n",
    "        latest = max(csvs, key=os.path.getmtime)\n",
    "        print(f\"[INFO] Using cleaned CSV: {os.path.basename(latest)}\")\n",
    "        return latest\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    if txts:\n",
    "        latest = max(txts, key=os.path.getmtime)\n",
    "        print(f\"[INFO] Using cleaned TXT: {os.path.basename(latest)}\")\n",
    "        return latest\n",
    "    print(\"[ERROR] No cleaned file found. Run the cleaning step first.\")\n",
    "    return None\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    \"\"\"Load 'cleaned' texts (CSV -> 'cleaned' column; TXT -> lines).\"\"\"\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        if \"cleaned\" not in df.columns:\n",
    "            raise ValueError(\"Column 'cleaned' not found in CSV.\")\n",
    "        lines = [str(x).strip() for x in df[\"cleaned\"].fillna(\"\")]\n",
    "    else:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    # Filter out blanks\n",
    "    return [s for s in lines if s]\n",
    "\n",
    "def score_texts(lines, show_progress=True):\n",
    "    \"\"\"SnowNLP scoring with safety fallback.\"\"\"\n",
    "    scores = []\n",
    "    iterator = tqdm(lines, desc=\"Scoring\", unit=\"danmaku\") if show_progress else lines\n",
    "    for t in iterator:\n",
    "        try:\n",
    "            scores.append(float(SnowNLP(t).sentiments))\n",
    "        except Exception:\n",
    "            scores.append(0.5)  # neutral fallback\n",
    "    return scores\n",
    "\n",
    "def bucketize(scores):\n",
    "    \"\"\"Count per 5-level bucket.\"\"\"\n",
    "    labels = [b[0] for b in BUCKETS]\n",
    "    counts = [0]*len(BUCKETS)\n",
    "    for s in scores:\n",
    "        for i, (_, cond) in enumerate(BUCKETS):\n",
    "            if cond(s):\n",
    "                counts[i] += 1\n",
    "                break\n",
    "    return labels, counts\n",
    "\n",
    "def export_scored_table(lines, scores, ts):\n",
    "    \"\"\"Save per-text scores & labels to CSV for downstream analysis.\"\"\"\n",
    "    def label_of(s):\n",
    "        for name, cond in BUCKETS:\n",
    "            if cond(s): return name\n",
    "        return \"Unknown\"\n",
    "    df = pd.DataFrame({\"text\": lines, \"score\": scores, \"label\": [label_of(s) for s in scores]})\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_scored_{ts}.csv\")\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    return out, df\n",
    "\n",
    "def export_extremes(df, ts, k=30):\n",
    "    \"\"\"Save top-K most negative/positive samples (for qualitative quotes).\"\"\"\n",
    "    neg = df.sort_values(\"score\", ascending=True).head(k)\n",
    "    pos = df.sort_values(\"score\", ascending=False).head(k)\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_extremes_{ts}.txt\")\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Most Negative Samples ===\\n\")\n",
    "        for i, r in neg.iterrows():\n",
    "            f.write(f\"[{r['score']:.4f}] {r['text']}\\n\")\n",
    "        f.write(\"\\n=== Most Positive Samples ===\\n\")\n",
    "        for i, r in pos.iterrows():\n",
    "            f.write(f\"[{r['score']:.4f}] {r['text']}\\n\")\n",
    "    return out\n",
    "\n",
    "def bar_chart_html(labels, counts, avg_score, ts):\n",
    "    \"\"\"Interactive pyecharts bar.\"\"\"\n",
    "    bar = (\n",
    "        Bar()\n",
    "        .add_xaxis(labels)\n",
    "        .add_yaxis(\"Count\", counts)\n",
    "        .set_global_opts(\n",
    "            title_opts=opts.TitleOpts(\n",
    "                title=\"Danmaku Sentiment Distribution\",\n",
    "                subtitle=f\"Average Sentiment: {avg_score:.4f} | N={sum(counts)}\"\n",
    "            ),\n",
    "            xaxis_opts=opts.AxisOpts(name=\"Category\"),\n",
    "            yaxis_opts=opts.AxisOpts(name=\"Count\"),\n",
    "            toolbox_opts=opts.ToolboxOpts(),\n",
    "            datazoom_opts=[opts.DataZoomOpts()]\n",
    "        )\n",
    "        .set_series_opts(\n",
    "            label_opts=opts.LabelOpts(is_show=True),\n",
    "            markpoint_opts=opts.MarkPointOpts(data=[opts.MarkPointItem(type_=\"max\", name=\"Max\")]),\n",
    "        )\n",
    "    )\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_distribution_{ts}.html\")\n",
    "    bar.render(out)\n",
    "    return out\n",
    "\n",
    "def histogram_png(scores, avg_score, ts):\n",
    "    \"\"\"Static histogram with region shading.\"\"\"\n",
    "    plt.figure(figsize=(12,7))\n",
    "    n, bins, _ = plt.hist(scores, bins=30, edgecolor='black', alpha=0.75)\n",
    "    plt.axvline(x=avg_score, color='red', linestyle='--', linewidth=2, label=f'Average = {avg_score:.4f}')\n",
    "    # Shade regions for the five levels\n",
    "    plt.axvspan(0.0, 0.3, color='red', alpha=0.10)\n",
    "    plt.axvspan(0.3, 0.4, color='orange', alpha=0.10)\n",
    "    plt.axvspan(0.4, 0.6, color='gold', alpha=0.10)\n",
    "    plt.axvspan(0.6, 0.7, color='lightgreen', alpha=0.10)\n",
    "    plt.axvspan(0.7, 1.0, color='green', alpha=0.10)\n",
    "\n",
    "    plt.title(\"Distribution of Sentiment Scores\", fontsize=14)\n",
    "    plt.xlabel(\"Score (0=negative, 1=positive)\", fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_histogram_{ts}.png\")\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=300); plt.close()\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*64)\n",
    "    print(\"Danmaku Sentiment Analysis (SnowNLP, 5-level buckets)\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    path = find_latest_cleaned_file()\n",
    "    if not path: return\n",
    "\n",
    "    lines = load_cleaned_lines(path)\n",
    "    print(f\"[INFO] Loaded {len(lines)} cleaned danmaku lines\")\n",
    "\n",
    "    scores = score_texts(lines, show_progress=True)\n",
    "    avg = float(np.mean(scores)) if scores else 0.5\n",
    "    labels, counts = bucketize(scores)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # Exports\n",
    "    scored_csv, df_scored = export_scored_table(lines, scores, ts)\n",
    "    extremes_txt = export_extremes(df_scored, ts, k=30)\n",
    "    html_bar = bar_chart_html(labels, counts, avg, ts)\n",
    "    hist_png = histogram_png(scores, avg, ts)\n",
    "\n",
    "    # Summary JSON (for paper/appendix reuse)\n",
    "    summary = {\n",
    "        \"n\": int(len(scores)),\n",
    "        \"average\": round(avg, 6),\n",
    "        \"buckets\": {labels[i]: int(counts[i]) for i in range(len(labels))},\n",
    "        \"ratios\": {labels[i]: round(counts[i]/max(1,len(scores)), 6) for i in range(len(labels))},\n",
    "        \"files\": {\n",
    "            \"scored_csv\": os.path.basename(scored_csv),\n",
    "            \"extremes_txt\": os.path.basename(extremes_txt),\n",
    "            \"bar_html\": os.path.basename(html_bar),\n",
    "            \"hist_png\": os.path.basename(hist_png),\n",
    "        }\n",
    "    }\n",
    "    json_path = os.path.join(OUT_DIR, f\"sentiment_summary_{ts}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Console report\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    print(f\"- N = {summary['n']} | Average = {summary['average']:.4f}\")\n",
    "    for k in labels:\n",
    "        c = summary[\"buckets\"][k]\n",
    "        r = summary[\"ratios\"][k]\n",
    "        print(f\"  {k:>14s}: {c:6d}  ({r*100:4.1f}%)\")\n",
    "    print(\"\\n[FILES]\")\n",
    "    for k, v in summary[\"files\"].items():\n",
    "        print(f\"- {k}: {os.path.join(OUT_DIR, v)}\")\n",
    "    print(f\"- summary_json: {json_path}\")\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T18:02:24.446972Z",
     "end_time": "2025-09-06T18:06:44.978753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
