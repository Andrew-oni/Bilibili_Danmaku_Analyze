{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-09-06T17:20:12.763227Z",
     "end_time": "2025-09-06T17:21:05.031422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Bç«™å¼¹å¹•æ‰¹é‡æŠ“å–ï¼ˆæŒ‰ url.txt åˆ—è¡¨ï¼›timestamp=è§†é¢‘å‘å¸ƒæ—¥æœŸï¼‰\n",
      "======================================================================\n",
      "\n",
      "=== å¤„ç† BV1iR8MzkEpc ===\n",
      "[META] æ ‡é¢˜: é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ åŸå¸‚äº‹æ•…ç‚¼ç‹± 26è½¦éƒ½è°ä¸èƒ½å›å®¶ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-24\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ åŸå¸‚äº‹æ•…ç‚¼ç‹± 26è½¦éƒ½è°ä¸èƒ½å›å®¶ï¼Ÿ (cid=31247631170) â€¦\n",
      "    é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ åŸå¸‚äº‹æ•…ç‚¼ç‹± 26è½¦éƒ½è°ä¸èƒ½å›å®¶ï¼Ÿ æŠ“åˆ° 28839 æ¡\n",
      "[ERROR] å¤„ç† BV1iR8MzkEpc å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1iR8MzkEpc_31247631170_é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ åŸå¸‚äº‹æ•…ç‚¼ç‹± 26è½¦éƒ½è°ä¸èƒ½å›å®¶ï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV1vz8FzDEyE ===\n",
      "[META] æ ‡é¢˜: å…¨çƒé¦–æ¬¡ é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ 36è¾†è¾…åŠ©é©¾é©¶é«˜é€Ÿäº‹æ•…æå‘½ ä½ æ•¢æŠŠå‘½äº¤ç»™è½¦å—ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-23\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å…¨çƒé¦–æ¬¡ é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ 36è¾†è¾…åŠ©é©¾é©¶é«˜é€Ÿäº‹æ•…æå‘½ ä½ æ•¢æŠŠå‘½äº¤ç»™è½¦å—ï¼Ÿ (cid=31226333587) â€¦\n",
      "    å…¨çƒé¦–æ¬¡ é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ 36è¾†è¾…åŠ©é©¾é©¶é«˜é€Ÿäº‹æ•…æå‘½ ä½ æ•¢æŠŠå‘½äº¤ç»™è½¦å—ï¼Ÿ æŠ“åˆ° 15717 æ¡\n",
      "[ERROR] å¤„ç† BV1vz8FzDEyE å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1vz8FzDEyE_31226333587_å…¨çƒé¦–æ¬¡ é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ 36è¾†è¾…åŠ©é©¾é©¶é«˜é€Ÿäº‹æ•…æå‘½ ä½ æ•¢æŠŠå‘½äº¤ç»™è½¦å—ï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV19T8wznEHB ===\n",
      "[META] æ ‡é¢˜: å¤ç°é«˜é€Ÿæ–½å·¥é©¾é©¶è¾…åŠ©äº‹æ•… é—®ç•Œ/å°ç±³/ç†æƒ³ç­‰ è°ä¼šé€‰æ‹©æ’å¡è½¦è‡ªæ–­AæŸ±ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-23\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å¤ç°é«˜é€Ÿæ–½å·¥é©¾é©¶è¾…åŠ©äº‹æ•… é—®ç•Œ/å°ç±³/ç†æƒ³ç­‰ è°ä¼šé€‰æ‹©æ’å¡è½¦è‡ªæ–­AæŸ±ï¼Ÿ (cid=31234132454) â€¦\n",
      "    å¤ç°é«˜é€Ÿæ–½å·¥é©¾é©¶è¾…åŠ©äº‹æ•… é—®ç•Œ/å°ç±³/ç†æƒ³ç­‰ è°ä¼šé€‰æ‹©æ’å¡è½¦è‡ªæ–­AæŸ±ï¼Ÿ æŠ“åˆ° 18037 æ¡\n",
      "[ERROR] å¤„ç† BV19T8wznEHB å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV19T8wznEHB_31234132454_å¤ç°é«˜é€Ÿæ–½å·¥é©¾é©¶è¾…åŠ©äº‹æ•… é—®ç•Œ/å°ç±³/ç†æƒ³ç­‰ è°ä¼šé€‰æ‹©æ’å¡è½¦è‡ªæ–­AæŸ±ï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV1tftdztEs9 ===\n",
      "[META] æ ‡é¢˜: å°ç±³YU7çš„1000ä¸‡clipsè¾…åŠ©é©¾é©¶ï¼Œæˆé•¿åˆ°ä»€ä¹ˆåœ°æ­¥äº†ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-08-12\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å°ç±³YU7çš„1000ä¸‡clipsè¾…åŠ©é©¾é©¶ï¼Œæˆé•¿åˆ°ä»€ä¹ˆåœ°æ­¥äº†ï¼Ÿ (cid=31796496131) â€¦\n",
      "    å°ç±³YU7çš„1000ä¸‡clipsè¾…åŠ©é©¾é©¶ï¼Œæˆé•¿åˆ°ä»€ä¹ˆåœ°æ­¥äº†ï¼Ÿ æŠ“åˆ° 1298 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1298 æ¡ â†’ combined_raw_BV1tftdztEs9_202509061720.csv / combined_raw_BV1tftdztEs9_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1jkbDzZEbo ===\n",
      "[META] æ ‡é¢˜: é«˜é€ŸæŒ‘æˆ˜å‡çº§  é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ èƒ½å¦åœ¨ç»å¢ƒä¸­è·å–ç”Ÿæœºï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-25\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é«˜é€ŸæŒ‘æˆ˜å‡çº§  é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ èƒ½å¦åœ¨ç»å¢ƒä¸­è·å–ç”Ÿæœºï¼Ÿ (cid=31265065994) â€¦\n",
      "    é«˜é€ŸæŒ‘æˆ˜å‡çº§  é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ èƒ½å¦åœ¨ç»å¢ƒä¸­è·å–ç”Ÿæœºï¼Ÿ æŠ“åˆ° 9204 æ¡\n",
      "[ERROR] å¤„ç† BV1jkbDzZEbo å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1jkbDzZEbo_31265065994_é«˜é€ŸæŒ‘æˆ˜å‡çº§  é—®ç•Œ/ç†æƒ³/å°ç±³/ç‰¹æ–¯æ‹‰ èƒ½å¦åœ¨ç»å¢ƒä¸­è·å–ç”Ÿæœºï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV1bK4y1P7Ba ===\n",
      "[META] æ ‡é¢˜: å¹²ç¿»ç‰¹æ–¯æ‹‰ï¼Ÿç™¾åº¦Apolloè‡ªåŠ¨é©¾é©¶è¾…åŠ©ä½“éªŒ | å‘å¸ƒæ—¥æœŸ: 2021-04-21\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– æˆç‰‡v2 (cid=327215289) â€¦\n",
      "    æˆç‰‡v2 æŠ“åˆ° 7777 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 7777 æ¡ â†’ combined_raw_BV1bK4y1P7Ba_202509061720.csv / combined_raw_BV1bK4y1P7Ba_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1xP8AzGEjc ===\n",
      "[META] æ ‡é¢˜: â€œç›´åˆ°ç¢°æ’å‰æœ€åä¸€ç§’ï¼Œæˆ‘éƒ½æ˜¯ç›¸ä¿¡å®ƒçš„â€ï¼Œâ€œæ™ºé©¾â€äº‹æ•…çš„èƒŒåï¼Œæœ‰é—®é¢˜å—ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-24\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– â€œç›´åˆ°ç¢°æ’å‰æœ€åä¸€ç§’ï¼Œæˆ‘éƒ½æ˜¯ç›¸ä¿¡å®ƒçš„â€ï¼Œâ€œæ™ºé©¾â€äº‹æ•…çš„èƒŒåï¼Œæœ‰é—®é¢˜å—ï¼Ÿ (cid=31231970391) â€¦\n",
      "    â€œç›´åˆ°ç¢°æ’å‰æœ€åä¸€ç§’ï¼Œæˆ‘éƒ½æ˜¯ç›¸ä¿¡å®ƒçš„â€ï¼Œâ€œæ™ºé©¾â€äº‹æ•…çš„èƒŒåï¼Œæœ‰é—®é¢˜å—ï¼Ÿ æŠ“åˆ° 6488 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 6488 æ¡ â†’ combined_raw_BV1xP8AzGEjc_202509061720.csv / combined_raw_BV1xP8AzGEjc_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1Xv4YeGED3 ===\n",
      "[META] æ ‡é¢˜: é£æœºå·¥ç¨‹å¸ˆï¼Œå¸®æˆ‘çš„å¸è±ªæ”¹è‡ªåŠ¨é©¾é©¶ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2024-09-11\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é£æœºå·¥ç¨‹å¸ˆï¼Œå¸®æˆ‘çš„å¸è±ªæ”¹è‡ªåŠ¨é©¾é©¶ï¼Ÿ (cid=25830033977) â€¦\n",
      "    é£æœºå·¥ç¨‹å¸ˆï¼Œå¸®æˆ‘çš„å¸è±ªæ”¹è‡ªåŠ¨é©¾é©¶ï¼Ÿ æŠ“åˆ° 7376 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 7376 æ¡ â†’ combined_raw_BV1Xv4YeGED3_202509061720.csv / combined_raw_BV1Xv4YeGED3_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1Sc9sYoEvG ===\n",
      "[META] æ ‡é¢˜: å°ä¼™å¼€è‡ªåŠ¨é©¾é©¶åæ”¾å¿ƒç¡ç€ï¼Œç»“æœæ’ä¸Šæ”¶è´¹ç«™ç¿»æ»š2åœˆï¼ã€1671æœŸã€‘ | å‘å¸ƒæ—¥æœŸ: 2025-03-05\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å°ä¼™å¼€è‡ªåŠ¨é©¾é©¶åæ”¾å¿ƒç¡ç€ï¼Œç»“æœæ’ä¸Šæ”¶è´¹ç«™ç¿»æ»š2åœˆï¼ã€1671æœŸã€‘ (cid=28696579943) â€¦\n",
      "    å°ä¼™å¼€è‡ªåŠ¨é©¾é©¶åæ”¾å¿ƒç¡ç€ï¼Œç»“æœæ’ä¸Šæ”¶è´¹ç«™ç¿»æ»š2åœˆï¼ã€1671æœŸã€‘ æŠ“åˆ° 6275 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 6275 æ¡ â†’ combined_raw_BV1Sc9sYoEvG_202509061720.csv / combined_raw_BV1Sc9sYoEvG_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1rb4y1R7Wd ===\n",
      "[META] æ ‡é¢˜: ä½ çœŸçš„ä¼šå¼€ç‰¹æ–¯æ‹‰å—ï¼Ÿç‰¹æ–¯æ‹‰å¤±æ§è½¦è¯„äººä¸ä¼šå‘Šè¯‰ä½ çš„ç§˜å¯†ã€åˆ€å“¥è°ˆè½¦ã€‘ | å‘å¸ƒæ—¥æœŸ: 2021-02-21\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– Produce_6 (cid=300739050) â€¦\n",
      "    Produce_6 æŠ“åˆ° 651 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 651 æ¡ â†’ combined_raw_BV1rb4y1R7Wd_202509061720.csv / combined_raw_BV1rb4y1R7Wd_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1D14y1r73p ===\n",
      "[META] æ ‡é¢˜: åä¸ºï¼šæ¯”æ™ºé©¾å¼€åŸæ˜¯å§ï¼Œå¹´åº•æˆ‘ç›´æ¥å…¨å›½å¯ç”¨ã€é—®ç•Œ M7 æ™ºé©¾ç‰ˆã€‘ | å‘å¸ƒæ—¥æœŸ: 2023-09-12\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– åä¸ºï¼šæ¯”æ™ºé©¾å¼€åŸæ˜¯å§ï¼Œå¹´åº•æˆ‘ç›´æ¥å…¨å›½å¯ç”¨ã€é—®ç•Œ M7 æ™ºé©¾ç‰ˆã€‘ (cid=1265055590) â€¦\n",
      "    åä¸ºï¼šæ¯”æ™ºé©¾å¼€åŸæ˜¯å§ï¼Œå¹´åº•æˆ‘ç›´æ¥å…¨å›½å¯ç”¨ã€é—®ç•Œ M7 æ™ºé©¾ç‰ˆã€‘ æŠ“åˆ° 3774 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 3774 æ¡ â†’ combined_raw_BV1D14y1r73p_202509061720.csv / combined_raw_BV1D14y1r73p_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1BTb1zjEps ===\n",
      "[META] æ ‡é¢˜: é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ é¢å¯¹é«˜é€Ÿæ–½å·¥ 36è½¦é™©äº›å…¨å†›è¦†æ²¡ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-07-25\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ é¢å¯¹é«˜é€Ÿæ–½å·¥ 36è½¦é™©äº›å…¨å†›è¦†æ²¡ï¼Ÿ (cid=31257460826) â€¦\n",
      "    é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ é¢å¯¹é«˜é€Ÿæ–½å·¥ 36è½¦é™©äº›å…¨å†›è¦†æ²¡ï¼Ÿ æŠ“åˆ° 4268 æ¡\n",
      "[ERROR] å¤„ç† BV1BTb1zjEps å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1BTb1zjEps_31257460826_é—®ç•Œ/ç†æƒ³/å°ç±³/ææ°ª/ç‰¹æ–¯æ‹‰ é¢å¯¹é«˜é€Ÿæ–½å·¥ 36è½¦é™©äº›å…¨å†›è¦†æ²¡ï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV1fK4y1g71C ===\n",
      "[META] æ ‡é¢˜: ä¸ºä»€ä¹ˆé€ è½¦æ–°åŠ¿åŠ›ä»¬çš„é¢†èˆªè¾…åŠ©ä¸€ä¸ªéƒ½æ²¡æ³•ç”¨ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2021-06-03\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ä¸ºä»€ä¹ˆé€ è½¦æ–°åŠ¿åŠ›ä»¬çš„é¢†èˆªè¾…åŠ©ä¸€ä¸ªéƒ½æ²¡æ³•ç”¨ï¼Ÿ (cid=348049570) â€¦\n",
      "    ä¸ºä»€ä¹ˆé€ è½¦æ–°åŠ¿åŠ›ä»¬çš„é¢†èˆªè¾…åŠ©ä¸€ä¸ªéƒ½æ²¡æ³•ç”¨ï¼Ÿ æŠ“åˆ° 3814 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 3814 æ¡ â†’ combined_raw_BV1fK4y1g71C_202509061720.csv / combined_raw_BV1fK4y1g71C_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1JT411k77P ===\n",
      "[META] æ ‡é¢˜: æ˜“è½¦æ¨ªè¯„ è”šæ¥ES7/å°é¹G9/é˜¿ç»´å¡”11æ¨ªè¯„ï¼è¾…åŠ©é©¾é©¶ç¥ä»™æ‰“æ¶ï¼Ÿå…¨å‘˜ç»­èˆªæ‰“å¯¹æŠ˜ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2023-03-17\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– æ˜“è½¦æ¨ªè¯„ è”šæ¥ES7/å°é¹G9/é˜¿ç»´å¡”11æ¨ªè¯„ï¼è¾…åŠ©é©¾é©¶ç¥ä»™æ‰“æ¶ï¼Ÿå…¨å‘˜ç»­èˆªæ‰“å¯¹æŠ˜ï¼Ÿ (cid=1055606894) â€¦\n",
      "    æ˜“è½¦æ¨ªè¯„ è”šæ¥ES7/å°é¹G9/é˜¿ç»´å¡”11æ¨ªè¯„ï¼è¾…åŠ©é©¾é©¶ç¥ä»™æ‰“æ¶ï¼Ÿå…¨å‘˜ç»­èˆªæ‰“å¯¹æŠ˜ï¼Ÿ æŠ“åˆ° 3645 æ¡\n",
      "[ERROR] å¤„ç† BV1JT411k77P å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1JT411k77P_1055606894_æ˜“è½¦æ¨ªè¯„ è”šæ¥ES7/å°é¹G9/é˜¿ç»´å¡”11æ¨ªè¯„ï¼è¾…åŠ©é©¾é©¶ç¥ä»™æ‰“æ¶ï¼Ÿå…¨å‘˜ç»­èˆªæ‰“å¯¹æŠ˜ï¼Ÿ.csv'\n",
      "\n",
      "=== å¤„ç† BV191421m7zx ===\n",
      "[META] æ ‡é¢˜: éª—è¿‡è½¦æœºï¼çº¯è§†è§‰çš„ç‰¹æ–¯æ‹‰ï¼Œçœ¼ç¥çœŸçš„ä¸å¤ªå¥½ï¼æˆ‘è‡ªå·±æ˜¯ä¸æ•¢ç”¨APäº† | å‘å¸ƒæ—¥æœŸ: 2024-04-08\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– éª—è¿‡è½¦æœºï¼çº¯è§†è§‰çš„ç‰¹æ–¯æ‹‰ï¼Œçœ¼ç¥çœŸçš„ä¸å¤ªå¥½ï¼æˆ‘è‡ªå·±æ˜¯ä¸æ•¢ç”¨APäº† (cid=1498402504) â€¦\n",
      "    éª—è¿‡è½¦æœºï¼çº¯è§†è§‰çš„ç‰¹æ–¯æ‹‰ï¼Œçœ¼ç¥çœŸçš„ä¸å¤ªå¥½ï¼æˆ‘è‡ªå·±æ˜¯ä¸æ•¢ç”¨APäº† æŠ“åˆ° 3592 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 3592 æ¡ â†’ combined_raw_BV191421m7zx_202509061720.csv / combined_raw_BV191421m7zx_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1xX4y1H7F3 ===\n",
      "[META] æ ‡é¢˜: â€œåä¸ºâ€é€ è½¦ï¼Œé¥é¥é¢†å…ˆï¼Ÿå…¨æ–¹ä½å®æµ‹è¾…åŠ©é©¾é©¶+AEBï¼šé—®ç•ŒM5 PK å°é¹G6 | å‘å¸ƒæ—¥æœŸ: 2023-07-06\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– â€œåä¸ºâ€é€ è½¦ï¼Œé¥é¥é¢†å…ˆï¼Ÿå…¨æ–¹ä½å®æµ‹è¾…åŠ©é©¾é©¶+AEBï¼šé—®ç•ŒM5 PK å°é¹G6 (cid=1187210517) â€¦\n",
      "    â€œåä¸ºâ€é€ è½¦ï¼Œé¥é¥é¢†å…ˆï¼Ÿå…¨æ–¹ä½å®æµ‹è¾…åŠ©é©¾é©¶+AEBï¼šé—®ç•ŒM5 PK å°é¹G6 æŠ“åˆ° 3509 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 3509 æ¡ â†’ combined_raw_BV1xX4y1H7F3_202509061720.csv / combined_raw_BV1xX4y1H7F3_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1Wg411k7Tp ===\n",
      "[META] æ ‡é¢˜: 915æœŸï¼šå°é¹P7é«˜æ¶80é€Ÿå¤±æ§æ’æ­»ä¿®è½¦äººå‘˜ï¼Œé©¾é©¶å‘˜ï¼šæˆ‘å¼€å¯äº†è¾…åŠ©é©¾é©¶ã€‚ | å‘å¸ƒæ—¥æœŸ: 2022-08-13\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– åºåˆ— 01_1 (cid=800361433) â€¦\n",
      "    åºåˆ— 01_1 æŠ“åˆ° 2660 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2660 æ¡ â†’ combined_raw_BV1Wg411k7Tp_202509061720.csv / combined_raw_BV1Wg411k7Tp_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1gD4y1g7iC ===\n",
      "[META] æ ‡é¢˜: ã€42Markã€‘æˆ‘èŠ±å‡ ä¸‡å—ä¹°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šä¸‹ç­é€šå‹¤æ›´è½»æ¾å—ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2023-03-03\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ã€42Markã€‘æˆ‘èŠ±å‡ ä¸‡å—ä¹°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šä¸‹ç­é€šå‹¤æ›´è½»æ¾å—ï¼Ÿ (cid=1036115057) â€¦\n",
      "    ã€42Markã€‘æˆ‘èŠ±å‡ ä¸‡å—ä¹°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šä¸‹ç­é€šå‹¤æ›´è½»æ¾å—ï¼Ÿ æŠ“åˆ° 2245 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2245 æ¡ â†’ combined_raw_BV1gD4y1g7iC_202509061720.csv / combined_raw_BV1gD4y1g7iC_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1Y54y1o7LC ===\n",
      "[META] æ ‡é¢˜: ã€42Markã€‘ä¸€å¹´è¿‡å»äº†ï¼Œå¯¼èˆªè¾…åŠ©é©¾é©¶å‘å±•åˆ°ä»€ä¹ˆæ°´å¹³äº†ï¼Ÿï¼ˆä¸Šï¼‰ | å‘å¸ƒæ—¥æœŸ: 2022-05-26\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– 42Mark å¯¼èˆªè¾…åŠ©é©¾é©¶ä¸Š 4k (cid=729887454) â€¦\n",
      "    42Mark å¯¼èˆªè¾…åŠ©é©¾é©¶ä¸Š 4k æŠ“åˆ° 2271 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2271 æ¡ â†’ combined_raw_BV1Y54y1o7LC_202509061720.csv / combined_raw_BV1Y54y1o7LC_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV16y411i7kQ ===\n",
      "[META] æ ‡é¢˜: æ²¡æœ‰åä¸º ADS 3.0 ä¸èƒ½å¼€çš„è·¯ï¼Ÿï¼äº«ç•Œ S9 æš´é›¨å¤©è¾…åŠ©é©¾é©¶æé™æµ‹è¯•ï¼ | å‘å¸ƒæ—¥æœŸ: 2024-08-02\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– æ²¡æœ‰åä¸º ADS 3.0 ä¸èƒ½å¼€çš„è·¯ï¼Ÿï¼äº«ç•Œ S9 æš´é›¨å¤©è¾…åŠ©é©¾é©¶æé™æµ‹è¯•ï¼ (cid=1635069917) â€¦\n",
      "    æ²¡æœ‰åä¸º ADS 3.0 ä¸èƒ½å¼€çš„è·¯ï¼Ÿï¼äº«ç•Œ S9 æš´é›¨å¤©è¾…åŠ©é©¾é©¶æé™æµ‹è¯•ï¼ æŠ“åˆ° 1800 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1800 æ¡ â†’ combined_raw_BV16y411i7kQ_202509061720.csv / combined_raw_BV16y411i7kQ_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1GB4y1e7LJ ===\n",
      "[META] æ ‡é¢˜: æç‹é˜¿å°”æ³• S Hi ç‰ˆè¾…åŠ©é©¾é©¶ä½“éªŒï¼åä¸ºåˆåˆ›é€ äº†ä¸€ä¸ªå¤©èŠ±æ¿ï¼Ÿï¼ | å‘å¸ƒæ—¥æœŸ: 2022-07-23\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– æç‹é˜¿å°”æ³• S Hi ç‰ˆè¾…åŠ©é©¾é©¶ä½“éªŒï¼åä¸ºåˆåˆ›é€ äº†ä¸€ä¸ªå¤©èŠ±æ¿ï¼Ÿï¼ (cid=781950203) â€¦\n",
      "    æç‹é˜¿å°”æ³• S Hi ç‰ˆè¾…åŠ©é©¾é©¶ä½“éªŒï¼åä¸ºåˆåˆ›é€ äº†ä¸€ä¸ªå¤©èŠ±æ¿ï¼Ÿï¼ æŠ“åˆ° 1352 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1352 æ¡ â†’ combined_raw_BV1GB4y1e7LJ_202509061720.csv / combined_raw_BV1GB4y1e7LJ_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1VQ5EzhEcm ===\n",
      "[META] æ ‡é¢˜: å…¨ç½‘é¦–å‘ï¼8å¤§çƒ­é—¨è½¦ï¼Œ100kmåŸåŒºè¾…åŠ©æ™ºé©¾æ¨ªè¯„ï¼Œåä¸ºã€ç‰¹æ–¯æ‹‰ã€å°ç±³ã€ç†æƒ³è°æ›´é è°±ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-04-18\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å…¨ç½‘é¦–å‘ï¼8å¤§çƒ­é—¨è½¦ï¼Œ100kmåŸåŒºè¾…åŠ©æ™ºé©¾æ¨ªè¯„ï¼Œåä¸ºã€ç‰¹æ–¯æ‹‰ã€å°ç±³ã€ç†æƒ³è°æ›´é è°±ï¼Ÿ (cid=29485828966) â€¦\n",
      "    å…¨ç½‘é¦–å‘ï¼8å¤§çƒ­é—¨è½¦ï¼Œ100kmåŸåŒºè¾…åŠ©æ™ºé©¾æ¨ªè¯„ï¼Œåä¸ºã€ç‰¹æ–¯æ‹‰ã€å°ç±³ã€ç†æƒ³è°æ›´é è°±ï¼Ÿ æŠ“åˆ° 2179 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2179 æ¡ â†’ combined_raw_BV1VQ5EzhEcm_202509061720.csv / combined_raw_BV1VQ5EzhEcm_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV18Yt1zvEqu ===\n",
      "[META] æ ‡é¢˜: è¾…åŠ©é©¾é©¶å¤§ä¹±æ–—ï¼çº¯è§†è§‰å’Œæ¿€å…‰é›·è¾¾ï¼Œè°æ›´è€å¸æœºï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-08-08\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– è¾…åŠ©é©¾é©¶å¤§ä¹±æ–—ï¼çº¯è§†è§‰å’Œæ¿€å…‰é›·è¾¾ï¼Œè°æ›´è€å¸æœºï¼Ÿ (cid=31567449077) â€¦\n",
      "    è¾…åŠ©é©¾é©¶å¤§ä¹±æ–—ï¼çº¯è§†è§‰å’Œæ¿€å…‰é›·è¾¾ï¼Œè°æ›´è€å¸æœºï¼Ÿ æŠ“åˆ° 1269 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1269 æ¡ â†’ combined_raw_BV18Yt1zvEqu_202509061720.csv / combined_raw_BV18Yt1zvEqu_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1hK4y1k7nx ===\n",
      "[META] æ ‡é¢˜: æœ€å¼ºè¾…åŠ©é©¾é©¶ä¹‹æˆ˜ï¼å®é©¬ç‰¹æ–¯æ‹‰è”šæ¥ç†æƒ³ï¼Œç”µåŠ¨è½¦è¿˜æ˜¯ç‡ƒæ²¹è½¦æ›´èƒœä¸€ç­¹ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2020-04-29\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ADAS æ¨ªè¯„v4 (cid=184580836) â€¦\n",
      "    ADAS æ¨ªè¯„v4 æŠ“åˆ° 2068 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2068 æ¡ â†’ combined_raw_BV1hK4y1k7nx_202509061720.csv / combined_raw_BV1hK4y1k7nx_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1TNPpeCEQX ===\n",
      "[META] æ ‡é¢˜: ç‰¹æ–¯æ‹‰ FSD è¿›å…¥ä¸­å›½ï¼Œä¸­ç¾æ™ºé©¾å¤§å¯¹å†³ | å‘å¸ƒæ—¥æœŸ: 2025-02-27\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ç‰¹æ–¯æ‹‰ FSD è¿›å…¥ä¸­å›½ï¼Œä¸­ç¾æ™ºé©¾å¤§å¯¹å†³ (cid=28598405743) â€¦\n",
      "    ç‰¹æ–¯æ‹‰ FSD è¿›å…¥ä¸­å›½ï¼Œä¸­ç¾æ™ºé©¾å¤§å¯¹å†³ æŠ“åˆ° 1852 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1852 æ¡ â†’ combined_raw_BV1TNPpeCEQX_202509061720.csv / combined_raw_BV1TNPpeCEQX_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1KuP3e3E5M ===\n",
      "[META] æ ‡é¢˜: ç‰¹æ–¯æ‹‰ FSD ä¸Šçº¿é¦–æ—¥ï¼Œä¸Šæµ·è¡—å¤´å¤œè€ƒï¼Œåˆ°åº•å¥½ä¸å¥½ä½¿ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-02-26\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ç‰¹æ–¯æ‹‰ FSD ä¸Šçº¿é¦–æ—¥ï¼Œä¸Šæµ·è¡—å¤´å¤œè€ƒï¼Œåˆ°åº•å¥½ä¸å¥½ä½¿ï¼Ÿ (cid=28578351159) â€¦\n",
      "    ç‰¹æ–¯æ‹‰ FSD ä¸Šçº¿é¦–æ—¥ï¼Œä¸Šæµ·è¡—å¤´å¤œè€ƒï¼Œåˆ°åº•å¥½ä¸å¥½ä½¿ï¼Ÿ æŠ“åˆ° 1529 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1529 æ¡ â†’ combined_raw_BV1KuP3e3E5M_202509061720.csv / combined_raw_BV1KuP3e3E5M_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1GD5DzHEJo ===\n",
      "[META] æ ‡é¢˜: 2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-04-23\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– 2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ (cid=29546709885) â€¦\n",
      "    2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ æŠ“åˆ° 1193 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1193 æ¡ â†’ combined_raw_BV1GD5DzHEJo_202509061720.csv / combined_raw_BV1GD5DzHEJo_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1NP411h7hm ===\n",
      "[META] æ ‡é¢˜: ä½ æ˜¯æƒ³å–ä»£é©¾é©¶å‘˜å—ï¼Ÿä½“éªŒé—®ç•Œæ–°M7å¤§äº”åº§  åä¸ºæ— å›¾æ™ºé©¾ä¸Šè½¦ï¼ | å‘å¸ƒæ—¥æœŸ: 2023-09-12\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ä½ æ˜¯æƒ³å–ä»£é©¾é©¶å‘˜å—ï¼Ÿä½“éªŒé—®ç•Œæ–°M7å¤§äº”åº§  åä¸ºæ— å›¾æ™ºé©¾ä¸Šè½¦ï¼ (cid=1264963436) â€¦\n",
      "    ä½ æ˜¯æƒ³å–ä»£é©¾é©¶å‘˜å—ï¼Ÿä½“éªŒé—®ç•Œæ–°M7å¤§äº”åº§  åä¸ºæ— å›¾æ™ºé©¾ä¸Šè½¦ï¼ æŠ“åˆ° 1176 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1176 æ¡ â†’ combined_raw_BV1NP411h7hm_202509061720.csv / combined_raw_BV1NP411h7hm_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1A4421f7ek ===\n",
      "[META] æ ‡é¢˜: ç©æ‰‹æœºã€ç¡è§‰ï¼è¾…åŠ©é©¾é©¶åˆ°åº•ç®¡ä¸ç®¡ï¼Ÿ8è½¦æ¨ªè¯„ï¼ | å‘å¸ƒæ—¥æœŸ: 2024-07-25\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ç©æ‰‹æœºã€ç¡è§‰ï¼è¾…åŠ©é©¾é©¶åˆ°åº•ç®¡ä¸ç®¡ï¼Ÿ8è½¦æ¨ªè¯„ï¼ (cid=1626545880) â€¦\n",
      "    ç©æ‰‹æœºã€ç¡è§‰ï¼è¾…åŠ©é©¾é©¶åˆ°åº•ç®¡ä¸ç®¡ï¼Ÿ8è½¦æ¨ªè¯„ï¼ æŠ“åˆ° 1183 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1183 æ¡ â†’ combined_raw_BV1A4421f7ek_202509061720.csv / combined_raw_BV1A4421f7ek_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1PVPpeMEGz ===\n",
      "[META] æ ‡é¢˜: ç‰¹æ–¯æ‹‰fsdè¾…åŠ©é©¾é©¶é‡åº†æ—©é«˜å³°ä½“éªŒï¼Œæ±ŸåŒ—åŒºå¼€å¾€æ¸ä¸­åŒºï¼ä¸€åˆ€ä¸å‰ª | å‘å¸ƒæ—¥æœŸ: 2025-02-27\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ç‰¹æ–¯æ‹‰fsdè¾…åŠ©é©¾é©¶é‡åº†æ—©é«˜å³°ä½“éªŒï¼Œæ±ŸåŒ—åŒºå¼€å¾€æ¸ä¸­åŒºï¼ä¸€åˆ€ä¸å‰ª (cid=28597028690) â€¦\n",
      "    ç‰¹æ–¯æ‹‰fsdè¾…åŠ©é©¾é©¶é‡åº†æ—©é«˜å³°ä½“éªŒï¼Œæ±ŸåŒ—åŒºå¼€å¾€æ¸ä¸­åŒºï¼ä¸€åˆ€ä¸å‰ª æŠ“åˆ° 1481 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1481 æ¡ â†’ combined_raw_BV1PVPpeMEGz_202509061720.csv / combined_raw_BV1PVPpeMEGz_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1gEvNzuEDq ===\n",
      "[META] æ ‡é¢˜: å°ç±³/å°é¹/ç†æƒ³/é—®ç•Œ è°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šç­ä¸è¿Ÿåˆ°ï¼Ÿã€Œå¿«é£ã€ | å‘å¸ƒæ—¥æœŸ: 2025-08-26\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å°ç±³/å°é¹/ç†æƒ³/é—®ç•Œ è°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šç­ä¸è¿Ÿåˆ°ï¼Ÿã€Œå¿«é£ã€ (cid=31955617882) â€¦\n",
      "    å°ç±³/å°é¹/ç†æƒ³/é—®ç•Œ è°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šç­ä¸è¿Ÿåˆ°ï¼Ÿã€Œå¿«é£ã€ æŠ“åˆ° 1037 æ¡\n",
      "[ERROR] å¤„ç† BV1gEvNzuEDq å¤±è´¥ï¼š[Errno 2] No such file or directory: 'C:\\\\Users\\\\Andrew\\\\Desktop\\\\homework\\\\cc\\\\bili_danmu_results\\\\BV1gEvNzuEDq_31955617882_å°ç±³/å°é¹/ç†æƒ³/é—®ç•Œ è°çš„è¾…åŠ©é©¾é©¶èƒ½è®©æˆ‘ä¸Šç­ä¸è¿Ÿåˆ°ï¼Ÿã€Œå¿«é£ã€.csv'\n",
      "\n",
      "=== å¤„ç† BV12h411i73Y ===\n",
      "[META] æ ‡é¢˜: è‡ªåŠ¨é©¾é©¶è¾…åŠ©ç³»ç»ŸçœŸçš„æ¯”äººç±»é©¾é©¶æ›´å®‰å…¨å—ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2021-08-17\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– è‡ªåŠ¨é©¾é©¶è¾…åŠ©ç³»ç»ŸçœŸçš„æ¯”äººç±»é©¾é©¶æ›´å®‰å…¨å—ï¼Ÿ (cid=390657546) â€¦\n",
      "    è‡ªåŠ¨é©¾é©¶è¾…åŠ©ç³»ç»ŸçœŸçš„æ¯”äººç±»é©¾é©¶æ›´å®‰å…¨å—ï¼Ÿ æŠ“åˆ° 1277 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1277 æ¡ â†’ combined_raw_BV12h411i73Y_202509061720.csv / combined_raw_BV12h411i73Y_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1ZaACeDEXU ===\n",
      "[META] æ ‡é¢˜: è‡ªæè…°åŒ…å…¨ç½‘é¦–æµ‹ï¼Œ9.98ä¸‡çš„æ¯”äºšè¿ªè‡ªåŠ¨æ³Šè½¦å¥½ç”¨å—ï¼Ÿã€Œç§¦Lã€ | å‘å¸ƒæ—¥æœŸ: 2025-02-23\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– è‡ªæè…°åŒ…å…¨ç½‘é¦–æµ‹ï¼Œ9.98ä¸‡çš„æ¯”äºšè¿ªè‡ªåŠ¨æ³Šè½¦å¥½ç”¨å—ï¼Ÿã€Œç§¦Lã€ (cid=28548860733) â€¦\n",
      "    è‡ªæè…°åŒ…å…¨ç½‘é¦–æµ‹ï¼Œ9.98ä¸‡çš„æ¯”äºšè¿ªè‡ªåŠ¨æ³Šè½¦å¥½ç”¨å—ï¼Ÿã€Œç§¦Lã€ æŠ“åˆ° 1523 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1523 æ¡ â†’ combined_raw_BV1ZaACeDEXU_202509061720.csv / combined_raw_BV1ZaACeDEXU_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1WxGwz4EVg ===\n",
      "[META] æ ‡é¢˜: ä¸¤åˆ†é’Ÿçœ‹å®Œæ¯”äºšè¿ªL4çº§æ™ºèƒ½æ³Šè½¦é‡å¤§å®˜å®£ #æ¯”äºšè¿ªç‡å…ˆå®ç°åª²ç¾L4çº§æ™ºèƒ½æ³Šè½¦# #æ¯”äºšè¿ªæ‰¿è¯ºä¸ºæ™ºèƒ½æ³Šè½¦å®‰å…¨å…œåº•# #æ¯”äºšè¿ªå°†è¿æ¥å²ä¸Šæœ€å¤§è§„æ¨¡æ™ºé©¾OTA# | å‘å¸ƒæ—¥æœŸ: 2025-07-09\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– video1752033400968 (cid=30933650317) â€¦\n",
      "    video1752033400968 æŠ“åˆ° 1618 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1618 æ¡ â†’ combined_raw_BV1WxGwz4EVg_202509061720.csv / combined_raw_BV1WxGwz4EVg_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1h5fnYcEyC ===\n",
      "[META] æ ‡é¢˜: å±é™©å±é™©å±é™©ï¼è½¦è¾†â€œè‡ªåŠ¨é©¾é©¶â€ï¼Œå¸æœºç›–è¢«ç¡è§‰ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-01-24\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å±é™©å±é™©å±é™©ï¼è½¦è¾†â€œè‡ªåŠ¨é©¾é©¶â€ï¼Œå¸æœºç›–è¢«ç¡è§‰ï¼Ÿ (cid=28054652505) â€¦\n",
      "    å±é™©å±é™©å±é™©ï¼è½¦è¾†â€œè‡ªåŠ¨é©¾é©¶â€ï¼Œå¸æœºç›–è¢«ç¡è§‰ï¼Ÿ æŠ“åˆ° 1608 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1608 æ¡ â†’ combined_raw_BV1h5fnYcEyC_202509061720.csv / combined_raw_BV1h5fnYcEyC_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1hBuFzfEY5 ===\n",
      "[META] æ ‡é¢˜: ã€å¤§è™¾æ²‰æµ¸å¼è¯•é©¾ã€‘å²šå›¾FREE+ğŸ‘‰æ™ºèƒ½é©¾é©¶Â·åº•ç›˜Â·ç™¾å…¬é‡ŒåŠ é€Ÿå…¨çŸ¥é“ï¼ | å‘å¸ƒæ—¥æœŸ: 2025-07-13\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ã€å¤§è™¾æ²‰æµ¸å¼è¯•é©¾ã€‘å²šå›¾FREE+ğŸ‘‰æ™ºèƒ½é©¾é©¶Â·åº•ç›˜Â·ç™¾å…¬é‡ŒåŠ é€Ÿå…¨çŸ¥é“ï¼ (cid=31011965200) â€¦\n",
      "    ã€å¤§è™¾æ²‰æµ¸å¼è¯•é©¾ã€‘å²šå›¾FREE+ğŸ‘‰æ™ºèƒ½é©¾é©¶Â·åº•ç›˜Â·ç™¾å…¬é‡ŒåŠ é€Ÿå…¨çŸ¥é“ï¼ æŠ“åˆ° 1945 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1945 æ¡ â†’ combined_raw_BV1hBuFzfEY5_202509061720.csv / combined_raw_BV1hBuFzfEY5_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1fcXQYUEik ===\n",
      "[META] æ ‡é¢˜: å…¨ç½‘æœ€å…¨â€œç‰¹æ–¯æ‹‰FSDå¯¹æ¯”åä¸ºADSâ€ï¼Œä¸­ç¾æ™ºé©¾æœ‰å¤šå¤§å·®è·ï¼Ÿã€ç§‘æŠ€ç‹ã€‘ | å‘å¸ƒæ—¥æœŸ: 2025-03-01\n",
      "[META] å…± 3 ä¸ªåˆ†P\n",
      "  - æŠ“å– å…¨ç½‘æœ€å…¨â€œç‰¹æ–¯æ‹‰FSDå¯¹æ¯”åä¸ºADSâ€ï¼Œä¸­ç¾æ™ºé©¾æœ‰å¤šå¤§å·®è·ï¼Ÿã€ç§‘æŠ€ç‹ã€‘ (cid=28632024035) â€¦\n",
      "    å…¨ç½‘æœ€å…¨â€œç‰¹æ–¯æ‹‰FSDå¯¹æ¯”åä¸ºADSâ€ï¼Œä¸­ç¾æ™ºé©¾æœ‰å¤šå¤§å·®è·ï¼Ÿã€ç§‘æŠ€ç‹ã€‘ æŠ“åˆ° 2299 æ¡\n",
      "  - æŠ“å– ç‰¹æ–¯æ‹‰FSDå¤œé—´å¹¿å·éƒŠåŒºåˆ°åŸåŒºä¸€é•œåˆ°åº• (cid=28696839809) â€¦\n",
      "    ç‰¹æ–¯æ‹‰FSDå¤œé—´å¹¿å·éƒŠåŒºåˆ°åŸåŒºä¸€é•œåˆ°åº• æŠ“åˆ° 8 æ¡\n",
      "  - æŠ“å– ç‰¹æ–¯æ‹‰FSDå¹¿å·åŸä¸­æ‘ä¸€é•œåˆ°åº• (cid=28696905721) â€¦\n",
      "    ç‰¹æ–¯æ‹‰FSDå¹¿å·åŸä¸­æ‘ä¸€é•œåˆ°åº• æŠ“åˆ° 5 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2312 æ¡ â†’ combined_raw_BV1fcXQYUEik_202509061720.csv / combined_raw_BV1fcXQYUEik_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1D197YCEsW ===\n",
      "[META] æ ‡é¢˜: å…¨ç½‘å”¯ä¸€æœ€çœŸå®å¯¹æ¯”ï¼Œç‰¹æ–¯æ‹‰ã€åä¸ºä»€ä¹ˆæ°´å¹³ï¼Ÿã€Œç‰¹æ–¯æ‹‰VSåä¸ºã€ | å‘å¸ƒæ—¥æœŸ: 2025-03-04\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å…¨ç½‘å”¯ä¸€æœ€çœŸå®å¯¹æ¯”ï¼Œç‰¹æ–¯æ‹‰ã€åä¸ºä»€ä¹ˆæ°´å¹³ï¼Ÿã€Œç‰¹æ–¯æ‹‰VSåä¸ºã€ (cid=28692122756) â€¦\n",
      "    å…¨ç½‘å”¯ä¸€æœ€çœŸå®å¯¹æ¯”ï¼Œç‰¹æ–¯æ‹‰ã€åä¸ºä»€ä¹ˆæ°´å¹³ï¼Ÿã€Œç‰¹æ–¯æ‹‰VSåä¸ºã€ æŠ“åˆ° 2666 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2666 æ¡ â†’ combined_raw_BV1D197YCEsW_202509061720.csv / combined_raw_BV1D197YCEsW_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV17pZfYLEu6 ===\n",
      "[META] æ ‡é¢˜: å°ç±³su7äº‹ä»¶ï¼Œ4æœˆ1å·ä¿¡æ¯åˆ†æğŸ§ | å‘å¸ƒæ—¥æœŸ: 2025-04-02\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å°ç±³su7äº‹ä»¶ï¼Œ4æœˆ1å·ä¿¡æ¯åˆ†æğŸ§ (cid=29199436543) â€¦\n",
      "    å°ç±³su7äº‹ä»¶ï¼Œ4æœˆ1å·ä¿¡æ¯åˆ†æğŸ§ æŠ“åˆ° 2582 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2582 æ¡ â†’ combined_raw_BV17pZfYLEu6_202509061720.csv / combined_raw_BV17pZfYLEu6_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1ZDGYz6Eko ===\n",
      "[META] æ ‡é¢˜: é•¿åŸæ±½è½¦æ™ºèƒ½åŒ–ï¼Œå¦‚ä½•å‘å‰èµ°ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-05-01\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é•¿åŸæ±½è½¦æ™ºèƒ½åŒ–ï¼Œå¦‚ä½•å‘å‰èµ°ï¼Ÿ (cid=29727722007) â€¦\n",
      "    é•¿åŸæ±½è½¦æ™ºèƒ½åŒ–ï¼Œå¦‚ä½•å‘å‰èµ°ï¼Ÿ æŠ“åˆ° 967 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 967 æ¡ â†’ combined_raw_BV1ZDGYz6Eko_202509061720.csv / combined_raw_BV1ZDGYz6Eko_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1TRLEzpE6k ===\n",
      "[META] æ ‡é¢˜: å°ç±³SU7äº‹ä»¶ï¼Œåº”è¯¥æ•™ä¼šæˆ‘ä»¬æ›´å¤š   ã€ä¸‹å°ºæŠ¥å‘Šã€‘ | å‘å¸ƒæ—¥æœŸ: 2025-04-25\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å°ç±³SU7äº‹ä»¶ï¼Œåº”è¯¥æ•™ä¼šæˆ‘ä»¬æ›´å¤š   ã€ä¸‹å°ºæŠ¥å‘Šã€‘ (cid=29618405918) â€¦\n",
      "    å°ç±³SU7äº‹ä»¶ï¼Œåº”è¯¥æ•™ä¼šæˆ‘ä»¬æ›´å¤š   ã€ä¸‹å°ºæŠ¥å‘Šã€‘ æŠ“åˆ° 4598 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 4598 æ¡ â†’ combined_raw_BV1TRLEzpE6k_202509061720.csv / combined_raw_BV1TRLEzpE6k_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1v7PjegENh ===\n",
      "[META] æ ‡é¢˜: ä¸€é•œåˆ°åº•ï¼šç‰¹æ–¯æ‹‰FSDæŒ‘æˆ˜ã€Œæ™ºé—¯çº½åŒ—ã€ | å‘å¸ƒæ—¥æœŸ: 2025-02-26\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ä¸€é•œåˆ°åº•ï¼šç‰¹æ–¯æ‹‰FSDæŒ‘æˆ˜ã€Œæ™ºé—¯çº½åŒ—ã€ (cid=28605743916) â€¦\n",
      "    ä¸€é•œåˆ°åº•ï¼šç‰¹æ–¯æ‹‰FSDæŒ‘æˆ˜ã€Œæ™ºé—¯çº½åŒ—ã€ æŠ“åˆ° 4489 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 4489 æ¡ â†’ combined_raw_BV1v7PjegENh_202509061720.csv / combined_raw_BV1v7PjegENh_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1bN9PYeEBm ===\n",
      "[META] æ ‡é¢˜: 52.99ä¸‡ï¼Œå°ç±³SU7æ™ºé©¾æ°´å¹³åº”è¯¥ä¸æ€ä¹ˆæ ·å§ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-02-27\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– 50å¤šä¸‡1548åŒ¹å°ç±³SU7ï¼Œæ™ºé©¾æ°´å¹³åº”è¯¥ä¸æ€ä¹ˆæ ·å§ï¼Ÿ (cid=28603977518) â€¦\n",
      "    50å¤šä¸‡1548åŒ¹å°ç±³SU7ï¼Œæ™ºé©¾æ°´å¹³åº”è¯¥ä¸æ€ä¹ˆæ ·å§ï¼Ÿ æŠ“åˆ° 2851 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 2851 æ¡ â†’ combined_raw_BV1bN9PYeEBm_202509061720.csv / combined_raw_BV1bN9PYeEBm_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1kNZ1YJEPH ===\n",
      "[META] æ ‡é¢˜: æ™ºèƒ½é©¾é©¶â‰ è‡ªåŠ¨é©¾é©¶ï¼å²šå›¾æ¢¦æƒ³å®¶â€œæ™ºé©¾â€è¦å¦‚ä½•æ›´å®‰å…¨ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-04-03\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– æ™ºèƒ½é©¾é©¶â‰ è‡ªåŠ¨é©¾é©¶ï¼å²šå›¾æ¢¦æƒ³å®¶â€œæ™ºé©¾â€è¦å¦‚ä½•æ›´å®‰å…¨ï¼Ÿ (cid=29212607774) â€¦\n",
      "    æ™ºèƒ½é©¾é©¶â‰ è‡ªåŠ¨é©¾é©¶ï¼å²šå›¾æ¢¦æƒ³å®¶â€œæ™ºé©¾â€è¦å¦‚ä½•æ›´å®‰å…¨ï¼Ÿ æŠ“åˆ° 734 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 734 æ¡ â†’ combined_raw_BV1kNZ1YJEPH_202509061720.csv / combined_raw_BV1kNZ1YJEPH_202509061720.txt\n",
      "\n",
      "=== å¤„ç† BV1Bh3jzxE2q ===\n",
      "[META] æ ‡é¢˜: å¤–å›½è½¦è¯„äººæ¥åˆ°æ­¦æ±‰ï¼Œæ„Ÿå—æ— äººé©¾é©¶ï¼Œæ‚¬æµ®ç©ºè½¨ï¼Œå®Œå…¨é¢ è¦†è®¤çŸ¥ï¼ | å‘å¸ƒæ—¥æœŸ: 2025-07-04\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– å¤–å›½è½¦è¯„äººæ¥åˆ°æ­¦æ±‰ï¼Œæ„Ÿå—æ— äººé©¾é©¶ï¼Œæ‚¬æµ®ç©ºè½¨ï¼Œå®Œå…¨é¢ è¦†è®¤çŸ¥ï¼ (cid=30840851939) â€¦\n",
      "    å¤–å›½è½¦è¯„äººæ¥åˆ°æ­¦æ±‰ï¼Œæ„Ÿå—æ— äººé©¾é©¶ï¼Œæ‚¬æµ®ç©ºè½¨ï¼Œå®Œå…¨é¢ è¦†è®¤çŸ¥ï¼ æŠ“åˆ° 3529 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 3529 æ¡ â†’ combined_raw_BV1Bh3jzxE2q_202509061721.csv / combined_raw_BV1Bh3jzxE2q_202509061721.txt\n",
      "\n",
      "=== å¤„ç† BV1f7b6zPESj ===\n",
      "[META] æ ‡é¢˜: é¦†é•¿8.14æ·±åœ³è¡Œâ‘£ï¼Œä½“éªŒæ— äººé©¾é©¶å‡ºç§Ÿè½¦ï¼šå“å“Ÿå–‚å‘€ï¼Œç¬¬ä¸€æ¬¡åï¼ŒçœŸæ˜¯å“æ­»æˆ‘äº†å¥½ä¸å¥½ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-08-14\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é¦†é•¿8.14æ·±åœ³è¡Œâ‘£ï¼Œä½“éªŒæ— äººé©¾é©¶å‡ºç§Ÿè½¦ï¼šå“å“Ÿå–‚å‘€ï¼Œç¬¬ä¸€æ¬¡åï¼ŒçœŸæ˜¯å“æ­»æˆ‘äº†å¥½ä¸å¥½ï¼Ÿ (cid=31686987287) â€¦\n",
      "    é¦†é•¿8.14æ·±åœ³è¡Œâ‘£ï¼Œä½“éªŒæ— äººé©¾é©¶å‡ºç§Ÿè½¦ï¼šå“å“Ÿå–‚å‘€ï¼Œç¬¬ä¸€æ¬¡åï¼ŒçœŸæ˜¯å“æ­»æˆ‘äº†å¥½ä¸å¥½ï¼Ÿ æŠ“åˆ° 1962 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1962 æ¡ â†’ combined_raw_BV1f7b6zPESj_202509061721.csv / combined_raw_BV1f7b6zPESj_202509061721.txt\n",
      "\n",
      "=== å¤„ç† BV1bxb1zPEUJ ===\n",
      "[META] æ ‡é¢˜: é¦–æ¬¡ä½“éªŒèåœå¿«è·‘æ— äººé©¾é©¶å‡ºç§Ÿè½¦ | å‘å¸ƒæ—¥æœŸ: 2025-07-25\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– é¦–æ¬¡ä½“éªŒèåœå¿«è·‘æ— äººé©¾é©¶å‡ºç§Ÿè½¦ (cid=31257921527) â€¦\n",
      "    é¦–æ¬¡ä½“éªŒèåœå¿«è·‘æ— äººé©¾é©¶å‡ºç§Ÿè½¦ æŠ“åˆ° 1724 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 1724 æ¡ â†’ combined_raw_BV1bxb1zPEUJ_202509061721.csv / combined_raw_BV1bxb1zPEUJ_202509061721.txt\n",
      "\n",
      "=== å¤„ç† BV1Y1JezEELN ===\n",
      "[META] æ ‡é¢˜: æœ¬ç”°æ€åŸŸä½ ä¹Ÿæ•¢æ— äººé©¾é©¶ï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-05-21\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– lv_0_20250520225935 (cid=30066738154) â€¦\n",
      "    lv_0_20250520225935 æŠ“åˆ° 535 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 535 æ¡ â†’ combined_raw_BV1Y1JezEELN_202509061721.csv / combined_raw_BV1Y1JezEELN_202509061721.txt\n",
      "\n",
      "=== å¤„ç† BV1wWfDYiEUi ===\n",
      "[META] æ ‡é¢˜: ã€æ·±åº¦è§£æã€‘æ™ºèƒ½é©¾é©¶çƒ§äº†1000äº¿ï¼Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ | å‘å¸ƒæ—¥æœŸ: 2025-01-27\n",
      "[META] å…± 1 ä¸ªåˆ†P\n",
      "  - æŠ“å– ã€æ·±åº¦è§£æã€‘æ™ºèƒ½é©¾é©¶çƒ§äº†1000äº¿ï¼Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ (cid=28111012375) â€¦\n",
      "    ã€æ·±åº¦è§£æã€‘æ™ºèƒ½é©¾é©¶çƒ§äº†1000äº¿ï¼Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ æŠ“åˆ° 7443 æ¡\n",
      "[OK] å•è§†é¢‘åˆè®¡ 7443 æ¡ â†’ combined_raw_BV1wWfDYiEUi_202509061721.csv / combined_raw_BV1wWfDYiEUi_202509061721.txt\n",
      "\n",
      "====== æ±‡æ€»å®Œæˆ ======\n",
      "æ€»è®¡æŠ“åˆ° 113175 æ¡å¼¹å¹•\n",
      "å…¨é‡ CSVï¼šC:\\Users\\Andrew\\Desktop\\homework\\cc\\bili_danmu_results\\combined_all_202509061721.csv\n",
      "å…¨é‡ TXTï¼šC:\\Users\\Andrew\\Desktop\\homework\\cc\\bili_danmu_results\\combined_all_202509061721.txt\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "æ‰¹é‡æŠ“å– B ç«™å¼¹å¹•ï¼ˆæ— éœ€è¾“å…¥æ—¥æœŸï¼‰ï¼Œä» url.txt è¯»å–å¤šä¸ªè§†é¢‘é“¾æ¥/BVï¼Œ\n",
    "å¯¹æ¯æ¡å¼¹å¹•çš„ timestamp ç»Ÿä¸€å†™å…¥è¯¥è§†é¢‘çš„â€œå‘å¸ƒæ—¥æœŸâ€ï¼ˆpubdateï¼‰ï¼Œ\n",
    "å¹¶åœ¨ bili_danmu_results/ ä¸‹ä¿å­˜åˆ†P CSVã€æ¯è§†é¢‘åˆå¹¶ CSV/TXTã€ä»¥åŠå…¨é‡æ±‡æ€» CSV/TXTã€‚\n",
    "\n",
    "ç”¨æ³•ï¼š\n",
    "    python crawl_bili_danmu_batch.py\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, csv, datetime\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ---------- å¸¸é‡ ----------\n",
    "HEADERS_BASE = {\n",
    "    \"origin\": \"https://www.bilibili.com\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DM_RE = re.compile(rb':(.*?)@', re.S)            # ä» seg.so æå–çº¯æ–‡æœ¬å¼¹å¹•\n",
    "BV_RE = re.compile(r'(BV[0-9A-Za-z]{10,})')\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "URL_LIST_FILE = \"url.txt\"  # æ¯è¡Œä¸€ä¸ª BV æˆ–è§†é¢‘é“¾æ¥ï¼Œæ”¯æŒæ³¨é‡Šï¼ˆä»¥ # å¼€å¤´ï¼‰\n",
    "\n",
    "# å¯é€‰ï¼šå¦‚éœ€ç™»å½•æ€ç¨³å®šæ€§ï¼Œå¯åœ¨æ­¤å¤„ç²˜è´´ä½ çš„ Cookieï¼ˆä¹Ÿå¯ä»¥ç•™ç©ºï¼‰\n",
    "DEFAULT_COOKIE = \"enable_web_push=DISABLE; buvid4=441674D3-73C6-EE18-D7F6-0A9AA6A8764B10929-024061616-tMB8uhs7bNpfYIQVaVKjtQ%3D%3D; DedeUserID=499303036; DedeUserID__ckMd5=7c2e754fb5285a0b; buvid_fp_plain=undefined; enable_feed_channel=ENABLE; hit-dyn-v2=1; fingerprint=d30d288e57446a6e2075d79545bcdece; buvid_fp=d30d288e57446a6e2075d79545bcdece; buvid3=D8E29309-F4DF-1535-6A27-C5ADFB04F3B208505infoc; b_nut=1750090608; _uuid=EE91029AA-C3ED-2D10D-A31B-8610DA6DC1BEE28428infoc; header_theme_version=OPEN; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; rpdid=|(J~R~uR))~u0J'u~lJkJl)Y|; LIVE_BUVID=AUTO2217563800546588; PVID=2; CURRENT_QUALITY=80; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTczMDE2NTEsImlhdCI6MTc1NzA0MjM5MSwicGx0IjotMX0.FWLzrbsTzZVO9pAvA2K-j8JpwDO4_00W2sqETwK91NA; bili_ticket_expires=1757301591; SESSDATA=fc10051c%2C1772610457%2C5b79d%2A91CjBHREFbDUZegrrFspSN0CJBiv6HiJF4vOUubDwDGrFJa2-yRA8gOqoqGpCwy29sjfwSVnVwNlBZR1hKb2VFaGI0NkZpcWZMYm5ubE5Kd2JFdkluaFp2Q1JfeFNLX1JqWWxxb19mUENfa193RkFUVDNPa3lZcUU4WmZrZ3ZlLXRpZ3RBRkQ5RnNRIIEC; bili_jct=e685ab8cae35933d20e223cb4b6a6d91; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; bsource=search_google; sid=7l86jgpi; b_lsid=D181510CA_1991E1E02C8; bp_t_offset_499303036=1109440247839588352; CURRENT_FNVAL=2000; home_feed_column=4; browser_resolution=1089-770\"\n",
    "# DEFAULT_COOKIE = \"SESSDATA=xxx; bili_jct=yyy; ...\"\n",
    "\n",
    "if not DEFAULT_COOKIE:\n",
    "    print(\"no cookie\")\n",
    "\n",
    "# ---------- å·¥å…· ----------\n",
    "def build_session(cookie=None, referer=None):\n",
    "    s = requests.Session()\n",
    "    headers = HEADERS_BASE.copy()\n",
    "    if cookie:\n",
    "        headers[\"cookie\"] = cookie\n",
    "    if referer:\n",
    "        headers[\"referer\"] = referer\n",
    "    retries = Retry(\n",
    "        total=3, backoff_factor=0.6,\n",
    "        status_forcelist=[412, 429, 500, 502, 503, 504]\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries, pool_connections=16, pool_maxsize=16))\n",
    "    s.headers.update(headers)\n",
    "    return s\n",
    "\n",
    "def extract_bvid(text: str) -> str:\n",
    "    m = BV_RE.search(text)\n",
    "    if not m:\n",
    "        raise ValueError(f\"æœªæ‰¾åˆ° BV å·ï¼š{text}\")\n",
    "    return m.group(1)\n",
    "\n",
    "def read_urls(path=URL_LIST_FILE):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ° {path}ï¼Œè¯·åˆ›å»ºå¹¶å†™å…¥è§†é¢‘é“¾æ¥æˆ– BV å·\")\n",
    "    bvids = []\n",
    "    seen = set()\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):  # è·³è¿‡æ³¨é‡Š/ç©ºè¡Œ\n",
    "                continue\n",
    "            try:\n",
    "                bvid = extract_bvid(line)\n",
    "                if bvid not in seen:\n",
    "                    seen.add(bvid)\n",
    "                    bvids.append(bvid)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] è·³è¿‡æ— æ³•è§£æçš„è¡Œï¼š{line} ({e})\")\n",
    "    return bvids\n",
    "\n",
    "def get_pagelist_by_bvid(session, bvid):\n",
    "    \"\"\"è·å–åˆ†Pä¿¡æ¯ï¼š[{cid, page, part, duration}, ...]\"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/player/pagelist?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0:\n",
    "        raise RuntimeError(f\"pagelist æ¥å£å¤±è´¥ï¼šcode={j.get('code')} msg={j.get('message')}\")\n",
    "    pages = j.get(\"data\") or []\n",
    "    if not pages:\n",
    "        raise RuntimeError(\"pagelist è¿”å›ç©ºï¼Œå¯èƒ½ BV æ— æ•ˆæˆ–éœ€è¦ç™»å½•ã€‚\")\n",
    "    return pages\n",
    "\n",
    "def get_video_meta(session, bvid):\n",
    "    \"\"\"\n",
    "    å–è§†é¢‘åŸºç¡€ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€å‘å¸ƒæ—¥æœŸï¼‰ã€‚\n",
    "    x/web-interface/view è¿”å› data.pubdateï¼ˆç§’çº§ Unix æ—¶é—´æˆ³ï¼‰\n",
    "    \"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/web-interface/view?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0 or not j.get(\"data\"):\n",
    "        raise RuntimeError(f\"view æ¥å£å¤±è´¥ï¼šcode={j.get('code')} msg={j.get('message')}\")\n",
    "    data = j[\"data\"]\n",
    "    title = data.get(\"title\", f\"{bvid}\")\n",
    "    pub_ts = data.get(\"pubdate\")  # int\n",
    "    if not pub_ts:\n",
    "        # æœ‰äº›æƒ…å†µä¸‹å¯èƒ½è¿”å› ctimeï¼›å…œåº•ä¸€ä¸‹\n",
    "        pub_ts = data.get(\"ctime\")\n",
    "    if not pub_ts:\n",
    "        # å†å…œåº•ä¸ºâ€œä»Šå¤©â€\n",
    "        pub_dt = datetime.datetime.now()\n",
    "    else:\n",
    "        pub_dt = datetime.datetime.fromtimestamp(int(pub_ts))\n",
    "    pubdate_str = pub_dt.strftime(\"%Y-%m-%d\")  # ç»Ÿä¸€å†™â€œæ—¥æœŸâ€åˆ° timestamp\n",
    "    return title, pubdate_str\n",
    "\n",
    "def fetch_seg_bytes(session, cid, seg_idx):\n",
    "    url = f\"https://api.bilibili.com/x/v2/dm/web/seg.so?type=1&oid={cid}&segment_index={seg_idx}\"\n",
    "    r = session.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def parse_danmu_from_seg(seg_bytes):\n",
    "    # ä»…æå–â€œå†…å®¹â€ï¼Œä¸è§£ææ›´ä¸°å¯Œå­—æ®µï¼ˆè‹¥éœ€è¦å¯ä»¥æ”¹ä¸º protobuf è§£æï¼‰\n",
    "    return [m.decode('utf-8', errors='ignore') for m in DM_RE.findall(seg_bytes)]\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def save_csv(rows, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"content\", \"video_title\", \"timestamp\"])\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def save_txt(lines, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write((line or \"\").strip() + \"\\n\")\n",
    "\n",
    "# ---------- æŠ“å–ä¸€ä¸ªè§†é¢‘ ----------\n",
    "def crawl_one_bv(session, bvid, cookie=None, sleep_s=0.05):\n",
    "    referer = f\"https://www.bilibili.com/video/{bvid}\"\n",
    "    session.headers.update({\"referer\": referer})\n",
    "    print(f\"\\n=== å¤„ç† {bvid} ===\")\n",
    "\n",
    "    # 1) å…ƒæ•°æ®ï¼šæ ‡é¢˜ & å‘å¸ƒæ—¥æœŸï¼ˆtimestamp ç»Ÿä¸€ç”¨å®ƒï¼‰\n",
    "    video_title_main, pubdate_str = get_video_meta(session, bvid)\n",
    "    print(f\"[META] æ ‡é¢˜: {video_title_main} | å‘å¸ƒæ—¥æœŸ: {pubdate_str}\")\n",
    "\n",
    "    # 2) åˆ† P åˆ—è¡¨\n",
    "    pages = get_pagelist_by_bvid(session, bvid)\n",
    "    print(f\"[META] å…± {len(pages)} ä¸ªåˆ†P\")\n",
    "\n",
    "    all_rows_video = []\n",
    "    for p in pages:\n",
    "        cid = p[\"cid\"]\n",
    "        part = p.get(\"part\") or f\"P{p.get('page', '?')}\"\n",
    "        title = f\"{bvid} | {part} | {video_title_main}\"\n",
    "        print(f\"  - æŠ“å– {part} (cid={cid}) â€¦\")\n",
    "\n",
    "        # 3) seg.so é€æ®µæŠ“\n",
    "        all_items = []\n",
    "        seg_idx = 1\n",
    "        empty_runs = 0\n",
    "        while True:\n",
    "            try:\n",
    "                seg_bytes = fetch_seg_bytes(session, cid, seg_idx)\n",
    "            except requests.HTTPError:\n",
    "                time.sleep(1.0)\n",
    "                seg_bytes = fetch_seg_bytes(session, cid, seg_idx)\n",
    "\n",
    "            contents = parse_danmu_from_seg(seg_bytes)\n",
    "            if not contents:\n",
    "                empty_runs += 1\n",
    "                if empty_runs >= 2:\n",
    "                    break\n",
    "            else:\n",
    "                empty_runs = 0\n",
    "                for c in contents:\n",
    "                    all_items.append({\n",
    "                        \"content\": c,\n",
    "                        \"video_title\": title,\n",
    "                        # ç»Ÿä¸€å†™â€œè§†é¢‘å‘å¸ƒæ—¥æœŸâ€\n",
    "                        \"timestamp\": pubdate_str\n",
    "                    })\n",
    "            seg_idx += 1\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "        print(f\"    {part} æŠ“åˆ° {len(all_items)} æ¡\")\n",
    "        all_rows_video.extend(all_items)\n",
    "\n",
    "        # åˆ†På„è‡ªä¿å­˜\n",
    "        csv_path = os.path.join(RAW_DIR, f\"{bvid}_{cid}_{part}.csv\")\n",
    "        save_csv(all_items, csv_path)\n",
    "\n",
    "    # å•è§†é¢‘åˆå¹¶è¾“å‡º\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    combined_csv = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.csv\")\n",
    "    combined_txt = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.txt\")\n",
    "    save_csv(all_rows_video, combined_csv)\n",
    "    save_txt([r[\"content\"] for r in all_rows_video], combined_txt)\n",
    "    print(f\"[OK] å•è§†é¢‘åˆè®¡ {len(all_rows_video)} æ¡ â†’ {os.path.basename(combined_csv)} / {os.path.basename(combined_txt)}\")\n",
    "\n",
    "    return all_rows_video  # ç»™æ€»æ±‡æ€»ç”¨\n",
    "\n",
    "# ---------- ä¸»æµç¨‹ ----------\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Bç«™å¼¹å¹•æ‰¹é‡æŠ“å–ï¼ˆæŒ‰ url.txt åˆ—è¡¨ï¼›timestamp=è§†é¢‘å‘å¸ƒæ—¥æœŸï¼‰\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    bvids = read_urls(URL_LIST_FILE)\n",
    "    if not bvids:\n",
    "        print(\"æœªä» url.txt è¯»åˆ°ä»»ä½• BV/é“¾æ¥ã€‚\"); return\n",
    "\n",
    "    sess = build_session(cookie=DEFAULT_COOKIE)\n",
    "\n",
    "    grand_total_rows = []\n",
    "    for bvid in bvids:\n",
    "        try:\n",
    "            rows = crawl_one_bv(sess, bvid, cookie=DEFAULT_COOKIE)\n",
    "            grand_total_rows.extend(rows)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] å¤„ç† {bvid} å¤±è´¥ï¼š{e}\")\n",
    "\n",
    "    # å…¨é‡æ±‡æ€»\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    all_csv = os.path.join(RAW_DIR, f\"combined_all_{ts}.csv\")\n",
    "    all_txt = os.path.join(RAW_DIR, f\"combined_all_{ts}.txt\")\n",
    "    save_csv(grand_total_rows, all_csv)\n",
    "    save_txt([r[\"content\"] for r in grand_total_rows], all_txt)\n",
    "\n",
    "    print(\"\\n====== æ±‡æ€»å®Œæˆ ======\")\n",
    "    print(f\"æ€»è®¡æŠ“åˆ° {len(grand_total_rows)} æ¡å¼¹å¹•\")\n",
    "    print(f\"å…¨é‡ CSVï¼š{all_csv}\")\n",
    "    print(f\"å…¨é‡ TXTï¼š{all_txt}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Bç«™å¼¹å¹•æ¸…æ´—ï¼ˆä»…å¤„ç† combined_all_* æ±‡æ€»æ–‡ä»¶ï¼‰\n",
      "============================================================\n",
      "å°†ä»…æ¸…æ´—ï¼šcombined_all_202509061721.csv\n",
      "è¯»å–ï¼šcombined_all_202509061721.csv\n",
      "è¯»å–è®°å½•æ•°ï¼š113175\n",
      "\n",
      "ä¿ç•™ 86961/113175 (76.8%)\n",
      "\n",
      "æ¸…æ´—åä¿å­˜ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\combined_cleaned_danmu_202509061738.txt\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\combined_cleaned_danmu_202509061738.csv\n",
      "ç§»é™¤åŸå› åˆ†å¸ƒï¼š {'noise': 13304, 'duplicate': 12910}\n",
      "ç§»é™¤æ˜ç»†ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\removed_reasons_202509061738.csv\n",
      "è°ƒè¯•å¯¹ç…§æ ·æœ¬ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\cc\\cleaned_danmu_results\\debug_original_vs_cleaned_202509061738.csv\n",
      "\n",
      "å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bç«™å¼¹å¹•æ‰¹é‡æ¸…æ´—å·¥å…·ï¼ˆä»…å¤„ç†æ±‡æ€»æ–‡ä»¶ combined_all_*ï¼‰\n",
    "- åªè¯»å– bili_danmu_results/combined_all_*.csvï¼ˆä¼˜å…ˆï¼‰æˆ–åŒå .txtï¼ˆå…œåº•ï¼‰\n",
    "- æŠ½å–â†’å»å™ªâ†’è§„èŒƒåŒ–â†’å»é‡ï¼Œä¿ç•™ video_title/timestamp\n",
    "- å½’ä¸€åŒ–æ—¥æœŸï¼šYYYY/MM/DDã€YYYY.M.D ç­‰ â†’ YYYY-MM-DD\n",
    "- è¾“å‡º cleaned_danmu_results/ï¼š\n",
    "    - combined_cleaned_danmu_*.txt / *.csv\n",
    "    - removed_reasons_*.csv ï¼ˆè¢«åˆ é™¤æ¡ç›®ä¸åŸå› ï¼‰\n",
    "    - debug_original_vs_cleaned_*.csv ï¼ˆæŠ½æ ·å¯¹ç…§ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "OUT_DIR = os.path.join(os.getcwd(), \"cleaned_danmu_results\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- é¢„ç¼–è¯‘æ­£åˆ™ ----------\n",
    "RE_CTRL = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')                    # æ§åˆ¶å­—ç¬¦\n",
    "RE_HTML = re.compile(r'<[^>]+>')\n",
    "RE_URL  = re.compile(r'https?://\\S+')\n",
    "# ä»…ä¿ç•™ï¼šä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ä¸­æ–‡æ ‡ç‚¹ã€è¿å­—ç¬¦\n",
    "RE_KEEP = re.compile(r'[^\\w\\u4e00-\\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š\"\\'()ã€ã€‘ã€Šã€‹Â·â€”â€¦\\-]+')\n",
    "RE_SP   = re.compile(r'\\s+')\n",
    "RE_MULTI_PUNCT = re.compile(r'([ï¼Œã€‚ï¼ï¼Ÿâ€¦])\\1+')\n",
    "# å¸¸è§è¡Œé¦–å™ªå£°å—ï¼ˆæ‹¬å·/å¥‡æ€ªå­—èŠ‚/çŸ­hex/å­¤ç«‹å­—æ¯/æ ‡ç‚¹ï¼‰ç»„åˆ\n",
    "RE_NOISY_PREFIX = re.compile(\n",
    "    r'^\\s*(?:[\\(\\)\\[\\]{}<>\\|\\\\/\\-_=+~^`Â·â€¢]+|[A-Za-z](?=\\s)|[0-9A-Fa-f]{6,10}|[,.:;ï¼Œã€‚ï¼›ï¼š!ï¼?ï¼Ÿ\\s])+'\n",
    ")\n",
    "\n",
    "def _cut_after_colon(raw: str) -> str:\n",
    "    \"\"\"å¦‚å«å†’å·ï¼Œå–æœ€åä¸€ä¸ªå†’å·åçš„ç‰‡æ®µ\"\"\"\n",
    "    if ':' in raw:\n",
    "        return raw.split(':')[-1]\n",
    "    return raw\n",
    "\n",
    "def _strip_to_first_textual_char(s: str) -> str:\n",
    "    \"\"\"å‰¥ç¦»å‰ç¼€å™ªå£°å—ï¼Œå®šä½ç¬¬ä¸€ä¸ªæœ‰æ•ˆå­—ç¬¦ï¼ˆä¸­è‹±æ•°ï¼‰\"\"\"\n",
    "    s = RE_NOISY_PREFIX.sub('', s)\n",
    "    idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch.isalnum() or ('\\u4e00' <= ch <= '\\u9fa5'):\n",
    "            idx = i\n",
    "            break\n",
    "    return s[idx:] if idx is not None else ''\n",
    "\n",
    "def _normalize_text(s: str) -> str:\n",
    "    s = RE_CTRL.sub(' ', s)                    # åˆ æ§åˆ¶å­—ç¬¦ï¼ˆå«ä½ æ ·ä¾‹é‡Œçš„å¥‡æ€ªå‰å¯¼å­—èŠ‚ï¼‰\n",
    "    s = unicodedata.normalize('NFKC', s)       # å…¨è§’â†’åŠè§’\n",
    "    s = RE_HTML.sub('', s)                     # å»HTML\n",
    "    s = RE_URL.sub('', s)                      # å»URL\n",
    "    s = RE_KEEP.sub(' ', s)                    # è¿‡æ»¤åˆ°å…è®¸å­—ç¬¦é›†\n",
    "    s = RE_MULTI_PUNCT.sub(r'\\1', s)           # è¿ç»­æ ‡ç‚¹å‹ç¼©\n",
    "    s = RE_SP.sub(' ', s).strip()              # ç©ºç™½è§„æ•´\n",
    "    return s\n",
    "\n",
    "def _looks_like_noise(s: str) -> bool:\n",
    "    \"\"\"å™ªå£°åˆ¤å®šï¼šçº¯æ•°å­—/å•å­—æ¯/å¤ªçŸ­/æ–‡æœ¬å æ¯”ä½\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    if s.isdigit():\n",
    "        return True\n",
    "    if len(s) == 1 and s.isalnum():\n",
    "        return True\n",
    "    if len(s) < 2:\n",
    "        return True\n",
    "    total = len(s)\n",
    "    keep = sum(1 for ch in s if ch.isalnum() or '\\u4e00' <= ch <= '\\u9fa5')\n",
    "    if keep / max(1, total) < 0.3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def _normalize_date(ts: str) -> str:\n",
    "    \"\"\"\n",
    "    å½’ä¸€åŒ–æ—¥æœŸåˆ° YYYY-MM-DDï¼š\n",
    "    æ”¯æŒï¼šYYYY/M/D, YYYY-M-D, YYYY.M.D, YYYY_MM_DDï¼ˆéƒ½å¯1ä½æœˆæ—¥ï¼‰\n",
    "    å…¶ä»–æƒ…å†µåŸæ ·è¿”å›ï¼ˆæˆ–ç©ºåˆ™è¿”å›ç©ºï¼‰\n",
    "    \"\"\"\n",
    "    if not ts or not str(ts).strip():\n",
    "        return \"\"\n",
    "    s = str(ts).strip()\n",
    "    s = RE_CTRL.sub(' ', s)         # é˜²æ­¢æ§åˆ¶å­—ç¬¦\n",
    "    s = s.replace('.', '-').replace('/', '-').replace('_', '-')\n",
    "    parts = [p for p in s.split('-') if p]\n",
    "    if len(parts) >= 3 and parts[0].isdigit():\n",
    "        y = parts[0]\n",
    "        m = parts[1].zfill(2) if parts[1].isdigit() else parts[1]\n",
    "        d = parts[2].zfill(2) if parts[2].isdigit() else parts[2]\n",
    "        try:\n",
    "            dt = datetime.date(int(y), int(m), int(d))\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "def clean_one(raw: str) -> str:\n",
    "    \"\"\"å•æ¡å¼¹å¹•æ¸…æ´—ï¼šæŠ½å–â†’å»å™ªâ†’è§„èŒƒåŒ–â†’é•¿åº¦ä¿æŠ¤\"\"\"\n",
    "    s = str(raw or '')\n",
    "    s = _cut_after_colon(s)\n",
    "    s = RE_CTRL.sub(' ', s)              # å…ˆå»æ§åˆ¶ç¬¦ï¼Œå†å®šä½æ–‡æœ¬èµ·ç‚¹\n",
    "    s = _strip_to_first_textual_char(s)\n",
    "    s = _normalize_text(s)\n",
    "    if len(s) > 100:\n",
    "        s = s[:100].rstrip()\n",
    "    return s\n",
    "\n",
    "# ---------- æ–‡ä»¶é€‰æ‹©ï¼šä»… combined_all_* ----------\n",
    "def pick_combined_all_files():\n",
    "    \"\"\"\n",
    "    ä¼˜å…ˆ CSVï¼š\n",
    "      - è‹¥å­˜åœ¨å¤šä¸ª combined_all_*.csvï¼ŒæŒ‰ä¿®æ”¹æ—¶é—´å–â€œæœ€æ–°çš„1ä¸ªâ€\n",
    "      - è‹¥æ²¡æœ‰ CSVï¼Œåˆ™å›é€€åˆ°æœ€æ–°çš„ combined_all_*.txt\n",
    "    è¿”å›ï¼šæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆé•¿åº¦=1ï¼‰ï¼Œå¹¶æ ‡æ³¨ç±»å‹\n",
    "    \"\"\"\n",
    "    csvs = glob.glob(os.path.join(RAW_DIR, \"combined_all_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(RAW_DIR, \"combined_all_*.txt\"))\n",
    "\n",
    "    pick = None\n",
    "    ftype = None\n",
    "\n",
    "    if csvs:\n",
    "        pick = max(csvs, key=os.path.getmtime)\n",
    "        ftype = \"csv\"\n",
    "    elif txts:\n",
    "        pick = max(txts, key=os.path.getmtime)\n",
    "        ftype = \"txt\"\n",
    "\n",
    "    if not pick:\n",
    "        return []\n",
    "    print(f\"å°†ä»…æ¸…æ´—ï¼š{os.path.basename(pick)}\")\n",
    "    return [(pick, ftype)]\n",
    "\n",
    "def read_records_from_csv(path: str):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in [\"content\", \"video_title\", \"timestamp\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    df = df[[\"content\", \"video_title\", \"timestamp\"]]\n",
    "    # å½’ä¸€åŒ– timestamp\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].map(_normalize_date)\n",
    "    return df.to_dict(\"records\")\n",
    "\n",
    "def read_records_from_txt(path: str):\n",
    "    # ä¸€è¡Œä¸€æ¡ï¼Œä»…æœ‰ contentï¼›video_title/timestamp ç½®ç©º\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            rows.append({\n",
    "                \"content\": line,\n",
    "                \"video_title\": \"\",\n",
    "                \"timestamp\": \"\"\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def read_combined_all():\n",
    "    picks = pick_combined_all_files()\n",
    "    if not picks:\n",
    "        print(f\"æœªåœ¨ {RAW_DIR} æ‰¾åˆ° combined_all_*.csv æˆ– .txt\")\n",
    "        return []\n",
    "    path, ftype = picks[0]\n",
    "    print(f\"è¯»å–ï¼š{os.path.basename(path)}\")\n",
    "    if ftype == \"csv\":\n",
    "        return read_records_from_csv(path)\n",
    "    else:\n",
    "        return read_records_from_txt(path)\n",
    "\n",
    "# ---------- æ¸…æ´—ä¸»æµç¨‹ ----------\n",
    "def clean_danmu_content(records):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ï¼šè®°å½•åˆ—è¡¨ï¼ˆè‡³å°‘å« contentï¼‰\n",
    "    è¾“å‡ºï¼š\n",
    "      - cleaned: [{original, cleaned, video_title, timestamp}]\n",
    "      - removed: [(original, reason)]\n",
    "    \"\"\"\n",
    "    cleaned, removed = [], []\n",
    "\n",
    "    for dm in records:\n",
    "        raw = str(dm.get(\"content\", \"\") or \"\")\n",
    "        if not raw:\n",
    "            removed.append((raw, \"empty\")); continue\n",
    "\n",
    "        out = clean_one(raw)\n",
    "        if _looks_like_noise(out):\n",
    "            removed.append((raw, \"noise\")); continue\n",
    "\n",
    "        cleaned.append({\n",
    "            \"original\": raw,\n",
    "            \"cleaned\": out,\n",
    "            \"video_title\": dm.get(\"video_title\", \"æœªçŸ¥è§†é¢‘\"),\n",
    "            \"timestamp\": _normalize_date(dm.get(\"timestamp\", \"\"))  # å†ä¿é™©\n",
    "        })\n",
    "\n",
    "    # å»é‡ï¼ˆæŒ‰ cleanedï¼‰\n",
    "    seen, dedup = set(), []\n",
    "    for row in cleaned:\n",
    "        key = row[\"cleaned\"]\n",
    "        if key in seen:\n",
    "            removed.append((row[\"original\"], \"duplicate\")); continue\n",
    "        seen.add(key)\n",
    "        dedup.append(row)\n",
    "\n",
    "    return dedup, removed\n",
    "\n",
    "# ---------- è¾“å‡º ----------\n",
    "def save_outputs(cleaned, removed):\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    txt_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.txt\")\n",
    "    csv_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.csv\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in cleaned:\n",
    "            f.write(r[\"cleaned\"] + \"\\n\")\n",
    "    pd.DataFrame(cleaned).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\næ¸…æ´—åä¿å­˜ï¼š\\n- {txt_path}\\n- {csv_path}\")\n",
    "\n",
    "    if removed:\n",
    "        rm_path = os.path.join(OUT_DIR, f\"removed_reasons_{ts}.csv\")\n",
    "        pd.DataFrame(removed, columns=[\"original\", \"reason\"]).to_csv(\n",
    "            rm_path, index=False, encoding=\"utf-8-sig\")\n",
    "        cnt = Counter([r for _, r in removed])\n",
    "        print(\"ç§»é™¤åŸå› åˆ†å¸ƒï¼š\", dict(cnt))\n",
    "        print(f\"ç§»é™¤æ˜ç»†ï¼š\\n- {rm_path}\")\n",
    "\n",
    "    dbg_path = os.path.join(OUT_DIR, f\"debug_original_vs_cleaned_{ts}.csv\")\n",
    "    pd.DataFrame(cleaned)[[\"original\", \"cleaned\"]].head(200)\\\n",
    "        .to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"è°ƒè¯•å¯¹ç…§æ ·æœ¬ï¼š\\n- {dbg_path}\")\n",
    "\n",
    "# ---------- å…¥å£ ----------\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Bç«™å¼¹å¹•æ¸…æ´—ï¼ˆä»…å¤„ç† combined_all_* æ±‡æ€»æ–‡ä»¶ï¼‰\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    records = read_combined_all()\n",
    "    if not records:\n",
    "        return\n",
    "    print(f\"è¯»å–è®°å½•æ•°ï¼š{len(records)}\")\n",
    "\n",
    "    cleaned, removed = clean_danmu_content(records)\n",
    "    keep_ratio = len(cleaned) / max(1, len(records)) * 100\n",
    "    print(f\"\\nä¿ç•™ {len(cleaned)}/{len(records)} ({keep_ratio:.1f}%)\")\n",
    "\n",
    "    save_outputs(cleaned, removed)\n",
    "    print(\"\\nå®Œæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T17:38:10.832812Z",
     "end_time": "2025-09-06T17:38:18.410113Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Danmu Analysis Pro (Word Frequency / Word Cloud / Adaptive Clustering / Sentiment / Monthly Trends)\n",
      "================================================================\n",
      "[INFO] Analyzing file: cleaned_danmu_results\\combined_cleaned_danmu_202509061738.csv\n",
      "[INFO] Number of sentences after cleaning: 86961\n",
      "[INFO] Number of documents after segmentation: 78047; total tokens: 219504\n",
      "[OK] Word frequency file: analysis_results\\word_frequency_202509061957.csv\n",
      "[OK] Word cloud: analysis_results\\wordcloud_202509061957.png\n",
      "[OK] Top bigram file: analysis_results\\top_bigram_202509061957.csv\n",
      "[INFO] Adaptive clustering â€¦\n",
      "[INFO] Silhouette score: K=7, score=0.0194\n",
      "[OK] Cluster overview: analysis_results\\cluster_summary_202509062001.png\n",
      "[OK] Cluster keywords file: analysis_results\\cluster_keywords_202509061957.csv\n",
      "[OK] Cluster sentiment file: analysis_results\\cluster_sentiment_202509061957.csv\n",
      "[OK] Monthly sentiment file: analysis_results\\monthly_sentiment_202509062005.csv\n",
      "[OK] Monthly sentiment chart: analysis_results\\monthly_sentiment_202509062005.png\n",
      "\n",
      "Top 10 Words:\n",
      "1. é©¾é©¶  3815\n",
      "2. åä¸º  2664\n",
      "3. æ²¡æœ‰  2424\n",
      "4. è‡ªåŠ¨  2409\n",
      "5. é—®é¢˜  1880\n",
      "6. æ™ºé©¾  1728\n",
      "7. è¾…åŠ©  1376\n",
      "8. è¯†åˆ«  1335\n",
      "9. å°ç±³  1066\n",
      "10. å¸æœº  1042\n",
      "\n",
      "Done. Output directory: analysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bç«™å¼¹å¹•åˆ†æ Proï¼ˆé€‚é… cleaned_danmu_resultsï¼‰\n",
    "- åªè¯»å–æœ€æ–° cleaned CSVï¼ˆcombined_cleaned_danmu_*.csvï¼‰\n",
    "- åˆ†è¯ï¼šè¯æ€§è¿‡æ»¤ + åœç”¨è¯ï¼ˆå«å£è¯­è¡¥å……ï¼‰ + å¯é€‰è¡Œä¸šè¯å…¸\n",
    "- å‘é‡åŒ–ï¼š1-2gram TF-IDFï¼ˆmin_df/max_df æŠ‘åˆ¶å£æ°´è¯ï¼‰\n",
    "- è‡ªé€‚åº”èšç±»ï¼šKMeans + è½®å»“ç³»æ•°é€‰ Kï¼ˆcosineï¼‰\n",
    "- ä¸»é¢˜å‘½åï¼šéåœç”¨è¯å…³é”®è¯ Top2\n",
    "- æƒ…æ„Ÿï¼šSnowNLPï¼ˆ<4å­—åˆ¤ä¸­æ€§ï¼‰ï¼Œæ±‡æ€»ä¸»é¢˜ï¼›å¹¶æ–°å¢â€œæŒ‰æœˆæƒ…æ„Ÿèµ°åŠ¿â€\n",
    "- å¯è§†åŒ–ï¼šè¯äº‘ã€ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾ + ä¸»é¢˜æƒ…æ„Ÿå †å æŸ±çŠ¶ + å…³é”®è¯è¡¨ + æœˆåº¦æƒ…æ„ŸæŠ˜çº¿\n",
    "- å¯¼å‡ºï¼šè¯é¢‘ã€ä¸»é¢˜å…³é”®è¯/æƒ…æ„Ÿã€Top n-gramã€æœˆåº¦æƒ…æ„Ÿ\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba, jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ===================== å‚æ•° =====================\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"      # å¯ç©º\n",
    "USER_DICT   = \"user_dict.txt\"         # å¯ç©º\n",
    "\n",
    "# è¯æ€§ä¿ç•™ï¼ˆåè¯/åŠ¨è¯/æœ¯è¯­/è‹±æ–‡ç­‰ï¼‰\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# å¼¹å¹•å£è¯­åœç”¨è¯è¡¥å……\n",
    "DANMU_STOP_EXTRA = {\n",
    "    \"è¿™ä¸ª\",\"é‚£ä¸ª\",\"å°±æ˜¯\",\"ä»€ä¹ˆ\",\"è¿˜æœ‰\",\"ç„¶å\",\"ä½†æ˜¯\",\"æ‰€ä»¥\",\"è¿˜æ˜¯\",\"å·²ç»\",\"çœŸçš„\",\"æ„Ÿè§‰\",\"è§‰å¾—\",\"çŸ¥é“\",\n",
    "    \"å¯ä»¥\",\"ä¸å¯ä»¥\",\"ä¸ä¼š\",\"ä¸èƒ½\",\"åº”è¯¥\",\"å¯èƒ½\",\"æœ‰ç‚¹\",\"æœ‰äº›\",\"æ€ä¹ˆ\",\"ä¸ºå•¥\",\"ä¸ºä»€ä¹ˆ\",\n",
    "    \"å•Š\",\"å‘€\",\"å‘¢\",\"å§\",\"å“¦\",\"å“‡\",\"è¯¶\",\"å˜›\",\"å“ˆ\",\"å“ˆå“ˆ\",\"å“ˆå“ˆå“ˆ\",\"emm\",\"å—¯\",\"å•Šå•Š\",\"å‘œå‘œ\",\n",
    "    \"è§†é¢‘\",\"å¼¹å¹•\",\"ç°åœ¨\",\"ä»Šå¤©\",\"æ˜¨å¤©\",\"æ˜å¤©\",\"è¿™é‡Œ\",\"é‚£é‡Œ\",\"è¿™æ ·\",\"é‚£æ ·\",\"å¾ˆå¤š\",\"éå¸¸\"\n",
    "}\n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 4000\n",
    "MIN_DF       = 5\n",
    "MAX_DF       = 0.6\n",
    "NGRAM        = (1,2)\n",
    "\n",
    "# K èŒƒå›´\n",
    "K_MIN, K_MAX = 2, 8\n",
    "\n",
    "# è¯äº‘ TopN\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# å°å¥æƒ…æ„Ÿä¸­æ€§é˜ˆ\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "# =================================================\n",
    "\n",
    "# ---------- å­—ä½“ ----------\n",
    "def setup_chinese_font():\n",
    "    candidates = ['SimHei','Microsoft YaHei','SimSun','KaiTi',\n",
    "                  'Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for name in candidates:\n",
    "        if any(name in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = name; break\n",
    "    if picked:\n",
    "        plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                font_path = f; break\n",
    "    if not font_path:\n",
    "        font_path = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return font_path\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "# ---------- åŸºç¡€å·¥å…· ----------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    base |= set(list(\"ï¼Œã€‚ã€ï¼ï¼Ÿï¼›ï¼šâ€â€œâ€˜â€˜ï¼ˆï¼‰()[]ã€ã€‘â€”â€¦- \"))\n",
    "    base |= DANMU_STOP_EXTRA\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] Loaded user dictionary: {USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_csv():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    if not csvs: return None\n",
    "    return max(csvs, key=os.path.getmtime)\n",
    "\n",
    "def normalize_date_str(s):\n",
    "    \"\"\"å…¼å®¹ 2025/8/12, 2025.8.12, 2025-8-12 ç­‰ â†’ YYYY-MM-DD\"\"\"\n",
    "    if not isinstance(s, str): s = str(s or \"\")\n",
    "    s = CTRL_RE.sub(\" \", s).strip().replace(\"/\", \"-\").replace(\".\", \"-\").replace(\"_\", \"-\")\n",
    "    parts = [p for p in s.split(\"-\") if p]\n",
    "    if len(parts) >= 3 and parts[0].isdigit() and parts[1].isdigit() and parts[2].isdigit():\n",
    "        try:\n",
    "            dt = datetime.date(int(parts[0]), int(parts[1]), int(parts[2]))\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "def load_cleaned_df(latest_csv):\n",
    "    df = pd.read_csv(latest_csv)\n",
    "    # å…¼å®¹åˆ—å\n",
    "    if \"cleaned\" not in df.columns:\n",
    "        raise ValueError(\"Column 'cleaned' not found in the cleaned CSV\")\n",
    "    for col in [\"video_title\",\"timestamp\"]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    # è½»é‡å»å™ª\n",
    "    df[\"cleaned\"] = df[\"cleaned\"].astype(str).map(lambda x: EMOJI_RE.sub(\"\", CTRL_RE.sub(\" \", x)).strip())\n",
    "    # æ—¥æœŸå½’ä¸€åŒ–\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].astype(str).map(normalize_date_str)\n",
    "    return df\n",
    "\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter, top_n=WORDCLOUD_TOPN):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(top_n)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('High-frequency Word Cloud', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------- èšç±» ----------\n",
    "def auto_kmeans(docs, min_k=K_MIN, max_k=K_MAX, max_features=MAX_FEATURES):\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    if n < 60:  # å°‘æ ·æœ¬ä¿æŠ¤\n",
    "        k = max(2, min(4, n // 15 or 2))\n",
    "        model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        return model, vec, None\n",
    "\n",
    "    X_eval = X\n",
    "    if n > 6000:  # æŠ½æ ·è¯„ä¼°\n",
    "        idx = np.random.RandomState(42).choice(n, 6000, replace=False)\n",
    "        X_eval = X[idx]\n",
    "\n",
    "    best_k, best_score, best_model = None, -1, None\n",
    "    for k in range(min_k, max_k+1):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        labels_eval = km.labels_ if X_eval is X else km.predict(X_eval)\n",
    "        score = silhouette_score(X_eval, labels_eval, metric='cosine')\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_model = k, score, km\n",
    "    return best_model, vec, best_score\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in range(topn*3):\n",
    "            idx = order[i, j]\n",
    "            if idx < len(terms):\n",
    "                t = terms[idx]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"Topic {i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # Pie: Topic distribution\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt} items ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"Topic Distribution\", fontsize=15)\n",
    "\n",
    "    # Stacked bars: Sentiment distribution\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='Positive')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='Neutral')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='Negative')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"Ratio\"); ax2.set_title(\"Sentiment Distribution\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # Keywords table\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"Keyword {i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"Cluster Keywords\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# ---------- æœˆåº¦æƒ…æ„Ÿ ----------\n",
    "def monthly_sentiment(df):\n",
    "    \"\"\"\n",
    "    è¯»å– cleanedï¼ˆåŸå¥ï¼‰å’Œ timestampï¼ˆæ—¥æœŸï¼‰ï¼ŒæŒ‰æœˆæ±‡æ€»ç§¯æ/ä¸­æ€§/æ¶ˆææ¯”ä¾‹\n",
    "    \"\"\"\n",
    "    if \"timestamp\" not in df.columns:\n",
    "        return None, None\n",
    "    tmp = df.copy()\n",
    "    tmp[\"date\"] = tmp[\"timestamp\"].map(normalize_date_str)\n",
    "    tmp = tmp[tmp[\"date\"].str.len() >= 8]\n",
    "    if tmp.empty:\n",
    "        return None, None\n",
    "\n",
    "    tmp[\"month\"] = tmp[\"date\"].map(lambda x: x[:7])  # YYYY-MM\n",
    "    recs = []\n",
    "    for m, sub in tmp.groupby(\"month\"):\n",
    "        pos = neu = neg = 0\n",
    "        total = 0\n",
    "        for t in sub[\"cleaned\"].astype(str):\n",
    "            t = t.strip()\n",
    "            if not t: continue\n",
    "            if len(t) < SHORT_NEUTRAL_LEN:\n",
    "                neu += 1\n",
    "            else:\n",
    "                s = SnowNLP(t).sentiments\n",
    "                if s > 0.6: pos += 1\n",
    "                elif s < 0.4: neg += 1\n",
    "                else: neu += 1\n",
    "            total += 1\n",
    "        if total > 0:\n",
    "            recs.append({\"month\": m,\n",
    "                         \"positive\": pos/total,\n",
    "                         \"neutral\": neu/total,\n",
    "                         \"negative\": neg/total,\n",
    "                         \"count\": total})\n",
    "    if not recs:\n",
    "        return None, None\n",
    "\n",
    "    ms_df = pd.DataFrame(sorted(recs, key=lambda x: x[\"month\"]))\n",
    "    # å¯è§†åŒ–\n",
    "    plt.figure(figsize=(12,6))\n",
    "    x = np.arange(len(ms_df))\n",
    "    plt.plot(x, ms_df[\"positive\"], marker=\"o\", label=\"Positive\")\n",
    "    plt.plot(x, ms_df[\"neutral\"], marker=\"o\", label=\"Neutral\")\n",
    "    plt.plot(x, ms_df[\"negative\"], marker=\"o\", label=\"Negative\")\n",
    "    plt.xticks(x, ms_df[\"month\"], rotation=45, ha=\"right\")\n",
    "    plt.ylim(0,1); plt.ylabel(\"Ratio\"); plt.title(\"Monthly Sentiment Trends\")\n",
    "    plt.legend()\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"monthly_sentiment_{ts}.png\")\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=300); plt.close()\n",
    "    csv_out = os.path.join(OUT_DIR, f\"monthly_sentiment_{ts}.csv\")\n",
    "    ms_df.to_csv(csv_out, index=False, encoding=\"utf-8-sig\")\n",
    "    return csv_out, out\n",
    "\n",
    "# ---------- ä¸»æµç¨‹ ----------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"Danmu Analysis Pro (Word Frequency / Word Cloud / Adaptive Clustering / Sentiment / Monthly Trends)\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_csv()\n",
    "    if not latest:\n",
    "        print(\"No cleaned CSV found, please run preprocessing first.\"); return\n",
    "    print(f\"[INFO] Analyzing file: {latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "\n",
    "    df = load_cleaned_df(latest)\n",
    "    lines = df[\"cleaned\"].astype(str).tolist()\n",
    "    print(f\"[INFO] Number of sentences after cleaning: {len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] Empty after segmentation (stopwords too aggressive or texts too short).\"); return\n",
    "    print(f\"[INFO] Number of documents after segmentation: {len(docs)}; total tokens: {len(all_words)}\")\n",
    "\n",
    "    # è¯é¢‘ã€è¯äº‘ã€Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter); print(f\"[OK] Word frequency file: {freq_csv}\")\n",
    "    wc_path = draw_wordcloud(counter)\n",
    "    if wc_path: print(f\"[OK] Word cloud: {wc_path}\")\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram = top_ngrams(docs, n=2, topk=50)\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigram file: {bigram_csv}\")\n",
    "\n",
    "    # èšç±»\n",
    "    if len(docs) >= 20:\n",
    "        print(\"[INFO] Adaptive clustering â€¦\")\n",
    "        model, vec, s_score = auto_kmeans(docs)\n",
    "        if s_score is not None:\n",
    "            print(f\"[INFO] Silhouette score: K={model.n_clusters}, score={s_score:.4f}\")\n",
    "        labels = model.labels_\n",
    "\n",
    "        # ä¸»é¢˜å‘½å + å…³é”®è¯\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, DANMU_STOP_EXTRA, topn=10)\n",
    "\n",
    "        # æ¯ç°‡åŸå¥ï¼ˆæƒ…æ„Ÿç”¨ï¼‰\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {}\n",
    "        for i, nm in enumerate(theme_names):\n",
    "            p,u,n = sentiment_ratio(groups[i])\n",
    "            sentiments[nm] = {\"positive\": p, \"neutral\": u, \"negative\": n}\n",
    "\n",
    "        # å›¾è¡¨\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] Cluster overview: {img}\")\n",
    "\n",
    "        # å¯¼å‡º CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] Cluster keywords file: {kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] Cluster sentiment file: {sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] Documents < 20, skip clustering/sentiment.\")\n",
    "\n",
    "    # æœˆåº¦æƒ…æ„Ÿèµ°åŠ¿\n",
    "    csv_out, img_out = monthly_sentiment(df)\n",
    "    if csv_out:\n",
    "        print(f\"[OK] Monthly sentiment file: {csv_out}\")\n",
    "        print(f\"[OK] Monthly sentiment chart: {img_out}\")\n",
    "    else:\n",
    "        print(\"[INFO] Monthly sentiment not generated (timestamp missing or <2 months of samples).\")\n",
    "\n",
    "    # æ‘˜è¦\n",
    "    print(\"\\nTop 10 Words:\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\nDone. Output directory: analysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T19:55:48.248807Z",
     "end_time": "2025-09-06T20:05:36.351056Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Danmaku Sentiment Analysis (SnowNLP, 5-level buckets)\n",
      "================================================================\n",
      "[INFO] Using cleaned CSV: combined_cleaned_danmu_202509061738.csv\n",
      "[INFO] Loaded 86959 cleaned danmaku lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 86959/86959 [04:18<00:00, 336.73danmaku/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUMMARY]\n",
      "- N = 86959 | Average = 0.4837\n",
      "   Very Negative:  27755  (31.9%)\n",
      "        Negative:   8403  ( 9.7%)\n",
      "         Neutral:  19240  (22.1%)\n",
      "        Positive:   7003  ( 8.1%)\n",
      "   Very Positive:  24558  (28.2%)\n",
      "\n",
      "[FILES]\n",
      "- scored_csv: sentiment_results\\sentiment_scored_202509061806.csv\n",
      "- extremes_txt: sentiment_results\\sentiment_extremes_202509061806.txt\n",
      "- bar_html: sentiment_results\\sentiment_distribution_202509061806.html\n",
      "- hist_png: sentiment_results\\sentiment_histogram_202509061806.png\n",
      "- summary_json: sentiment_results\\sentiment_summary_202509061806.json\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sentiment Analysis for Cleaned Bilibili Danmaku\n",
    "- Input: latest cleaned file from cleaned_danmu_results (CSV preferred; TXT fallback)\n",
    "- Scoring: SnowNLP (0..1, higher = more positive)\n",
    "- Buckets (five-level):\n",
    "    Very Negative (<0.3) | Negative (0.3-0.4) | Neutral (0.4-0.6) | Positive (0.6-0.7) | Very Positive (>0.7)\n",
    "- Outputs (sentiment_results/):\n",
    "    * Interactive bar chart (HTML, pyecharts)\n",
    "    * Static histogram (PNG, matplotlib)\n",
    "    * Scored table CSV (text, score, label)\n",
    "    * Summary JSON\n",
    "    * Extreme-sample snippets (most negative / most positive)\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, time, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from snownlp import SnowNLP\n",
    "from pyecharts.charts import Bar\n",
    "from pyecharts import options as opts\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ----------------------- Paths -----------------------\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"sentiment_results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------- Buckets -----------------------\n",
    "# Boundaries reflect the spec precisely.\n",
    "BUCKETS = [\n",
    "    (\"Very Negative\", lambda s: s < 0.3),\n",
    "    (\"Negative\",      lambda s: 0.3 <= s < 0.4),\n",
    "    (\"Neutral\",       lambda s: 0.4 <= s <= 0.6),   # inclusive both ends\n",
    "    (\"Positive\",      lambda s: 0.6 <  s <= 0.7),\n",
    "    (\"Very Positive\", lambda s: s > 0.7),\n",
    "]\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    \"\"\"Prefer CSV, fallback to TXT.\"\"\"\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    if csvs:\n",
    "        latest = max(csvs, key=os.path.getmtime)\n",
    "        print(f\"[INFO] Using cleaned CSV: {os.path.basename(latest)}\")\n",
    "        return latest\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    if txts:\n",
    "        latest = max(txts, key=os.path.getmtime)\n",
    "        print(f\"[INFO] Using cleaned TXT: {os.path.basename(latest)}\")\n",
    "        return latest\n",
    "    print(\"[ERROR] No cleaned file found. Run the cleaning step first.\")\n",
    "    return None\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    \"\"\"Load 'cleaned' texts (CSV -> 'cleaned' column; TXT -> lines).\"\"\"\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        if \"cleaned\" not in df.columns:\n",
    "            raise ValueError(\"Column 'cleaned' not found in CSV.\")\n",
    "        lines = [str(x).strip() for x in df[\"cleaned\"].fillna(\"\")]\n",
    "    else:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    # Filter out blanks\n",
    "    return [s for s in lines if s]\n",
    "\n",
    "def score_texts(lines, show_progress=True):\n",
    "    \"\"\"SnowNLP scoring with safety fallback.\"\"\"\n",
    "    scores = []\n",
    "    iterator = tqdm(lines, desc=\"Scoring\", unit=\"danmaku\") if show_progress else lines\n",
    "    for t in iterator:\n",
    "        try:\n",
    "            scores.append(float(SnowNLP(t).sentiments))\n",
    "        except Exception:\n",
    "            scores.append(0.5)  # neutral fallback\n",
    "    return scores\n",
    "\n",
    "def bucketize(scores):\n",
    "    \"\"\"Count per 5-level bucket.\"\"\"\n",
    "    labels = [b[0] for b in BUCKETS]\n",
    "    counts = [0]*len(BUCKETS)\n",
    "    for s in scores:\n",
    "        for i, (_, cond) in enumerate(BUCKETS):\n",
    "            if cond(s):\n",
    "                counts[i] += 1\n",
    "                break\n",
    "    return labels, counts\n",
    "\n",
    "def export_scored_table(lines, scores, ts):\n",
    "    \"\"\"Save per-text scores & labels to CSV for downstream analysis.\"\"\"\n",
    "    def label_of(s):\n",
    "        for name, cond in BUCKETS:\n",
    "            if cond(s): return name\n",
    "        return \"Unknown\"\n",
    "    df = pd.DataFrame({\"text\": lines, \"score\": scores, \"label\": [label_of(s) for s in scores]})\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_scored_{ts}.csv\")\n",
    "    df.to_csv(out, index=False, encoding=\"utf-8-sig\")\n",
    "    return out, df\n",
    "\n",
    "def export_extremes(df, ts, k=30):\n",
    "    \"\"\"Save top-K most negative/positive samples (for qualitative quotes).\"\"\"\n",
    "    neg = df.sort_values(\"score\", ascending=True).head(k)\n",
    "    pos = df.sort_values(\"score\", ascending=False).head(k)\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_extremes_{ts}.txt\")\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== Most Negative Samples ===\\n\")\n",
    "        for i, r in neg.iterrows():\n",
    "            f.write(f\"[{r['score']:.4f}] {r['text']}\\n\")\n",
    "        f.write(\"\\n=== Most Positive Samples ===\\n\")\n",
    "        for i, r in pos.iterrows():\n",
    "            f.write(f\"[{r['score']:.4f}] {r['text']}\\n\")\n",
    "    return out\n",
    "\n",
    "def bar_chart_html(labels, counts, avg_score, ts):\n",
    "    \"\"\"Interactive pyecharts bar.\"\"\"\n",
    "    bar = (\n",
    "        Bar()\n",
    "        .add_xaxis(labels)\n",
    "        .add_yaxis(\"Count\", counts)\n",
    "        .set_global_opts(\n",
    "            title_opts=opts.TitleOpts(\n",
    "                title=\"Danmaku Sentiment Distribution\",\n",
    "                subtitle=f\"Average Sentiment: {avg_score:.4f} | N={sum(counts)}\"\n",
    "            ),\n",
    "            xaxis_opts=opts.AxisOpts(name=\"Category\"),\n",
    "            yaxis_opts=opts.AxisOpts(name=\"Count\"),\n",
    "            toolbox_opts=opts.ToolboxOpts(),\n",
    "            datazoom_opts=[opts.DataZoomOpts()]\n",
    "        )\n",
    "        .set_series_opts(\n",
    "            label_opts=opts.LabelOpts(is_show=True),\n",
    "            markpoint_opts=opts.MarkPointOpts(data=[opts.MarkPointItem(type_=\"max\", name=\"Max\")]),\n",
    "        )\n",
    "    )\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_distribution_{ts}.html\")\n",
    "    bar.render(out)\n",
    "    return out\n",
    "\n",
    "def histogram_png(scores, avg_score, ts):\n",
    "    \"\"\"Static histogram with region shading.\"\"\"\n",
    "    plt.figure(figsize=(12,7))\n",
    "    n, bins, _ = plt.hist(scores, bins=30, edgecolor='black', alpha=0.75)\n",
    "    plt.axvline(x=avg_score, color='red', linestyle='--', linewidth=2, label=f'Average = {avg_score:.4f}')\n",
    "    # Shade regions for the five levels\n",
    "    plt.axvspan(0.0, 0.3, color='red', alpha=0.10)\n",
    "    plt.axvspan(0.3, 0.4, color='orange', alpha=0.10)\n",
    "    plt.axvspan(0.4, 0.6, color='gold', alpha=0.10)\n",
    "    plt.axvspan(0.6, 0.7, color='lightgreen', alpha=0.10)\n",
    "    plt.axvspan(0.7, 1.0, color='green', alpha=0.10)\n",
    "\n",
    "    plt.title(\"Distribution of Sentiment Scores\", fontsize=14)\n",
    "    plt.xlabel(\"Score (0=negative, 1=positive)\", fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    out = os.path.join(OUT_DIR, f\"sentiment_histogram_{ts}.png\")\n",
    "    plt.tight_layout(); plt.savefig(out, dpi=300); plt.close()\n",
    "    return out\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*64)\n",
    "    print(\"Danmaku Sentiment Analysis (SnowNLP, 5-level buckets)\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    path = find_latest_cleaned_file()\n",
    "    if not path: return\n",
    "\n",
    "    lines = load_cleaned_lines(path)\n",
    "    print(f\"[INFO] Loaded {len(lines)} cleaned danmaku lines\")\n",
    "\n",
    "    scores = score_texts(lines, show_progress=True)\n",
    "    avg = float(np.mean(scores)) if scores else 0.5\n",
    "    labels, counts = bucketize(scores)\n",
    "\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # Exports\n",
    "    scored_csv, df_scored = export_scored_table(lines, scores, ts)\n",
    "    extremes_txt = export_extremes(df_scored, ts, k=30)\n",
    "    html_bar = bar_chart_html(labels, counts, avg, ts)\n",
    "    hist_png = histogram_png(scores, avg, ts)\n",
    "\n",
    "    # Summary JSON (for paper/appendix reuse)\n",
    "    summary = {\n",
    "        \"n\": int(len(scores)),\n",
    "        \"average\": round(avg, 6),\n",
    "        \"buckets\": {labels[i]: int(counts[i]) for i in range(len(labels))},\n",
    "        \"ratios\": {labels[i]: round(counts[i]/max(1,len(scores)), 6) for i in range(len(labels))},\n",
    "        \"files\": {\n",
    "            \"scored_csv\": os.path.basename(scored_csv),\n",
    "            \"extremes_txt\": os.path.basename(extremes_txt),\n",
    "            \"bar_html\": os.path.basename(html_bar),\n",
    "            \"hist_png\": os.path.basename(hist_png),\n",
    "        }\n",
    "    }\n",
    "    json_path = os.path.join(OUT_DIR, f\"sentiment_summary_{ts}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Console report\n",
    "    print(\"\\n[SUMMARY]\")\n",
    "    print(f\"- N = {summary['n']} | Average = {summary['average']:.4f}\")\n",
    "    for k in labels:\n",
    "        c = summary[\"buckets\"][k]\n",
    "        r = summary[\"ratios\"][k]\n",
    "        print(f\"  {k:>14s}: {c:6d}  ({r*100:4.1f}%)\")\n",
    "    print(\"\\n[FILES]\")\n",
    "    for k, v in summary[\"files\"].items():\n",
    "        print(f\"- {k}: {os.path.join(OUT_DIR, v)}\")\n",
    "    print(f\"- summary_json: {json_path}\")\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T18:02:24.446972Z",
     "end_time": "2025-09-06T18:06:44.978753Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
