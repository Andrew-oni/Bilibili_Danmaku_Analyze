{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:18:49.287200Z",
     "end_time": "2025-09-06T16:18:59.172688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Bç«™å¼¹å¹•æŠ“å–ï¼ˆä¿å­˜ä¸º CSVï¼Œä¾¿äºåç»­æ¸…æ´—ï¼‰\n",
      "============================================================\n",
      "å…± 1 ä¸ªåˆ†Pï¼š\n",
      "- æŠ“å– 2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ (cid=29546709885) â€¦\n",
      "  2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ æŠ“åˆ° 1193 æ¡\n",
      "\n",
      "åˆè®¡æŠ“åˆ° 1193 æ¡å¼¹å¹•\n",
      "å·²ä¿å­˜ï¼šC:\\Users\\Andrew\\Desktop\\homework\\bb\\bili_danmu_results\\combined_raw_BV1GD5DzHEJo_202509061618.csv\n",
      "ä¹Ÿå†™äº†çº¯æ–‡æœ¬ï¼šC:\\Users\\Andrew\\Desktop\\homework\\bb\\bili_danmu_results\\combined_raw_BV1GD5DzHEJo_202509061618.txt\n",
      "å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "æŒ‰ BV æŠ“å–â€œå…¨é‡å¼¹å¹•â€ï¼ˆsegment_index é€’å¢åˆ°ç©ºï¼‰ï¼Œ\n",
    "å°†æ¯ä¸ªåˆ†Pä¿å­˜ä¸º CSVï¼ˆcontent/video_title/timestampï¼‰ï¼Œ\n",
    "å¹¶åœ¨ bili_danmu_results/ ç›®å½•ä¸‹å†ç”Ÿæˆåˆå¹¶ CSV/TXTã€‚\n",
    "\n",
    "ç”¨æ³•ï¼š\n",
    "python crawl_bili_danmu.py\n",
    "ç²˜è´´ BV æˆ– URLï¼ŒCookie å¯ç•™ç©ºã€‚\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, csv, datetime, sys\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ---------- å¸¸é‡ ----------\n",
    "HEADERS_BASE = {\n",
    "    \"origin\": \"https://www.bilibili.com\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DM_RE = re.compile(rb':(.*?)@', re.S)            # ä» seg.so ä¸­æå–å¼¹å¹•æ–‡æœ¬\n",
    "BV_RE = re.compile(r'(BV[0-9A-Za-z]{10,})')\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- å·¥å…· ----------\n",
    "def build_session(cookie=None, referer=None):\n",
    "    s = requests.Session()\n",
    "    headers = HEADERS_BASE.copy()\n",
    "    if cookie: headers[\"cookie\"] = cookie\n",
    "    if referer: headers[\"referer\"] = referer\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[412, 429, 500, 502, 503, 504])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries, pool_connections=16, pool_maxsize=16))\n",
    "    s.headers.update(headers)\n",
    "    return s\n",
    "\n",
    "def extract_bvid(text: str) -> str:\n",
    "    m = BV_RE.search(text)\n",
    "    if not m:\n",
    "        raise ValueError(\"æœªæ‰¾åˆ° BV å·ï¼Œè¯·æ£€æŸ¥è¾“å…¥ã€‚\")\n",
    "    return m.group(1)\n",
    "\n",
    "def get_pagelist_by_bvid(session, bvid):\n",
    "    \"\"\"ç”¨ pagelist å–æ‰€æœ‰åˆ†Pï¼š[{cid, page, part, duration}, ...]\"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/player/pagelist?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10); r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0:\n",
    "        raise RuntimeError(f\"pagelist æ¥å£å¤±è´¥ï¼šcode={j.get('code')} msg={j.get('message')}\")\n",
    "    pages = j[\"data\"] or []\n",
    "    if not pages:\n",
    "        raise RuntimeError(\"pagelist è¿”å›ç©ºï¼Œå¯èƒ½ BV æ— æ•ˆæˆ–éœ€è¦ç™»å½•ã€‚\")\n",
    "    return pages\n",
    "\n",
    "def fetch_seg_bytes(session, cid, seg_idx):\n",
    "    url = f\"https://api.bilibili.com/x/v2/dm/web/seg.so?type=1&oid={cid}&segment_index={seg_idx}\"\n",
    "    r = session.get(url, timeout=10); r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def parse_danmu_from_seg(seg_bytes):\n",
    "    # è¿™é‡ŒåªæŠ½å†…å®¹ï¼›è‹¥è¦æ›´ä¸°å¯Œå­—æ®µéœ€è§£æ protobuf\n",
    "    return [m.decode('utf-8', errors='ignore') for m in DM_RE.findall(seg_bytes)]\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def save_csv(rows, path):\n",
    "    # rows: list of dicts with keys: content, video_title, timestamp\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"content\", \"video_title\", \"timestamp\"])\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def save_txt(lines, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# ---------- ä¸»æµç¨‹ ----------\n",
    "def crawl_all_by_cid(session, cid, video_title, sleep_s=0.05):\n",
    "    all_items = []\n",
    "    seg = 1\n",
    "    empty_runs = 0\n",
    "    while True:\n",
    "        try:\n",
    "            seg_bytes = fetch_seg_bytes(session, cid, seg)\n",
    "        except requests.HTTPError:\n",
    "            time.sleep(1.0)\n",
    "            seg_bytes = fetch_seg_bytes(session, cid, seg)\n",
    "        contents = parse_danmu_from_seg(seg_bytes)\n",
    "        if not contents:\n",
    "            empty_runs += 1\n",
    "            if empty_runs >= 2:\n",
    "                break\n",
    "        else:\n",
    "            empty_runs = 0\n",
    "            for c in contents:\n",
    "                all_items.append({\n",
    "                    \"content\": c,\n",
    "                    \"video_title\": video_title,\n",
    "                    \"timestamp\": \"\"      # seg.so æœªè§£ææ—¶é—´æˆ³ï¼Œè¿™é‡Œç•™ç©ºå ä½\n",
    "                })\n",
    "        seg += 1\n",
    "        time.sleep(sleep_s)\n",
    "    return all_items\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Bç«™å¼¹å¹•æŠ“å–ï¼ˆä¿å­˜ä¸º CSVï¼Œä¾¿äºåç»­æ¸…æ´—ï¼‰\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    raw_input_str = input(\"è¾“å…¥ BV æˆ–å®Œæ•´é“¾æ¥ï¼š\").strip()\n",
    "    cookie = \"enable_web_push=DISABLE; buvid4=441674D3-73C6-EE18-D7F6-0A9AA6A8764B10929-024061616-tMB8uhs7bNpfYIQVaVKjtQ%3D%3D; DedeUserID=499303036; DedeUserID__ckMd5=7c2e754fb5285a0b; buvid_fp_plain=undefined; enable_feed_channel=ENABLE; hit-dyn-v2=1; fingerprint=d30d288e57446a6e2075d79545bcdece; buvid_fp=d30d288e57446a6e2075d79545bcdece; buvid3=D8E29309-F4DF-1535-6A27-C5ADFB04F3B208505infoc; b_nut=1750090608; _uuid=EE91029AA-C3ED-2D10D-A31B-8610DA6DC1BEE28428infoc; header_theme_version=OPEN; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; rpdid=|(J~R~uR))~u0J'u~lJkJl)Y|; LIVE_BUVID=AUTO2217563800546588; PVID=2; CURRENT_QUALITY=80; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTczMDE2NTEsImlhdCI6MTc1NzA0MjM5MSwicGx0IjotMX0.FWLzrbsTzZVO9pAvA2K-j8JpwDO4_00W2sqETwK91NA; bili_ticket_expires=1757301591; SESSDATA=fc10051c%2C1772610457%2C5b79d%2A91CjBHREFbDUZegrrFspSN0CJBiv6HiJF4vOUubDwDGrFJa2-yRA8gOqoqGpCwy29sjfwSVnVwNlBZR1hKb2VFaGI0NkZpcWZMYm5ubE5Kd2JFdkluaFp2Q1JfeFNLX1JqWWxxb19mUENfa193RkFUVDNPa3lZcUU4WmZrZ3ZlLXRpZ3RBRkQ5RnNRIIEC; bili_jct=e685ab8cae35933d20e223cb4b6a6d91; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; bsource=search_google; sid=7l86jgpi; bp_t_offset_499303036=1109422191797075968; b_lsid=D181510CA_1991E1E02C8; home_feed_column=5; browser_resolution=1530-770; CURRENT_FNVAL=2000\"#input(\"å¯é€‰ï¼šCookieï¼š\").strip() or None\n",
    "\n",
    "    bvid = extract_bvid(raw_input_str)\n",
    "    referer = f\"https://www.bilibili.com/video/{bvid}\"\n",
    "    sess = build_session(cookie=cookie, referer=referer)\n",
    "\n",
    "    pages = get_pagelist_by_bvid(sess, bvid)\n",
    "    print(f\"å…± {len(pages)} ä¸ªåˆ†Pï¼š\")\n",
    "\n",
    "    all_rows = []\n",
    "    for p in pages:\n",
    "        cid = p[\"cid\"]\n",
    "        part = p.get(\"part\") or f\"P{p.get('page', '?')}\"\n",
    "        print(f\"- æŠ“å– {part} (cid={cid}) â€¦\")\n",
    "        rows = crawl_all_by_cid(sess, cid, video_title=f\"{bvid} | {part}\")\n",
    "        print(f\"  {part} æŠ“åˆ° {len(rows)} æ¡\")\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "        # åˆ†På„è‡ªä¿å­˜\n",
    "        csv_path = os.path.join(RAW_DIR, f\"{bvid}_{cid}_{part}.csv\")\n",
    "        save_csv(rows, csv_path)\n",
    "\n",
    "    # åˆå¹¶ä¿å­˜ï¼ˆCSV + TXTï¼‰\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    combined_csv = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.csv\")\n",
    "    combined_txt = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.txt\")\n",
    "    save_csv(all_rows, combined_csv)\n",
    "    save_txt([r[\"content\"] for r in all_rows], combined_txt)\n",
    "\n",
    "    print(f\"\\nåˆè®¡æŠ“åˆ° {len(all_rows)} æ¡å¼¹å¹•\")\n",
    "    print(f\"å·²ä¿å­˜ï¼š{combined_csv}\")\n",
    "    print(f\"ä¹Ÿå†™äº†çº¯æ–‡æœ¬ï¼š{combined_txt}\")\n",
    "    print(\"å®Œæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Bç«™å¼¹å¹•æ‰¹é‡æ¸…æ´—å·¥å…·ï¼ˆå¢å¼ºç‰ˆ v2ï¼‰\n",
      "==================================================\n",
      "è¯»å–: BV17pZfYLEu6_29199436543_å°ç±³su7äº‹ä»¶ï¼Œ4æœˆ1å·ä¿¡æ¯åˆ†æğŸ§.csvï¼ˆ2582 æ¡ï¼‰\n",
      "è¯»å–: BV1Bh3jzxE2q_30840851939_å¤–å›½è½¦è¯„äººæ¥åˆ°æ­¦æ±‰ï¼Œæ„Ÿå—æ— äººé©¾é©¶ï¼Œæ‚¬æµ®ç©ºè½¨ï¼Œå®Œå…¨é¢ è¦†è®¤çŸ¥ï¼.csvï¼ˆ3529 æ¡ï¼‰\n",
      "è¯»å–: BV1bN9PYeEBm_28603977518_50å¤šä¸‡1548åŒ¹å°ç±³SU7ï¼Œæ™ºé©¾æ°´å¹³åº”è¯¥ä¸æ€ä¹ˆæ ·å§ï¼Ÿ.csvï¼ˆ2851 æ¡ï¼‰\n",
      "è¯»å–: BV1bxb1zPEUJ_31257921527_é¦–æ¬¡ä½“éªŒèåœå¿«è·‘æ— äººé©¾é©¶å‡ºç§Ÿè½¦.csvï¼ˆ1724 æ¡ï¼‰\n",
      "è¯»å–: BV1D197YCEsW_28692122756_å…¨ç½‘å”¯ä¸€æœ€çœŸå®å¯¹æ¯”ï¼Œç‰¹æ–¯æ‹‰ã€åä¸ºä»€ä¹ˆæ°´å¹³ï¼Ÿã€Œç‰¹æ–¯æ‹‰VSåä¸ºã€.csvï¼ˆ2666 æ¡ï¼‰\n",
      "è¯»å–: BV1f7b6zPESj_31686987287_é¦†é•¿8.14æ·±åœ³è¡Œâ‘£ï¼Œä½“éªŒæ— äººé©¾é©¶å‡ºç§Ÿè½¦ï¼šå“å“Ÿå–‚å‘€ï¼Œç¬¬ä¸€æ¬¡åï¼ŒçœŸæ˜¯å“æ­»æˆ‘äº†å¥½ä¸å¥½ï¼Ÿ.csvï¼ˆ1962 æ¡ï¼‰\n",
      "è¯»å–: BV1fcXQYUEik_28632024035_å…¨ç½‘æœ€å…¨â€œç‰¹æ–¯æ‹‰FSDå¯¹æ¯”åä¸ºADSâ€ï¼Œä¸­ç¾æ™ºé©¾æœ‰å¤šå¤§å·®è·ï¼Ÿã€ç§‘æŠ€ç‹ã€‘.csvï¼ˆ2299 æ¡ï¼‰\n",
      "è¯»å–: BV1fcXQYUEik_28696839809_ç‰¹æ–¯æ‹‰FSDå¤œé—´å¹¿å·éƒŠåŒºåˆ°åŸåŒºä¸€é•œåˆ°åº•.csvï¼ˆ8 æ¡ï¼‰\n",
      "è¯»å–: BV1fcXQYUEik_28696905721_ç‰¹æ–¯æ‹‰FSDå¹¿å·åŸä¸­æ‘ä¸€é•œåˆ°åº•.csvï¼ˆ5 æ¡ï¼‰\n",
      "è¯»å–: BV1GD5DzHEJo_29546709885_2025å¹´äº†ï¼Œåä¸ºè¾…åŠ©é©¾é©¶æœ‰ä»€ä¹ˆæ§½ç‚¹ï¼Ÿ.csvï¼ˆ1193 æ¡ï¼‰\n",
      "è¯»å–: BV1h5fnYcEyC_28054652505_å±é™©å±é™©å±é™©ï¼è½¦è¾†â€œè‡ªåŠ¨é©¾é©¶â€ï¼Œå¸æœºç›–è¢«ç¡è§‰ï¼Ÿ.csvï¼ˆ1608 æ¡ï¼‰\n",
      "è¯»å–: BV1hBuFzfEY5_31011965200_ã€å¤§è™¾æ²‰æµ¸å¼è¯•é©¾ã€‘å²šå›¾FREE+ğŸ‘‰æ™ºèƒ½é©¾é©¶Â·åº•ç›˜Â·ç™¾å…¬é‡ŒåŠ é€Ÿå…¨çŸ¥é“ï¼.csvï¼ˆ1943 æ¡ï¼‰\n",
      "è¯»å–: BV1kNZ1YJEPH_29212607774_æ™ºèƒ½é©¾é©¶â‰ è‡ªåŠ¨é©¾é©¶ï¼å²šå›¾æ¢¦æƒ³å®¶â€œæ™ºé©¾â€è¦å¦‚ä½•æ›´å®‰å…¨ï¼Ÿ.csvï¼ˆ734 æ¡ï¼‰\n",
      "è¯»å–: BV1TRLEzpE6k_29618405918_å°ç±³SU7äº‹ä»¶ï¼Œåº”è¯¥æ•™ä¼šæˆ‘ä»¬æ›´å¤š   ã€ä¸‹å°ºæŠ¥å‘Šã€‘.csvï¼ˆ4598 æ¡ï¼‰\n",
      "è¯»å–: BV1v7PjegENh_28605743916_ä¸€é•œåˆ°åº•ï¼šç‰¹æ–¯æ‹‰FSDæŒ‘æˆ˜ã€Œæ™ºé—¯çº½åŒ—ã€.csvï¼ˆ4489 æ¡ï¼‰\n",
      "è¯»å–: BV1wWfDYiEUi_28111012375_ã€æ·±åº¦è§£æã€‘æ™ºèƒ½é©¾é©¶çƒ§äº†1000äº¿ï¼Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ.csvï¼ˆ7443 æ¡ï¼‰\n",
      "è¯»å–: BV1WxGwz4EVg_30933650317_video1752033400968.csvï¼ˆ1618 æ¡ï¼‰\n",
      "è¯»å–: BV1Y1JezEELN_30066738154_lv_0_20250520225935.csvï¼ˆ535 æ¡ï¼‰\n",
      "è¯»å–: BV1ZaACeDEXU_28548860733_è‡ªæè…°åŒ…å…¨ç½‘é¦–æµ‹ï¼Œ9.98ä¸‡çš„æ¯”äºšè¿ªè‡ªåŠ¨æ³Šè½¦å¥½ç”¨å—ï¼Ÿã€Œç§¦Lã€.csvï¼ˆ1523 æ¡ï¼‰\n",
      "è¯»å–: BV1ZDGYz6Eko_29727722007_é•¿åŸæ±½è½¦æ™ºèƒ½åŒ–ï¼Œå¦‚ä½•å‘å‰èµ°ï¼Ÿ.csvï¼ˆ967 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV17pZfYLEu6_202509061611.csvï¼ˆ2582 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1Bh3jzxE2q_202509061600.csvï¼ˆ3529 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1bN9PYeEBm_202509061608.csvï¼ˆ2851 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1bxb1zPEUJ_202509061601.csvï¼ˆ1724 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1D197YCEsW_202509061611.csvï¼ˆ2666 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1D197YCEsW_202509061615.csvï¼ˆ2666 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1f7b6zPESj_202509061601.csvï¼ˆ1962 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1fcXQYUEik_202509061612.csvï¼ˆ2312 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1fcXQYUEik_202509061616.csvï¼ˆ2312 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1GD5DzHEJo_202509061618.csvï¼ˆ1193 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1h5fnYcEyC_202509061617.csvï¼ˆ1608 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1hBuFzfEY5_202509061616.csvï¼ˆ1943 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1kNZ1YJEPH_202509061608.csvï¼ˆ734 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1TRLEzpE6k_202509061610.csvï¼ˆ4598 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1v7PjegENh_202509061609.csvï¼ˆ4489 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1wWfDYiEUi_202509061557.csvï¼ˆ7443 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1WxGwz4EVg_202509061617.csvï¼ˆ1618 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1Y1JezEELN_202509061602.csvï¼ˆ535 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1ZaACeDEXU_202509061618.csvï¼ˆ1523 æ¡ï¼‰\n",
      "è¯»å–: combined_raw_BV1ZDGYz6Eko_202509061610.csvï¼ˆ967 æ¡ï¼‰\n",
      "æ€»è®¡è¯»å– 93532 æ¡\n",
      "\n",
      "ä¿ç•™ 36167/93532 (38.7%)\n",
      "\n",
      "æ¸…æ´—åä¿å­˜ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\combined_cleaned_danmu_202509061626.txt\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "ç§»é™¤åŸå› åˆ†å¸ƒï¼š {'noise': 7733, 'duplicate': 49632}\n",
      "ç§»é™¤æ˜ç»†ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\removed_reasons_202509061626.csv\n",
      "è°ƒè¯•å¯¹ç…§æ ·æœ¬ï¼š\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\debug_original_vs_cleaned_202509061626.csv\n",
      "\n",
      "å®Œæˆã€‚\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bç«™å¼¹å¹•æ‰¹é‡æ¸…æ´—å·¥å…·ï¼ˆå¢å¼ºç‰ˆ v2ï¼‰\n",
    "- è¯»å– bili_danmu_results/*.csv ï¼ˆå­—æ®µè‡³å°‘å« contentï¼‰\n",
    "- â€œå…ˆæŠ½å–ï¼Œå†æ¸…æ´—â€ï¼šä¼˜å…ˆæˆªå–å†’å·åçš„æ­£æ–‡ï¼›å‰¥ç¦»å‰ç¼€å™ªå£°å—ï¼›å»æ§åˆ¶å­—ç¬¦\n",
    "- è¾“å‡º cleaned_danmu_results/ï¼š\n",
    "    - combined_cleaned_danmu_*.txt / *.csv\n",
    "    - removed_reasons_*.csv ï¼ˆè¢«åˆ é™¤æ¡ç›®ä¸åŸå› ï¼‰\n",
    "    - debug_original_vs_cleaned_*.csv ï¼ˆæŠ½æ ·å¯¹ç…§ï¼Œä¾¿äºäººå·¥å¤æ ¸ï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "OUT_DIR = os.path.join(os.getcwd(), \"cleaned_danmu_results\")\n",
    "\n",
    "# ---------- é¢„ç¼–è¯‘æ­£åˆ™ ----------\n",
    "RE_CTRL = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')                    # æ§åˆ¶å­—ç¬¦\n",
    "RE_HTML = re.compile(r'<[^>]+>')\n",
    "RE_URL  = re.compile(r'https?://\\S+')\n",
    "# åªä¿ç•™ï¼šä¸­è‹±æ–‡ã€æ•°å­—ã€å¸¸è§ä¸­æ–‡æ ‡ç‚¹ã€è¿å­—ç¬¦\n",
    "RE_KEEP = re.compile(r'[^\\w\\u4e00-\\u9fa5ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š\"\\'()ã€ã€‘ã€Šã€‹Â·â€”â€¦\\-]+')\n",
    "RE_SP   = re.compile(r'\\s+')\n",
    "RE_MULTI_PUNCT = re.compile(r'([ï¼Œã€‚ï¼ï¼Ÿâ€¦])\\1+')\n",
    "# è¡Œé¦–å™ªå£°å—ï¼ˆæ‹¬å·/å¥‡æ€ªå­—èŠ‚/çŸ­hex/å­¤ç«‹å­—æ¯/æ ‡ç‚¹ï¼‰åå¤å‡ºç°çš„ç»„åˆ\n",
    "RE_NOISY_PREFIX = re.compile(\n",
    "    r'^\\s*(?:[\\(\\)\\[\\]{}<>\\|\\\\/\\-_=+~^`Â·â€¢]+|[A-Za-z](?=\\s)|[0-9A-Fa-f]{6,10}|[,.:;ï¼Œã€‚ï¼›ï¼š!ï¼?ï¼Ÿ\\s])+'\n",
    ")\n",
    "\n",
    "def _cut_after_colon(raw: str) -> str:\n",
    "    \"\"\"è‹¥å«å†’å·ï¼Œå–æœ€åä¸€ä¸ªå†’å·åçš„ç‰‡æ®µï¼ˆæ›´æ¥è¿‘æ­£æ–‡ï¼‰\"\"\"\n",
    "    if ':' in raw:\n",
    "        return raw.split(':')[-1]\n",
    "    return raw\n",
    "\n",
    "def _strip_to_first_textual_char(s: str) -> str:\n",
    "    \"\"\"\n",
    "    ä»å¼€å¤´å‰¥ç¦»â€œå™ªå£°å—â€ï¼Œç›´åˆ°é‡åˆ°ç¬¬ä¸€ä¸ªä¸­æ–‡/è‹±æ–‡/æ•°å­—ï¼›\n",
    "    å…ˆç”¨ä¸€ä¸ªå¿«é€Ÿå‰ç¼€æ¸…ç†ï¼Œå†ç²¾ç¡®å®šä½ç¬¬ä¸€ä¸ªæœ‰æ•ˆå­—ç¬¦ã€‚\n",
    "    \"\"\"\n",
    "    s = RE_NOISY_PREFIX.sub('', s)\n",
    "    idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch.isalnum() or ('\\u4e00' <= ch <= '\\u9fa5'):\n",
    "            idx = i\n",
    "            break\n",
    "    return s[idx:] if idx is not None else ''\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    s = RE_CTRL.sub(' ', s)                    # åˆ æ§åˆ¶å­—ç¬¦\n",
    "    s = unicodedata.normalize('NFKC', s)       # å…¨è§’â†’åŠè§’\n",
    "    s = RE_HTML.sub('', s)                     # å»HTML\n",
    "    s = RE_URL.sub('', s)                      # å»URL\n",
    "    s = RE_KEEP.sub(' ', s)                    # è¿‡æ»¤åˆ°å…è®¸å­—ç¬¦é›†\n",
    "    s = RE_MULTI_PUNCT.sub(r'\\1', s)           # è¿ç»­æ ‡ç‚¹å‹ç¼©\n",
    "    s = RE_SP.sub(' ', s).strip()              # ç©ºç™½è§„æ•´\n",
    "    return s\n",
    "\n",
    "def _looks_like_noise(s: str) -> bool:\n",
    "    \"\"\"å™ªå£°åˆ¤å®šï¼šçº¯æ•°å­—/å•å­—æ¯/å¤ªçŸ­/æ–‡å­—å æ¯”ä½\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    if s.isdigit():\n",
    "        return True\n",
    "    if len(s) == 1 and s.isalnum():\n",
    "        return True\n",
    "    if len(s) < 2:\n",
    "        return True\n",
    "    # æ–‡å­—å æ¯”ï¼ˆä¸­è‹±æ•°å­—ï¼‰\n",
    "    total = len(s)\n",
    "    keep = sum(1 for ch in s if ch.isalnum() or '\\u4e00' <= ch <= '\\u9fa5')\n",
    "    if keep / max(1, total) < 0.3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_one(raw: str) -> str:\n",
    "    \"\"\"å¯¹å•æ¡å¼¹å¹•åšâ€˜æŠ½å–â†’å»å™ªâ†’è§„èŒƒåŒ–â€™\"\"\"\n",
    "    s = str(raw or '')\n",
    "    s = _cut_after_colon(s)            # å…ˆæˆªå†’å·å\n",
    "    s = RE_CTRL.sub(' ', s)            # å»æ§åˆ¶ç¬¦ï¼ˆä¿è¯åç»­å®šä½ä¸è¢«å¹²æ‰°ï¼‰\n",
    "    s = _strip_to_first_textual_char(s)# ä¸¢å¼ƒå‰ç¼€å™ªå£°å—\n",
    "    s = _normalize(s)                  # å½’ä¸€åŒ–+è¿‡æ»¤\n",
    "    if len(s) > 100:                   # ä¸Šé™ä¿æŠ¤\n",
    "        s = s[:100].rstrip()\n",
    "    return s\n",
    "\n",
    "def clean_danmu_content(records):\n",
    "    \"\"\"\n",
    "    è¾“å…¥ï¼šå­—å…¸åˆ—è¡¨ï¼ˆè‡³å°‘å« 'content'ï¼‰\n",
    "    è¾“å‡ºï¼š\n",
    "      - cleaned: [{original, cleaned, video_title, timestamp}]\n",
    "      - removed: [(original, reason)]\n",
    "    \"\"\"\n",
    "    cleaned, removed = [], []\n",
    "\n",
    "    for dm in records:\n",
    "        raw = str(dm.get('content', '') or '')\n",
    "        if not raw:\n",
    "            removed.append((raw, \"empty\"))\n",
    "            continue\n",
    "\n",
    "        out = clean_one(raw)\n",
    "\n",
    "        if _looks_like_noise(out):\n",
    "            removed.append((raw, \"noise\"))\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\n",
    "            \"original\": raw,\n",
    "            \"cleaned\": out,\n",
    "            \"video_title\": dm.get(\"video_title\", \"æœªçŸ¥è§†é¢‘\"),\n",
    "            \"timestamp\": dm.get(\"timestamp\", \"\")\n",
    "        })\n",
    "\n",
    "    # å»é‡ï¼ˆä»¥ cleaned ä¸ºé”®ï¼‰\n",
    "    seen, dedup = set(), []\n",
    "    for row in cleaned:\n",
    "        key = row['cleaned']\n",
    "        if key in seen:\n",
    "            removed.append((row['original'], \"duplicate\"))\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(row)\n",
    "\n",
    "    return dedup, removed\n",
    "\n",
    "# ---------- æ–‡ä»¶ I/O ----------\n",
    "def find_csv_files(directory):\n",
    "    files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    return [f for f in files\n",
    "            if '_cleaned.csv' not in os.path.basename(f)\n",
    "            and not os.path.basename(f).startswith('combined_cleaned_')]\n",
    "\n",
    "def read_all_records(paths):\n",
    "    allrows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            for col in [\"content\", \"video_title\", \"timestamp\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = \"\"\n",
    "            df = df[[\"content\", \"video_title\", \"timestamp\"]]\n",
    "            print(f\"è¯»å–: {os.path.basename(p)}ï¼ˆ{len(df)} æ¡ï¼‰\")\n",
    "            allrows.extend(df.to_dict(\"records\"))\n",
    "        except Exception as e:\n",
    "            print(f\"è¯»å–å¤±è´¥: {p} - {e}\")\n",
    "    print(f\"æ€»è®¡è¯»å– {len(allrows)} æ¡\")\n",
    "    return allrows\n",
    "\n",
    "def save_outputs(cleaned, removed):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # ä¸»è¾“å‡º\n",
    "    txt_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.txt\")\n",
    "    csv_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.csv\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in cleaned:\n",
    "            f.write(r[\"cleaned\"] + \"\\n\")\n",
    "    pd.DataFrame(cleaned).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\næ¸…æ´—åä¿å­˜ï¼š\\n- {txt_path}\\n- {csv_path}\")\n",
    "\n",
    "    # è¢«ç§»é™¤æ¡ç›®ç»Ÿè®¡\n",
    "    if removed:\n",
    "        rm_path = os.path.join(OUT_DIR, f\"removed_reasons_{ts}.csv\")\n",
    "        pd.DataFrame(removed, columns=[\"original\", \"reason\"]).to_csv(\n",
    "            rm_path, index=False, encoding=\"utf-8-sig\")\n",
    "        cnt = Counter([r for _, r in removed])\n",
    "        print(\"ç§»é™¤åŸå› åˆ†å¸ƒï¼š\", dict(cnt))\n",
    "        print(f\"ç§»é™¤æ˜ç»†ï¼š\\n- {rm_path}\")\n",
    "\n",
    "    # è°ƒè¯•å¯¹ç…§ï¼ˆéšæœºæŠ½æ ·æˆ–å‰Næ¡ï¼‰\n",
    "    dbg_path = os.path.join(OUT_DIR, f\"debug_original_vs_cleaned_{ts}.csv\")\n",
    "    dbg_df = pd.DataFrame(cleaned)[[\"original\", \"cleaned\"]].head(200)\n",
    "    dbg_df.to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"è°ƒè¯•å¯¹ç…§æ ·æœ¬ï¼š\\n- {dbg_path}\")\n",
    "\n",
    "# ---------- ä¸»æµç¨‹ ----------\n",
    "def main():\n",
    "    print(\"=\"*50)\n",
    "    print(\"Bç«™å¼¹å¹•æ‰¹é‡æ¸…æ´—å·¥å…·ï¼ˆå¢å¼ºç‰ˆ v2ï¼‰\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    paths = find_csv_files(RAW_DIR)\n",
    "    if not paths:\n",
    "        print(f\"æœªåœ¨ {RAW_DIR} æ‰¾åˆ°å¾…å¤„ç† CSV\"); return\n",
    "\n",
    "    records = read_all_records(paths)\n",
    "    if not records:\n",
    "        print(\"æ²¡æœ‰å¯å¤„ç†æ•°æ®\"); return\n",
    "\n",
    "    cleaned, removed = clean_danmu_content(records)\n",
    "    print(f\"\\nä¿ç•™ {len(cleaned)}/{len(records)} \"\n",
    "          f\"({len(cleaned)/max(1,len(records))*100:.1f}%)\")\n",
    "\n",
    "    save_outputs(cleaned, removed)\n",
    "    print(\"\\nå®Œæˆã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:26:09.063372Z",
     "end_time": "2025-09-06T16:26:11.237332Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "å¼¹å¹•æ–‡æœ¬åˆ†æ Proï¼ˆè¯é¢‘/è¯äº‘/è‡ªé€‚åº”èšç±»/æƒ…æ„Ÿï¼‰\n",
      "================================================================\n",
      "[INFO] åˆ†ææ–‡ä»¶ï¼šcleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "[INFO] æ¸…æ´—åè¡Œæ•°ï¼š36165\n",
      "[INFO] åˆ†è¯åæ–‡æ¡£æ•°ï¼š32655 ï¼›æ€»è¯æ•°ï¼š97466\n",
      "[OK] è¯é¢‘è¡¨ï¼šanalysis_results\\word_frequency_202509061630.csv\n",
      "[OK] è¯äº‘ï¼šanalysis_results\\wordcloud_202509061630.png\n",
      "[OK] Top bigramï¼šanalysis_results\\top_bigram_202509061630.csv\n",
      "[INFO] å¼€å§‹è‡ªé€‚åº”èšç±» â€¦\n",
      "[INFO] è½®å»“ç³»æ•°æœ€ä¼˜ï¼šK=8ï¼Œscore=0.0172\n",
      "[OK] èšç±»æ€»è§ˆå›¾ï¼šanalysis_results\\cluster_summary_202509061632.png\n",
      "[OK] ä¸»é¢˜å…³é”®è¯è¡¨ï¼šanalysis_results\\cluster_keywords_202509061630.csv\n",
      "[OK] ä¸»é¢˜æƒ…æ„Ÿè¡¨ï¼šanalysis_results\\cluster_sentiment_202509061630.csv\n",
      "\n",
      "è¯é¢‘TOP10ï¼š\n",
      "1. é©¾é©¶  1849\n",
      "2. è‡ªåŠ¨  1241\n",
      "3. æ²¡æœ‰  1066\n",
      "4. æ™ºé©¾  1042\n",
      "5. é—®é¢˜  975\n",
      "6. åä¸º  860\n",
      "7. å°ç±³  691\n",
      "8. è¯†åˆ«  651\n",
      "9. è¾…åŠ©  553\n",
      "10. å¸æœº  542\n",
      "\n",
      "å®Œæˆã€‚è¾“å‡ºç›®å½•ï¼šanalysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bç«™å¼¹å¹•åˆ†æ Proï¼šè¯é¢‘ / è¯äº‘ / è‡ªé€‚åº”èšç±» / æƒ…æ„Ÿï¼Œé€‚é… cleaned_danmu_results è¾“å‡º\n",
    "- è‡ªåŠ¨è¯»å–æœ€æ–° cleaned æ–‡ä»¶ï¼ˆcsv ä¼˜å…ˆï¼Œå…¶æ¬¡ txtï¼‰\n",
    "- åˆ†è¯ï¼šè¯æ€§è¿‡æ»¤ + è¡Œä¸šè¯å…¸ï¼ˆå¯é€‰ï¼‰\n",
    "- å‘é‡åŒ–ï¼š1-2gram TF-IDFã€min_df/max_df æŠ‘åˆ¶å£æ°´è¯\n",
    "- è‡ªé€‚åº” KMeansï¼ˆè½®å»“ç³»æ•°é€‰ Kï¼‰\n",
    "- ä¸»é¢˜å‘½åï¼šç”¨éåœç”¨è¯å…³é”®è¯å‰2ä¸ª\n",
    "- æƒ…æ„Ÿï¼šSnowNLPï¼ˆ<4å­—è§†ä¸ºä¸­æ€§ï¼‰ï¼ŒæŒ‰ä¸»é¢˜æ±‡æ€»\n",
    "- å¯è§†åŒ–ï¼šè¯äº‘ã€ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾ + æƒ…æ„Ÿå †å æŸ±çŠ¶ + å…³é”®è¯è¡¨\n",
    "- é¢å¤–å¯¼å‡ºï¼šè¯é¢‘ã€ä¸»é¢˜å…³é”®è¯è¡¨ã€ä¸»é¢˜æƒ…æ„Ÿè¡¨ã€Top n-gram\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ===================== å¯è°ƒå‚æ•° =====================\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"      # é€šç”¨åœç”¨è¯ï¼Œå¦‚æ— å¯ä¸ºç©º\n",
    "USER_DICT   = \"user_dict.txt\"         # è¡Œä¸šè¯å…¸ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "# è¯æ€§ä¿ç•™ï¼šåè¯/åŠ¨è¯/è‹±æ–‡/æœ¯è¯­/ç»„ç»‡/äººåç­‰\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# é¢å¤–å£è¯­åœç”¨è¯ï¼ˆå¼¹å¹•é«˜é¢‘â€œæ— ä¿¡æ¯â€è¯ï¼‰\n",
    "DANMU_STOP_EXTRA = {\n",
    "    \"è¿™ä¸ª\",\"é‚£ä¸ª\",\"å°±æ˜¯\",\"ä»€ä¹ˆ\",\"è¿˜æœ‰\",\"ç„¶å\",\"ä½†æ˜¯\",\"æ‰€ä»¥\",\"è¿˜æ˜¯\",\"å·²ç»\",\"çœŸçš„\",\"æ„Ÿè§‰\",\"è§‰å¾—\",\"çŸ¥é“\",\n",
    "    \"å¯ä»¥\",\"ä¸å¯ä»¥\",\"ä¸ä¼š\",\"ä¸èƒ½\",\"åº”è¯¥\",\"å¯èƒ½\",\"è¿˜æ˜¯\",\"æœ‰ç‚¹\",\"æœ‰äº›\",\"æ€ä¹ˆ\",\"ä¸ºå•¥\",\"ä¸ºä»€ä¹ˆ\",\n",
    "    \"å•Š\",\"å‘€\",\"å‘¢\",\"å§\",\"å“¦\",\"å“‡\",\"è¯¶\",\"å˜›\",\"å“ˆ\",\"å“ˆå“ˆ\",\"å“ˆå“ˆå“ˆ\",\"emm\",\"å—¯\",\"å•Šå•Š\",\"å‘œå‘œ\",\n",
    "    \"è§†é¢‘\",\"å¼¹å¹•\",\"ç°åœ¨\",\"ä»Šå¤©\",\"æ˜¨å¤©\",\"æ˜å¤©\",\"è¿™é‡Œ\",\"é‚£é‡Œ\",\"è¿™æ ·\",\"é‚£æ ·\",\"å¾ˆå¤š\",\"éå¸¸\"\n",
    "}\n",
    "\n",
    "# TF-IDF å‚æ•°\n",
    "MAX_FEATURES = 4000\n",
    "MIN_DF       = 5       # å‡ºç°ä¸åˆ° MIN_DF ä¸ªæ–‡æ¡£çš„ç‰¹å¾ä¸¢å¼ƒ\n",
    "MAX_DF       = 0.6     # å‡ºç°åœ¨ >60% æ–‡æ¡£çš„ç‰¹å¾ä¸¢å¼ƒ\n",
    "NGRAM        = (1,2)\n",
    "\n",
    "# è‡ªé€‚åº” K èŒƒå›´\n",
    "K_MIN, K_MAX = 2, 8\n",
    "\n",
    "# è¯äº‘ TopN\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# å°å¥æƒ…æ„Ÿå¤„ç†ï¼ˆé•¿åº¦<4åˆ¤ä¸­æ€§ï¼‰\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# ---------------- å­—ä½“è®¾ç½® ----------------\n",
    "def setup_chinese_font():\n",
    "    candidates = ['SimHei','Microsoft YaHei','SimSun','KaiTi',\n",
    "                  'Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for name in candidates:\n",
    "        if any(name in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = name; break\n",
    "    if picked:\n",
    "        plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                font_path = f; break\n",
    "    if not font_path:\n",
    "        font_path = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return font_path\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "\n",
    "# ---------------- å·¥å…·å‡½æ•° ----------------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    # å¸¸è§ç¬¦å· & å¼¹å¹•å£è¯­è¡¥å……\n",
    "    base |= set(list(\"ï¼Œã€‚ã€ï¼ï¼Ÿï¼›ï¼šâ€â€œâ€˜â€˜ï¼ˆï¼‰()[]ã€ã€‘â€”â€¦- \"))\n",
    "    base |= DANMU_STOP_EXTRA\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] å·²åŠ è½½è¡Œä¸šè¯å…¸ï¼š{USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    files = csvs + txts\n",
    "    if not files: return None\n",
    "    return max(files, key=os.path.getmtime)\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        col = \"cleaned\" if \"cleaned\" in df.columns else df.columns[0]\n",
    "        lines = [str(x) for x in df[col].fillna(\"\").tolist()]\n",
    "    else:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    # è½»é‡å»å™ª + å»é‡\n",
    "    cleaned, seen = [], set()\n",
    "    for s in lines:\n",
    "        s = CTRL_RE.sub(' ', s)\n",
    "        s = EMOJI_RE.sub('', s)\n",
    "        s = re.sub(r'\\s+',' ', s).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:        # å…è®¸çŸ­å¥ï¼Œä½†å¤ªçŸ­å°±è·³è¿‡\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)     # ä¿ç•™åŸå¥ç”¨äºæƒ…æ„Ÿ\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter, top_n=WORDCLOUD_TOPN):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(top_n)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('å¼¹å¹•é«˜é¢‘è¯äº‘', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "# --- n-gram è¾…åŠ©ï¼ˆç»Ÿè®¡ top bigramï¼‰ ---\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------------- èšç±»ï¼ˆè‡ªé€‚åº”Kï¼‰ ----------------\n",
    "def auto_kmeans(docs, min_k=K_MIN, max_k=K_MAX, max_features=MAX_FEATURES):\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # å°‘æ ·æœ¬ï¼šå›ºå®šè¾ƒå°K\n",
    "    if n < 60:\n",
    "        k = max(2, min(4, n // 15 or 2))\n",
    "        model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        return model, vec, None\n",
    "\n",
    "    # è½®å»“ç³»æ•°é€‰æ‹© Kï¼ˆæŠ½æ ·è¯„ä¼°ï¼‰\n",
    "    if n > 6000:\n",
    "        idx = np.random.RandomState(42).choice(n, 6000, replace=False)\n",
    "        X_eval = X[idx]\n",
    "    else:\n",
    "        X_eval = X\n",
    "\n",
    "    best_k, best_score, best_model = None, -1, None\n",
    "    for k in range(min_k, max_k+1):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        labels_eval = km.labels_ if X_eval is X else km.predict(X_eval)\n",
    "        score = silhouette_score(X_eval, labels_eval, metric='cosine')\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_model = k, score, km\n",
    "    return best_model, vec, best_score\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in order[i, :topn*3]:  # å¤šå–å†è¿‡æ»¤\n",
    "            if j < len(terms):\n",
    "                t = terms[j]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"ä¸»é¢˜{i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # é¥¼å›¾ï¼šä¸»é¢˜åˆ†å¸ƒ\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt}æ¡ ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"ä¸»é¢˜åˆ†å¸ƒ\", fontsize=15)\n",
    "\n",
    "    # æŸ±çŠ¶ï¼šæƒ…æ„Ÿå †å ï¼ˆç»¿/è“/çº¢ï¼‰\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='ç§¯æ')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='ä¸­æ€§')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='æ¶ˆæ')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"æ¯”ä¾‹\"); ax2.set_title(\"æƒ…æ„Ÿåˆ†å¸ƒ\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # ä¸»é¢˜å…³é”®è¯è¡¨\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"å…³é”®è¯{i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    # è¡¥é½ä¸ç­‰é•¿\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"ä¸»é¢˜å…³é”®è¯\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------- ä¸»æµç¨‹ ----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"å¼¹å¹•æ–‡æœ¬åˆ†æ Proï¼ˆè¯é¢‘/è¯äº‘/è‡ªé€‚åº”èšç±»/æƒ…æ„Ÿï¼‰\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_file()\n",
    "    if not latest:\n",
    "        print(\"æœªæ‰¾åˆ°æ¸…æ´—åçš„æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œæ¸…æ´—ã€‚\"); return\n",
    "    print(f\"[INFO] åˆ†ææ–‡ä»¶ï¼š{latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "    lines = load_cleaned_lines(latest)\n",
    "    print(f\"[INFO] æ¸…æ´—åè¡Œæ•°ï¼š{len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] åˆ†è¯åä¸ºç©ºï¼ˆå¯èƒ½åœç”¨è¯è¿‡å¤šæˆ–æ–‡æœ¬å¤ªçŸ­ï¼‰ã€‚\"); return\n",
    "    print(f\"[INFO] åˆ†è¯åæ–‡æ¡£æ•°ï¼š{len(docs)} ï¼›æ€»è¯æ•°ï¼š{len(all_words)}\")\n",
    "\n",
    "    # è¯é¢‘ã€è¯äº‘ã€Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter)\n",
    "    print(f\"[OK] è¯é¢‘è¡¨ï¼š{freq_csv}\")\n",
    "\n",
    "    wc_path = draw_wordcloud(counter)\n",
    "    if wc_path: print(f\"[OK] è¯äº‘ï¼š{wc_path}\")\n",
    "\n",
    "    bigram_top = top_ngrams(docs, n=2, topk=50)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram_top, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigramï¼š{bigram_csv}\")\n",
    "\n",
    "    # èšç±»\n",
    "    if len(docs) >= 20:\n",
    "        print(\"[INFO] å¼€å§‹è‡ªé€‚åº”èšç±» â€¦\")\n",
    "        model, vec, s_score = auto_kmeans(docs)\n",
    "        if s_score is not None:\n",
    "            print(f\"[INFO] è½®å»“ç³»æ•°æœ€ä¼˜ï¼šK={model.n_clusters}ï¼Œscore={s_score:.4f}\")\n",
    "        labels = model.labels_\n",
    "\n",
    "        # ä¸»é¢˜å…³é”®è¯ä¸å‘½å\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, DANMU_STOP_EXTRA, topn=10)\n",
    "\n",
    "        # æ¯ç°‡åŸå¥æ±‡æ€»ï¼ˆç”¨äºæƒ…æ„Ÿï¼‰\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {}\n",
    "        for i, nm in enumerate(theme_names):\n",
    "            p,u,n = sentiment_ratio(groups[i])\n",
    "            sentiments[nm] = {\"positive\": p, \"neutral\": u, \"negative\": n}\n",
    "\n",
    "        # å¯è§†åŒ–\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] èšç±»æ€»è§ˆå›¾ï¼š{img}\")\n",
    "\n",
    "        # å¯¼å‡º CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] ä¸»é¢˜å…³é”®è¯è¡¨ï¼š{kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] ä¸»é¢˜æƒ…æ„Ÿè¡¨ï¼š{sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] æ–‡æ¡£æ•° <20ï¼Œè·³è¿‡èšç±»/æƒ…æ„Ÿã€‚\")\n",
    "\n",
    "    # æ‘˜è¦\n",
    "    print(\"\\nè¯é¢‘TOP10ï¼š\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\nå®Œæˆã€‚è¾“å‡ºç›®å½•ï¼šanalysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:29:26.787319Z",
     "end_time": "2025-09-06T16:32:05.735686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "#èšç±» k=7"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:38:37.925696Z",
     "end_time": "2025-09-06T16:38:37.966462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "å¼¹å¹•æ–‡æœ¬åˆ†æï¼ˆå›ºå®šK=7ç‰ˆæœ¬ï¼‰\n",
      "================================================================\n",
      "[INFO] åˆ†ææ–‡ä»¶ï¼šcleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "[INFO] æ¸…æ´—åè¡Œæ•°ï¼š36165\n",
      "[INFO] åˆ†è¯åæ–‡æ¡£æ•°ï¼š32492 ï¼›æ€»è¯æ•°ï¼š92643\n",
      "[OK] è¯é¢‘è¡¨ï¼šanalysis_results\\word_frequency_202509061639.csv\n",
      "[OK] è¯äº‘ï¼šanalysis_results\\wordcloud_202509061639.png\n",
      "[OK] Top bigramï¼šanalysis_results\\top_bigram_202509061639.csv\n",
      "[OK] èšç±»æ€»è§ˆå›¾ï¼šanalysis_results\\cluster_summary_202509061641.png\n",
      "[OK] ä¸»é¢˜å…³é”®è¯è¡¨ï¼šanalysis_results\\cluster_keywords_202509061639.csv\n",
      "[OK] ä¸»é¢˜æƒ…æ„Ÿè¡¨ï¼šanalysis_results\\cluster_sentiment_202509061639.csv\n",
      "\n",
      "è¯é¢‘TOP10ï¼š\n",
      "1. æ²¡æœ‰  1066\n",
      "2. æ™ºé©¾  1042\n",
      "3. åä¸º  860\n",
      "4. å°ç±³  691\n",
      "5. è¯†åˆ«  651\n",
      "6. çŸ¥é“  602\n",
      "7. å¸æœº  542\n",
      "8. å¼€è½¦  488\n",
      "9. å¯èƒ½  479\n",
      "10. å®‰å…¨  455\n",
      "\n",
      "å®Œæˆã€‚è¾“å‡ºç›®å½•ï¼šanalysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bç«™å¼¹å¹•åˆ†æï¼ˆå›ºå®šK=7ç‰ˆæœ¬ï¼‰ï¼šè¯é¢‘ / è¯äº‘ / èšç±» / æƒ…æ„Ÿ\n",
    "- è¾“å…¥ï¼šcleaned_danmu_results/combined_cleaned_danmu_*.{csv,txt}\n",
    "- è¯æ€§è¿‡æ»¤ + è¡Œä¸šè¯å…¸ + å¼ºåœç”¨\n",
    "- TF-IDF: 1~3gram, min_df=5, max_df=0.5\n",
    "- èšç±»ï¼šKMeans(K=7) å›ºå®šä¸»é¢˜æ•°ï¼Œé¿å…ä¸»é¢˜å¡Œç¼©\n",
    "- å¯è§†åŒ–ï¼šè¯äº‘ã€ä¸»é¢˜åˆ†å¸ƒé¥¼å›¾ã€æƒ…æ„Ÿå †å æŸ± + å…³é”®è¯è¡¨\n",
    "- å¯¼å‡ºï¼šè¯é¢‘ã€ä¸»é¢˜å…³é”®è¯è¡¨ã€ä¸»é¢˜æƒ…æ„Ÿè¡¨ã€Top bigram\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------------- è·¯å¾„ & å‚æ•° ----------------\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"     # å¯ç•™ç©º\n",
    "USER_DICT   = \"user_dict.txt\"        # å¯ç•™ç©ºï¼ˆå»ºè®®æ·»åŠ è¡Œä¸šè¯ï¼‰\n",
    "\n",
    "# å…è®¸çš„è¯æ€§ï¼šåè¯/åŠ¨è¯/è‹±æ–‡/æœ¯è¯­/ç»„ç»‡/äººåç­‰\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# é¢å¤–â€œé€šç”¨å¼±ä¿¡æ¯è¯â€åœç”¨ï¼ˆé˜²ä¸»é¢˜è¢«â€œæ™ºèƒ½/é©¾é©¶/è¾…åŠ©/ç³»ç»Ÿ/åŠŸèƒ½â€ç­‰å¸èµ°ï¼‰\n",
    "GENERIC_WEAK = {\n",
    "    \"æ™ºèƒ½\",\"é©¾é©¶\",\"è¾…åŠ©\",\"ç³»ç»Ÿ\",\"åŠŸèƒ½\",\"è‡ªåŠ¨\",\"è½¦è¾†\",\"è®¾å¤‡\",\"æŠ€æœ¯\",\"æ¨¡å¼\",\"ä¿¡æ¯\",\"ä½“éªŒ\",\"æ„Ÿè§‰\",\"é—®é¢˜\",\"æƒ…å†µ\",\n",
    "    \"æ–¹é¢\",\"è¿›è¡Œ\",\"å®ç°\",\"å¤„ç†\",\"æ¯”è¾ƒ\",\"å¯ä»¥\",\"ä¸ä¼š\",\"ä¸èƒ½\",\"åº”è¯¥\",\"å·²ç»\",\"è¿˜æ˜¯\",\"å°±æ˜¯\",\"ä»€ä¹ˆ\",\"è¿˜æœ‰\",\"ç„¶å\",\"ä½†æ˜¯\",\n",
    "    \"å•Š\",\"å‘€\",\"å‘¢\",\"å§\",\"å“¦\",\"å“ˆ\",\"å“ˆå“ˆ\",\"å“ˆå“ˆå“ˆ\",\"çœŸçš„\",\"è§‰å¾—\",\"æ„Ÿè§‰\",\"æˆ‘ä»¬\",\"ä»–ä»¬\",\"è‡ªå·±\",\"è¿™ä¸ª\",\"é‚£ä¸ª\",\"è¿™æ ·\",\"é‚£æ ·\"\n",
    "}\n",
    "\n",
    "# è¡Œä¸šè¯å…¸å»ºè®®ï¼šæŠŠâ€œè‡ªåŠ¨é©¾é©¶/ç«¯åˆ°ç«¯/FSD/ADS/NOA/æ¿€å…‰é›·è¾¾/Robotaxi/è½¦è·¯ååŒ/æ„ŸçŸ¥/è§„åˆ’/æ§åˆ¶/\n",
    "# è´£ä»»/æ³•å¾‹/äº‹æ•…/ä¿é™©/ç‰¹æ–¯æ‹‰/å°é¹/ç†æƒ³/æ¯”äºšè¿ª/åä¸º/èåœå¿«è·‘/NOH/NCAâ€ç­‰æ”¾åˆ° user_dict.txtï¼Œä¸€è¡Œä¸€ä¸ªè¯ã€‚\n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 5000\n",
    "MIN_DF       = 5\n",
    "MAX_DF       = 0.5\n",
    "NGRAM        = (1,3)\n",
    "\n",
    "# èšç±»ä¸»é¢˜æ•°ï¼ˆå›ºå®šï¼‰\n",
    "K_FIXED      = 7\n",
    "\n",
    "# è¯äº‘\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# çŸ­å¥æƒ…æ„Ÿï¼ˆ<4å­—åˆ¤ä¸­æ€§ï¼‰\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "\n",
    "# ---------------- å­—ä½“ ----------------\n",
    "def setup_chinese_font():\n",
    "    cands = ['SimHei','Microsoft YaHei','SimSun','KaiTi','Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for n in cands:\n",
    "        if any(n in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = n; break\n",
    "    if picked: plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    # for wordcloud\n",
    "    fp = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                fp = f; break\n",
    "    if not fp:\n",
    "        fp = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return fp\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "# ---------------- åŸºç¡€å·¥å…· ----------------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    base |= set(list(\"ï¼Œã€‚ã€ï¼ï¼Ÿï¼›ï¼šâ€â€œâ€˜â€˜ï¼ˆï¼‰()[]ã€ã€‘â€”â€¦- \"))\n",
    "    base |= GENERIC_WEAK\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] å·²åŠ è½½è¡Œä¸šè¯å…¸ï¼š{USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    files = csvs + txts\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        col = \"cleaned\" if \"cleaned\" in df.columns else df.columns[0]\n",
    "        lines = [str(x) for x in df[col].fillna(\"\").tolist()]\n",
    "    else:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    cleaned, seen = [], set()\n",
    "    for s in lines:\n",
    "        s = CTRL_RE.sub(' ', s)\n",
    "        s = EMOJI_RE.sub('', s)\n",
    "        s = re.sub(r'\\s+',' ', s).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "# åˆ†è¯ï¼šè¯æ€§è¿‡æ»¤ + å¼ºåœç”¨\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(WORDCLOUD_TOPN)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('å¼¹å¹•é«˜é¢‘è¯äº‘', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------------- KMeansï¼ˆå›ºå®šK=7ï¼‰ ----------------\n",
    "def kmeans_fixed(docs):\n",
    "    vec = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    k = min(K_FIXED, max(2, X.shape[0] // 30))  # æå°‘æ ·æœ¬æ—¶è‡ªä¿\n",
    "    model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "    return model, vec\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in order[i, :topn*3]:\n",
    "            if j < len(terms):\n",
    "                t = terms[j]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"ä¸»é¢˜{i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # é¥¼å›¾\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt}æ¡ ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"ä¸»é¢˜åˆ†å¸ƒ\", fontsize=15)\n",
    "\n",
    "    # æƒ…æ„ŸæŸ±\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='ç§¯æ')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='ä¸­æ€§')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='æ¶ˆæ')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"æ¯”ä¾‹\"); ax2.set_title(\"æƒ…æ„Ÿåˆ†å¸ƒ\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # å…³é”®è¯è¡¨\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"å…³é”®è¯{i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"ä¸»é¢˜å…³é”®è¯\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# ---------------- ä¸»æµç¨‹ ----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"å¼¹å¹•æ–‡æœ¬åˆ†æï¼ˆå›ºå®šK=7ç‰ˆæœ¬ï¼‰\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_file()\n",
    "    if not latest:\n",
    "        print(\"æœªæ‰¾åˆ°æ¸…æ´—åçš„æ–‡ä»¶ã€‚\"); return\n",
    "    print(f\"[INFO] åˆ†ææ–‡ä»¶ï¼š{latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "    lines = load_cleaned_lines(latest)\n",
    "    print(f\"[INFO] æ¸…æ´—åè¡Œæ•°ï¼š{len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] åˆ†è¯åä¸ºç©ºã€‚\"); return\n",
    "    print(f\"[INFO] åˆ†è¯åæ–‡æ¡£æ•°ï¼š{len(docs)} ï¼›æ€»è¯æ•°ï¼š{len(all_words)}\")\n",
    "\n",
    "    # è¯é¢‘/è¯äº‘/Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter); print(f\"[OK] è¯é¢‘è¡¨ï¼š{freq_csv}\")\n",
    "    wc_path = draw_wordcloud(counter);\n",
    "    if wc_path: print(f\"[OK] è¯äº‘ï¼š{wc_path}\")\n",
    "    bigram = top_ngrams(docs, n=2, topk=50)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigramï¼š{bigram_csv}\")\n",
    "\n",
    "    # å›ºå®šKèšç±»\n",
    "    if len(docs) >= 20:\n",
    "        model, vec = kmeans_fixed(docs)\n",
    "        labels = model.labels_\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, GENERIC_WEAK, topn=10)\n",
    "\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {theme_names[i]: dict(zip([\"positive\",\"neutral\",\"negative\"],\n",
    "                          sentiment_ratio(groups[i]))) for i in range(model.n_clusters)}\n",
    "\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] èšç±»æ€»è§ˆå›¾ï¼š{img}\")\n",
    "\n",
    "        # å¯¼å‡º CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] ä¸»é¢˜å…³é”®è¯è¡¨ï¼š{kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] ä¸»é¢˜æƒ…æ„Ÿè¡¨ï¼š{sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] æ–‡æ¡£æ•° <20ï¼Œè·³è¿‡èšç±»/æƒ…æ„Ÿã€‚\")\n",
    "\n",
    "    # æ‘˜è¦\n",
    "    print(\"\\nè¯é¢‘TOP10ï¼š\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\nå®Œæˆã€‚è¾“å‡ºç›®å½•ï¼šanalysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:38:40.883837Z",
     "end_time": "2025-09-06T16:41:18.853092Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
