{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:18:49.287200Z",
     "end_time": "2025-09-06T16:18:59.172688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "B站弹幕抓取（保存为 CSV，便于后续清洗）\n",
      "============================================================\n",
      "共 1 个分P：\n",
      "- 抓取 2025年了，华为辅助驾驶有什么槽点？ (cid=29546709885) …\n",
      "  2025年了，华为辅助驾驶有什么槽点？ 抓到 1193 条\n",
      "\n",
      "合计抓到 1193 条弹幕\n",
      "已保存：C:\\Users\\Andrew\\Desktop\\homework\\bb\\bili_danmu_results\\combined_raw_BV1GD5DzHEJo_202509061618.csv\n",
      "也写了纯文本：C:\\Users\\Andrew\\Desktop\\homework\\bb\\bili_danmu_results\\combined_raw_BV1GD5DzHEJo_202509061618.txt\n",
      "完成。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "按 BV 抓取“全量弹幕”（segment_index 递增到空），\n",
    "将每个分P保存为 CSV（content/video_title/timestamp），\n",
    "并在 bili_danmu_results/ 目录下再生成合并 CSV/TXT。\n",
    "\n",
    "用法：\n",
    "python crawl_bili_danmu.py\n",
    "粘贴 BV 或 URL，Cookie 可留空。\n",
    "\"\"\"\n",
    "\n",
    "import os, re, time, csv, datetime, sys\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# ---------- 常量 ----------\n",
    "HEADERS_BASE = {\n",
    "    \"origin\": \"https://www.bilibili.com\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36\"\n",
    "}\n",
    "DM_RE = re.compile(rb':(.*?)@', re.S)            # 从 seg.so 中提取弹幕文本\n",
    "BV_RE = re.compile(r'(BV[0-9A-Za-z]{10,})')\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- 工具 ----------\n",
    "def build_session(cookie=None, referer=None):\n",
    "    s = requests.Session()\n",
    "    headers = HEADERS_BASE.copy()\n",
    "    if cookie: headers[\"cookie\"] = cookie\n",
    "    if referer: headers[\"referer\"] = referer\n",
    "    retries = Retry(total=3, backoff_factor=0.6, status_forcelist=[412, 429, 500, 502, 503, 504])\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries, pool_connections=16, pool_maxsize=16))\n",
    "    s.headers.update(headers)\n",
    "    return s\n",
    "\n",
    "def extract_bvid(text: str) -> str:\n",
    "    m = BV_RE.search(text)\n",
    "    if not m:\n",
    "        raise ValueError(\"未找到 BV 号，请检查输入。\")\n",
    "    return m.group(1)\n",
    "\n",
    "def get_pagelist_by_bvid(session, bvid):\n",
    "    \"\"\"用 pagelist 取所有分P：[{cid, page, part, duration}, ...]\"\"\"\n",
    "    url = f\"https://api.bilibili.com/x/player/pagelist?bvid={bvid}\"\n",
    "    r = session.get(url, timeout=10); r.raise_for_status()\n",
    "    j = r.json()\n",
    "    if j.get(\"code\", -1) != 0:\n",
    "        raise RuntimeError(f\"pagelist 接口失败：code={j.get('code')} msg={j.get('message')}\")\n",
    "    pages = j[\"data\"] or []\n",
    "    if not pages:\n",
    "        raise RuntimeError(\"pagelist 返回空，可能 BV 无效或需要登录。\")\n",
    "    return pages\n",
    "\n",
    "def fetch_seg_bytes(session, cid, seg_idx):\n",
    "    url = f\"https://api.bilibili.com/x/v2/dm/web/seg.so?type=1&oid={cid}&segment_index={seg_idx}\"\n",
    "    r = session.get(url, timeout=10); r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def parse_danmu_from_seg(seg_bytes):\n",
    "    # 这里只抽内容；若要更丰富字段需解析 protobuf\n",
    "    return [m.decode('utf-8', errors='ignore') for m in DM_RE.findall(seg_bytes)]\n",
    "\n",
    "# ---------- I/O ----------\n",
    "def save_csv(rows, path):\n",
    "    # rows: list of dicts with keys: content, video_title, timestamp\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"content\", \"video_title\", \"timestamp\"])\n",
    "        w.writeheader()\n",
    "        for r in rows:\n",
    "            w.writerow(r)\n",
    "\n",
    "def save_txt(lines, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line in lines:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# ---------- 主流程 ----------\n",
    "def crawl_all_by_cid(session, cid, video_title, sleep_s=0.05):\n",
    "    all_items = []\n",
    "    seg = 1\n",
    "    empty_runs = 0\n",
    "    while True:\n",
    "        try:\n",
    "            seg_bytes = fetch_seg_bytes(session, cid, seg)\n",
    "        except requests.HTTPError:\n",
    "            time.sleep(1.0)\n",
    "            seg_bytes = fetch_seg_bytes(session, cid, seg)\n",
    "        contents = parse_danmu_from_seg(seg_bytes)\n",
    "        if not contents:\n",
    "            empty_runs += 1\n",
    "            if empty_runs >= 2:\n",
    "                break\n",
    "        else:\n",
    "            empty_runs = 0\n",
    "            for c in contents:\n",
    "                all_items.append({\n",
    "                    \"content\": c,\n",
    "                    \"video_title\": video_title,\n",
    "                    \"timestamp\": \"\"      # seg.so 未解析时间戳，这里留空占位\n",
    "                })\n",
    "        seg += 1\n",
    "        time.sleep(sleep_s)\n",
    "    return all_items\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"B站弹幕抓取（保存为 CSV，便于后续清洗）\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    raw_input_str = input(\"输入 BV 或完整链接：\").strip()\n",
    "    cookie = \"enable_web_push=DISABLE; buvid4=441674D3-73C6-EE18-D7F6-0A9AA6A8764B10929-024061616-tMB8uhs7bNpfYIQVaVKjtQ%3D%3D; DedeUserID=499303036; DedeUserID__ckMd5=7c2e754fb5285a0b; buvid_fp_plain=undefined; enable_feed_channel=ENABLE; hit-dyn-v2=1; fingerprint=d30d288e57446a6e2075d79545bcdece; buvid_fp=d30d288e57446a6e2075d79545bcdece; buvid3=D8E29309-F4DF-1535-6A27-C5ADFB04F3B208505infoc; b_nut=1750090608; _uuid=EE91029AA-C3ED-2D10D-A31B-8610DA6DC1BEE28428infoc; header_theme_version=OPEN; theme-tip-show=SHOWED; theme-avatar-tip-show=SHOWED; rpdid=|(J~R~uR))~u0J'u~lJkJl)Y|; LIVE_BUVID=AUTO2217563800546588; PVID=2; CURRENT_QUALITY=80; bili_ticket=eyJhbGciOiJIUzI1NiIsImtpZCI6InMwMyIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3NTczMDE2NTEsImlhdCI6MTc1NzA0MjM5MSwicGx0IjotMX0.FWLzrbsTzZVO9pAvA2K-j8JpwDO4_00W2sqETwK91NA; bili_ticket_expires=1757301591; SESSDATA=fc10051c%2C1772610457%2C5b79d%2A91CjBHREFbDUZegrrFspSN0CJBiv6HiJF4vOUubDwDGrFJa2-yRA8gOqoqGpCwy29sjfwSVnVwNlBZR1hKb2VFaGI0NkZpcWZMYm5ubE5Kd2JFdkluaFp2Q1JfeFNLX1JqWWxxb19mUENfa193RkFUVDNPa3lZcUU4WmZrZ3ZlLXRpZ3RBRkQ5RnNRIIEC; bili_jct=e685ab8cae35933d20e223cb4b6a6d91; bmg_af_switch=1; bmg_src_def_domain=i2.hdslb.com; bsource=search_google; sid=7l86jgpi; bp_t_offset_499303036=1109422191797075968; b_lsid=D181510CA_1991E1E02C8; home_feed_column=5; browser_resolution=1530-770; CURRENT_FNVAL=2000\"#input(\"可选：Cookie：\").strip() or None\n",
    "\n",
    "    bvid = extract_bvid(raw_input_str)\n",
    "    referer = f\"https://www.bilibili.com/video/{bvid}\"\n",
    "    sess = build_session(cookie=cookie, referer=referer)\n",
    "\n",
    "    pages = get_pagelist_by_bvid(sess, bvid)\n",
    "    print(f\"共 {len(pages)} 个分P：\")\n",
    "\n",
    "    all_rows = []\n",
    "    for p in pages:\n",
    "        cid = p[\"cid\"]\n",
    "        part = p.get(\"part\") or f\"P{p.get('page', '?')}\"\n",
    "        print(f\"- 抓取 {part} (cid={cid}) …\")\n",
    "        rows = crawl_all_by_cid(sess, cid, video_title=f\"{bvid} | {part}\")\n",
    "        print(f\"  {part} 抓到 {len(rows)} 条\")\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "        # 分P各自保存\n",
    "        csv_path = os.path.join(RAW_DIR, f\"{bvid}_{cid}_{part}.csv\")\n",
    "        save_csv(rows, csv_path)\n",
    "\n",
    "    # 合并保存（CSV + TXT）\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "    combined_csv = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.csv\")\n",
    "    combined_txt = os.path.join(RAW_DIR, f\"combined_raw_{bvid}_{ts}.txt\")\n",
    "    save_csv(all_rows, combined_csv)\n",
    "    save_txt([r[\"content\"] for r in all_rows], combined_txt)\n",
    "\n",
    "    print(f\"\\n合计抓到 {len(all_rows)} 条弹幕\")\n",
    "    print(f\"已保存：{combined_csv}\")\n",
    "    print(f\"也写了纯文本：{combined_txt}\")\n",
    "    print(\"完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "B站弹幕批量清洗工具（增强版 v2）\n",
      "==================================================\n",
      "读取: BV17pZfYLEu6_29199436543_小米su7事件，4月1号信息分析🧐.csv（2582 条）\n",
      "读取: BV1Bh3jzxE2q_30840851939_外国车评人来到武汉，感受无人驾驶，悬浮空轨，完全颠覆认知！.csv（3529 条）\n",
      "读取: BV1bN9PYeEBm_28603977518_50多万1548匹小米SU7，智驾水平应该不怎么样吧？.csv（2851 条）\n",
      "读取: BV1bxb1zPEUJ_31257921527_首次体验萝卜快跑无人驾驶出租车.csv（1724 条）\n",
      "读取: BV1D197YCEsW_28692122756_全网唯一最真实对比，特斯拉、华为什么水平？「特斯拉VS华为」.csv（2666 条）\n",
      "读取: BV1f7b6zPESj_31686987287_馆长8.14深圳行④，体验无人驾驶出租车：哎哟喂呀，第一次坐，真是吓死我了好不好？.csv（1962 条）\n",
      "读取: BV1fcXQYUEik_28632024035_全网最全“特斯拉FSD对比华为ADS”，中美智驾有多大差距？【科技狐】.csv（2299 条）\n",
      "读取: BV1fcXQYUEik_28696839809_特斯拉FSD夜间广州郊区到城区一镜到底.csv（8 条）\n",
      "读取: BV1fcXQYUEik_28696905721_特斯拉FSD广州城中村一镜到底.csv（5 条）\n",
      "读取: BV1GD5DzHEJo_29546709885_2025年了，华为辅助驾驶有什么槽点？.csv（1193 条）\n",
      "读取: BV1h5fnYcEyC_28054652505_危险危险危险！车辆“自动驾驶”，司机盖被睡觉？.csv（1608 条）\n",
      "读取: BV1hBuFzfEY5_31011965200_【大虾沉浸式试驾】岚图FREE+👉智能驾驶·底盘·百公里加速全知道！.csv（1943 条）\n",
      "读取: BV1kNZ1YJEPH_29212607774_智能驾驶≠自动驾驶！岚图梦想家“智驾”要如何更安全？.csv（734 条）\n",
      "读取: BV1TRLEzpE6k_29618405918_小米SU7事件，应该教会我们更多   【下尺报告】.csv（4598 条）\n",
      "读取: BV1v7PjegENh_28605743916_一镜到底：特斯拉FSD挑战「智闯纽北」.csv（4489 条）\n",
      "读取: BV1wWfDYiEUi_28111012375_【深度解析】智能驾驶烧了1000亿，发生了什么？.csv（7443 条）\n",
      "读取: BV1WxGwz4EVg_30933650317_video1752033400968.csv（1618 条）\n",
      "读取: BV1Y1JezEELN_30066738154_lv_0_20250520225935.csv（535 条）\n",
      "读取: BV1ZaACeDEXU_28548860733_自掏腰包全网首测，9.98万的比亚迪自动泊车好用吗？「秦L」.csv（1523 条）\n",
      "读取: BV1ZDGYz6Eko_29727722007_长城汽车智能化，如何向前走？.csv（967 条）\n",
      "读取: combined_raw_BV17pZfYLEu6_202509061611.csv（2582 条）\n",
      "读取: combined_raw_BV1Bh3jzxE2q_202509061600.csv（3529 条）\n",
      "读取: combined_raw_BV1bN9PYeEBm_202509061608.csv（2851 条）\n",
      "读取: combined_raw_BV1bxb1zPEUJ_202509061601.csv（1724 条）\n",
      "读取: combined_raw_BV1D197YCEsW_202509061611.csv（2666 条）\n",
      "读取: combined_raw_BV1D197YCEsW_202509061615.csv（2666 条）\n",
      "读取: combined_raw_BV1f7b6zPESj_202509061601.csv（1962 条）\n",
      "读取: combined_raw_BV1fcXQYUEik_202509061612.csv（2312 条）\n",
      "读取: combined_raw_BV1fcXQYUEik_202509061616.csv（2312 条）\n",
      "读取: combined_raw_BV1GD5DzHEJo_202509061618.csv（1193 条）\n",
      "读取: combined_raw_BV1h5fnYcEyC_202509061617.csv（1608 条）\n",
      "读取: combined_raw_BV1hBuFzfEY5_202509061616.csv（1943 条）\n",
      "读取: combined_raw_BV1kNZ1YJEPH_202509061608.csv（734 条）\n",
      "读取: combined_raw_BV1TRLEzpE6k_202509061610.csv（4598 条）\n",
      "读取: combined_raw_BV1v7PjegENh_202509061609.csv（4489 条）\n",
      "读取: combined_raw_BV1wWfDYiEUi_202509061557.csv（7443 条）\n",
      "读取: combined_raw_BV1WxGwz4EVg_202509061617.csv（1618 条）\n",
      "读取: combined_raw_BV1Y1JezEELN_202509061602.csv（535 条）\n",
      "读取: combined_raw_BV1ZaACeDEXU_202509061618.csv（1523 条）\n",
      "读取: combined_raw_BV1ZDGYz6Eko_202509061610.csv（967 条）\n",
      "总计读取 93532 条\n",
      "\n",
      "保留 36167/93532 (38.7%)\n",
      "\n",
      "清洗后保存：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\combined_cleaned_danmu_202509061626.txt\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "移除原因分布： {'noise': 7733, 'duplicate': 49632}\n",
      "移除明细：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\removed_reasons_202509061626.csv\n",
      "调试对照样本：\n",
      "- C:\\Users\\Andrew\\Desktop\\homework\\bb\\cleaned_danmu_results\\debug_original_vs_cleaned_202509061626.csv\n",
      "\n",
      "完成。\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "B站弹幕批量清洗工具（增强版 v2）\n",
    "- 读取 bili_danmu_results/*.csv （字段至少含 content）\n",
    "- “先抽取，再清洗”：优先截取冒号后的正文；剥离前缀噪声块；去控制字符\n",
    "- 输出 cleaned_danmu_results/：\n",
    "    - combined_cleaned_danmu_*.txt / *.csv\n",
    "    - removed_reasons_*.csv （被删除条目与原因）\n",
    "    - debug_original_vs_cleaned_*.csv （抽样对照，便于人工复核）\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "RAW_DIR = os.path.join(os.getcwd(), \"bili_danmu_results\")\n",
    "OUT_DIR = os.path.join(os.getcwd(), \"cleaned_danmu_results\")\n",
    "\n",
    "# ---------- 预编译正则 ----------\n",
    "RE_CTRL = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')                    # 控制字符\n",
    "RE_HTML = re.compile(r'<[^>]+>')\n",
    "RE_URL  = re.compile(r'https?://\\S+')\n",
    "# 只保留：中英文、数字、常见中文标点、连字符\n",
    "RE_KEEP = re.compile(r'[^\\w\\u4e00-\\u9fa5，。！？、；：\"\\'()【】《》·—…\\-]+')\n",
    "RE_SP   = re.compile(r'\\s+')\n",
    "RE_MULTI_PUNCT = re.compile(r'([，。！？…])\\1+')\n",
    "# 行首噪声块（括号/奇怪字节/短hex/孤立字母/标点）反复出现的组合\n",
    "RE_NOISY_PREFIX = re.compile(\n",
    "    r'^\\s*(?:[\\(\\)\\[\\]{}<>\\|\\\\/\\-_=+~^`·•]+|[A-Za-z](?=\\s)|[0-9A-Fa-f]{6,10}|[,.:;，。；：!！?？\\s])+'\n",
    ")\n",
    "\n",
    "def _cut_after_colon(raw: str) -> str:\n",
    "    \"\"\"若含冒号，取最后一个冒号后的片段（更接近正文）\"\"\"\n",
    "    if ':' in raw:\n",
    "        return raw.split(':')[-1]\n",
    "    return raw\n",
    "\n",
    "def _strip_to_first_textual_char(s: str) -> str:\n",
    "    \"\"\"\n",
    "    从开头剥离“噪声块”，直到遇到第一个中文/英文/数字；\n",
    "    先用一个快速前缀清理，再精确定位第一个有效字符。\n",
    "    \"\"\"\n",
    "    s = RE_NOISY_PREFIX.sub('', s)\n",
    "    idx = None\n",
    "    for i, ch in enumerate(s):\n",
    "        if ch.isalnum() or ('\\u4e00' <= ch <= '\\u9fa5'):\n",
    "            idx = i\n",
    "            break\n",
    "    return s[idx:] if idx is not None else ''\n",
    "\n",
    "def _normalize(s: str) -> str:\n",
    "    s = RE_CTRL.sub(' ', s)                    # 删控制字符\n",
    "    s = unicodedata.normalize('NFKC', s)       # 全角→半角\n",
    "    s = RE_HTML.sub('', s)                     # 去HTML\n",
    "    s = RE_URL.sub('', s)                      # 去URL\n",
    "    s = RE_KEEP.sub(' ', s)                    # 过滤到允许字符集\n",
    "    s = RE_MULTI_PUNCT.sub(r'\\1', s)           # 连续标点压缩\n",
    "    s = RE_SP.sub(' ', s).strip()              # 空白规整\n",
    "    return s\n",
    "\n",
    "def _looks_like_noise(s: str) -> bool:\n",
    "    \"\"\"噪声判定：纯数字/单字母/太短/文字占比低\"\"\"\n",
    "    if not s:\n",
    "        return True\n",
    "    if s.isdigit():\n",
    "        return True\n",
    "    if len(s) == 1 and s.isalnum():\n",
    "        return True\n",
    "    if len(s) < 2:\n",
    "        return True\n",
    "    # 文字占比（中英数字）\n",
    "    total = len(s)\n",
    "    keep = sum(1 for ch in s if ch.isalnum() or '\\u4e00' <= ch <= '\\u9fa5')\n",
    "    if keep / max(1, total) < 0.3:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clean_one(raw: str) -> str:\n",
    "    \"\"\"对单条弹幕做‘抽取→去噪→规范化’\"\"\"\n",
    "    s = str(raw or '')\n",
    "    s = _cut_after_colon(s)            # 先截冒号后\n",
    "    s = RE_CTRL.sub(' ', s)            # 去控制符（保证后续定位不被干扰）\n",
    "    s = _strip_to_first_textual_char(s)# 丢弃前缀噪声块\n",
    "    s = _normalize(s)                  # 归一化+过滤\n",
    "    if len(s) > 100:                   # 上限保护\n",
    "        s = s[:100].rstrip()\n",
    "    return s\n",
    "\n",
    "def clean_danmu_content(records):\n",
    "    \"\"\"\n",
    "    输入：字典列表（至少含 'content'）\n",
    "    输出：\n",
    "      - cleaned: [{original, cleaned, video_title, timestamp}]\n",
    "      - removed: [(original, reason)]\n",
    "    \"\"\"\n",
    "    cleaned, removed = [], []\n",
    "\n",
    "    for dm in records:\n",
    "        raw = str(dm.get('content', '') or '')\n",
    "        if not raw:\n",
    "            removed.append((raw, \"empty\"))\n",
    "            continue\n",
    "\n",
    "        out = clean_one(raw)\n",
    "\n",
    "        if _looks_like_noise(out):\n",
    "            removed.append((raw, \"noise\"))\n",
    "            continue\n",
    "\n",
    "        cleaned.append({\n",
    "            \"original\": raw,\n",
    "            \"cleaned\": out,\n",
    "            \"video_title\": dm.get(\"video_title\", \"未知视频\"),\n",
    "            \"timestamp\": dm.get(\"timestamp\", \"\")\n",
    "        })\n",
    "\n",
    "    # 去重（以 cleaned 为键）\n",
    "    seen, dedup = set(), []\n",
    "    for row in cleaned:\n",
    "        key = row['cleaned']\n",
    "        if key in seen:\n",
    "            removed.append((row['original'], \"duplicate\"))\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        dedup.append(row)\n",
    "\n",
    "    return dedup, removed\n",
    "\n",
    "# ---------- 文件 I/O ----------\n",
    "def find_csv_files(directory):\n",
    "    files = glob.glob(os.path.join(directory, '*.csv'))\n",
    "    return [f for f in files\n",
    "            if '_cleaned.csv' not in os.path.basename(f)\n",
    "            and not os.path.basename(f).startswith('combined_cleaned_')]\n",
    "\n",
    "def read_all_records(paths):\n",
    "    allrows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            for col in [\"content\", \"video_title\", \"timestamp\"]:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = \"\"\n",
    "            df = df[[\"content\", \"video_title\", \"timestamp\"]]\n",
    "            print(f\"读取: {os.path.basename(p)}（{len(df)} 条）\")\n",
    "            allrows.extend(df.to_dict(\"records\"))\n",
    "        except Exception as e:\n",
    "            print(f\"读取失败: {p} - {e}\")\n",
    "    print(f\"总计读取 {len(allrows)} 条\")\n",
    "    return allrows\n",
    "\n",
    "def save_outputs(cleaned, removed):\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    ts = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "    # 主输出\n",
    "    txt_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.txt\")\n",
    "    csv_path = os.path.join(OUT_DIR, f\"combined_cleaned_danmu_{ts}.csv\")\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for r in cleaned:\n",
    "            f.write(r[\"cleaned\"] + \"\\n\")\n",
    "    pd.DataFrame(cleaned).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"\\n清洗后保存：\\n- {txt_path}\\n- {csv_path}\")\n",
    "\n",
    "    # 被移除条目统计\n",
    "    if removed:\n",
    "        rm_path = os.path.join(OUT_DIR, f\"removed_reasons_{ts}.csv\")\n",
    "        pd.DataFrame(removed, columns=[\"original\", \"reason\"]).to_csv(\n",
    "            rm_path, index=False, encoding=\"utf-8-sig\")\n",
    "        cnt = Counter([r for _, r in removed])\n",
    "        print(\"移除原因分布：\", dict(cnt))\n",
    "        print(f\"移除明细：\\n- {rm_path}\")\n",
    "\n",
    "    # 调试对照（随机抽样或前N条）\n",
    "    dbg_path = os.path.join(OUT_DIR, f\"debug_original_vs_cleaned_{ts}.csv\")\n",
    "    dbg_df = pd.DataFrame(cleaned)[[\"original\", \"cleaned\"]].head(200)\n",
    "    dbg_df.to_csv(dbg_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"调试对照样本：\\n- {dbg_path}\")\n",
    "\n",
    "# ---------- 主流程 ----------\n",
    "def main():\n",
    "    print(\"=\"*50)\n",
    "    print(\"B站弹幕批量清洗工具（增强版 v2）\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    paths = find_csv_files(RAW_DIR)\n",
    "    if not paths:\n",
    "        print(f\"未在 {RAW_DIR} 找到待处理 CSV\"); return\n",
    "\n",
    "    records = read_all_records(paths)\n",
    "    if not records:\n",
    "        print(\"没有可处理数据\"); return\n",
    "\n",
    "    cleaned, removed = clean_danmu_content(records)\n",
    "    print(f\"\\n保留 {len(cleaned)}/{len(records)} \"\n",
    "          f\"({len(cleaned)/max(1,len(records))*100:.1f}%)\")\n",
    "\n",
    "    save_outputs(cleaned, removed)\n",
    "    print(\"\\n完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:26:09.063372Z",
     "end_time": "2025-09-06T16:26:11.237332Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "弹幕文本分析 Pro（词频/词云/自适应聚类/情感）\n",
      "================================================================\n",
      "[INFO] 分析文件：cleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "[INFO] 清洗后行数：36165\n",
      "[INFO] 分词后文档数：32655 ；总词数：97466\n",
      "[OK] 词频表：analysis_results\\word_frequency_202509061630.csv\n",
      "[OK] 词云：analysis_results\\wordcloud_202509061630.png\n",
      "[OK] Top bigram：analysis_results\\top_bigram_202509061630.csv\n",
      "[INFO] 开始自适应聚类 …\n",
      "[INFO] 轮廓系数最优：K=8，score=0.0172\n",
      "[OK] 聚类总览图：analysis_results\\cluster_summary_202509061632.png\n",
      "[OK] 主题关键词表：analysis_results\\cluster_keywords_202509061630.csv\n",
      "[OK] 主题情感表：analysis_results\\cluster_sentiment_202509061630.csv\n",
      "\n",
      "词频TOP10：\n",
      "1. 驾驶  1849\n",
      "2. 自动  1241\n",
      "3. 没有  1066\n",
      "4. 智驾  1042\n",
      "5. 问题  975\n",
      "6. 华为  860\n",
      "7. 小米  691\n",
      "8. 识别  651\n",
      "9. 辅助  553\n",
      "10. 司机  542\n",
      "\n",
      "完成。输出目录：analysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "B站弹幕分析 Pro：词频 / 词云 / 自适应聚类 / 情感，适配 cleaned_danmu_results 输出\n",
    "- 自动读取最新 cleaned 文件（csv 优先，其次 txt）\n",
    "- 分词：词性过滤 + 行业词典（可选）\n",
    "- 向量化：1-2gram TF-IDF、min_df/max_df 抑制口水词\n",
    "- 自适应 KMeans（轮廓系数选 K）\n",
    "- 主题命名：用非停用词关键词前2个\n",
    "- 情感：SnowNLP（<4字视为中性），按主题汇总\n",
    "- 可视化：词云、主题分布饼图 + 情感堆叠柱状 + 关键词表\n",
    "- 额外导出：词频、主题关键词表、主题情感表、Top n-gram\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ===================== 可调参数 =====================\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"      # 通用停用词，如无可为空\n",
    "USER_DICT   = \"user_dict.txt\"         # 行业词典（可选）\n",
    "\n",
    "# 词性保留：名词/动词/英文/术语/组织/人名等\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# 额外口语停用词（弹幕高频“无信息”词）\n",
    "DANMU_STOP_EXTRA = {\n",
    "    \"这个\",\"那个\",\"就是\",\"什么\",\"还有\",\"然后\",\"但是\",\"所以\",\"还是\",\"已经\",\"真的\",\"感觉\",\"觉得\",\"知道\",\n",
    "    \"可以\",\"不可以\",\"不会\",\"不能\",\"应该\",\"可能\",\"还是\",\"有点\",\"有些\",\"怎么\",\"为啥\",\"为什么\",\n",
    "    \"啊\",\"呀\",\"呢\",\"吧\",\"哦\",\"哇\",\"诶\",\"嘛\",\"哈\",\"哈哈\",\"哈哈哈\",\"emm\",\"嗯\",\"啊啊\",\"呜呜\",\n",
    "    \"视频\",\"弹幕\",\"现在\",\"今天\",\"昨天\",\"明天\",\"这里\",\"那里\",\"这样\",\"那样\",\"很多\",\"非常\"\n",
    "}\n",
    "\n",
    "# TF-IDF 参数\n",
    "MAX_FEATURES = 4000\n",
    "MIN_DF       = 5       # 出现不到 MIN_DF 个文档的特征丢弃\n",
    "MAX_DF       = 0.6     # 出现在 >60% 文档的特征丢弃\n",
    "NGRAM        = (1,2)\n",
    "\n",
    "# 自适应 K 范围\n",
    "K_MIN, K_MAX = 2, 8\n",
    "\n",
    "# 词云 TopN\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# 小句情感处理（长度<4判中性）\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "# ====================================================\n",
    "\n",
    "\n",
    "# ---------------- 字体设置 ----------------\n",
    "def setup_chinese_font():\n",
    "    candidates = ['SimHei','Microsoft YaHei','SimSun','KaiTi',\n",
    "                  'Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for name in candidates:\n",
    "        if any(name in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = name; break\n",
    "    if picked:\n",
    "        plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    font_path = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                font_path = f; break\n",
    "    if not font_path:\n",
    "        font_path = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return font_path\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "\n",
    "# ---------------- 工具函数 ----------------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    # 常见符号 & 弹幕口语补充\n",
    "    base |= set(list(\"，。、！？；：”“‘‘（）()[]【】—…- \"))\n",
    "    base |= DANMU_STOP_EXTRA\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] 已加载行业词典：{USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    files = csvs + txts\n",
    "    if not files: return None\n",
    "    return max(files, key=os.path.getmtime)\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        col = \"cleaned\" if \"cleaned\" in df.columns else df.columns[0]\n",
    "        lines = [str(x) for x in df[col].fillna(\"\").tolist()]\n",
    "    else:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    # 轻量去噪 + 去重\n",
    "    cleaned, seen = [], set()\n",
    "    for s in lines:\n",
    "        s = CTRL_RE.sub(' ', s)\n",
    "        s = EMOJI_RE.sub('', s)\n",
    "        s = re.sub(r'\\s+',' ', s).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:        # 允许短句，但太短就跳过\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)     # 保留原句用于情感\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter, top_n=WORDCLOUD_TOPN):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(top_n)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('弹幕高频词云', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "# --- n-gram 辅助（统计 top bigram） ---\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------------- 聚类（自适应K） ----------------\n",
    "def auto_kmeans(docs, min_k=K_MIN, max_k=K_MAX, max_features=MAX_FEATURES):\n",
    "    vec = TfidfVectorizer(max_features=max_features, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    # 少样本：固定较小K\n",
    "    if n < 60:\n",
    "        k = max(2, min(4, n // 15 or 2))\n",
    "        model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        return model, vec, None\n",
    "\n",
    "    # 轮廓系数选择 K（抽样评估）\n",
    "    if n > 6000:\n",
    "        idx = np.random.RandomState(42).choice(n, 6000, replace=False)\n",
    "        X_eval = X[idx]\n",
    "    else:\n",
    "        X_eval = X\n",
    "\n",
    "    best_k, best_score, best_model = None, -1, None\n",
    "    for k in range(min_k, max_k+1):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "        labels_eval = km.labels_ if X_eval is X else km.predict(X_eval)\n",
    "        score = silhouette_score(X_eval, labels_eval, metric='cosine')\n",
    "        if score > best_score:\n",
    "            best_k, best_score, best_model = k, score, km\n",
    "    return best_model, vec, best_score\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in order[i, :topn*3]:  # 多取再过滤\n",
    "            if j < len(terms):\n",
    "                t = terms[j]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"主题{i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # 饼图：主题分布\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt}条 ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"主题分布\", fontsize=15)\n",
    "\n",
    "    # 柱状：情感堆叠（绿/蓝/红）\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='积极')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='中性')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='消极')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"比例\"); ax2.set_title(\"情感分布\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # 主题关键词表\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"关键词{i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    # 补齐不等长\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"主题关键词\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------- 主流程 ----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"弹幕文本分析 Pro（词频/词云/自适应聚类/情感）\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_file()\n",
    "    if not latest:\n",
    "        print(\"未找到清洗后的文件，请先运行清洗。\"); return\n",
    "    print(f\"[INFO] 分析文件：{latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "    lines = load_cleaned_lines(latest)\n",
    "    print(f\"[INFO] 清洗后行数：{len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] 分词后为空（可能停用词过多或文本太短）。\"); return\n",
    "    print(f\"[INFO] 分词后文档数：{len(docs)} ；总词数：{len(all_words)}\")\n",
    "\n",
    "    # 词频、词云、Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter)\n",
    "    print(f\"[OK] 词频表：{freq_csv}\")\n",
    "\n",
    "    wc_path = draw_wordcloud(counter)\n",
    "    if wc_path: print(f\"[OK] 词云：{wc_path}\")\n",
    "\n",
    "    bigram_top = top_ngrams(docs, n=2, topk=50)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram_top, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigram：{bigram_csv}\")\n",
    "\n",
    "    # 聚类\n",
    "    if len(docs) >= 20:\n",
    "        print(\"[INFO] 开始自适应聚类 …\")\n",
    "        model, vec, s_score = auto_kmeans(docs)\n",
    "        if s_score is not None:\n",
    "            print(f\"[INFO] 轮廓系数最优：K={model.n_clusters}，score={s_score:.4f}\")\n",
    "        labels = model.labels_\n",
    "\n",
    "        # 主题关键词与命名\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, DANMU_STOP_EXTRA, topn=10)\n",
    "\n",
    "        # 每簇原句汇总（用于情感）\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {}\n",
    "        for i, nm in enumerate(theme_names):\n",
    "            p,u,n = sentiment_ratio(groups[i])\n",
    "            sentiments[nm] = {\"positive\": p, \"neutral\": u, \"negative\": n}\n",
    "\n",
    "        # 可视化\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] 聚类总览图：{img}\")\n",
    "\n",
    "        # 导出 CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] 主题关键词表：{kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] 主题情感表：{sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] 文档数 <20，跳过聚类/情感。\")\n",
    "\n",
    "    # 摘要\n",
    "    print(\"\\n词频TOP10：\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\n完成。输出目录：analysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:29:26.787319Z",
     "end_time": "2025-09-06T16:32:05.735686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "#聚类 k=7"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:38:37.925696Z",
     "end_time": "2025-09-06T16:38:37.966462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "弹幕文本分析（固定K=7版本）\n",
      "================================================================\n",
      "[INFO] 分析文件：cleaned_danmu_results\\combined_cleaned_danmu_202509061626.csv\n",
      "[INFO] 清洗后行数：36165\n",
      "[INFO] 分词后文档数：32492 ；总词数：92643\n",
      "[OK] 词频表：analysis_results\\word_frequency_202509061639.csv\n",
      "[OK] 词云：analysis_results\\wordcloud_202509061639.png\n",
      "[OK] Top bigram：analysis_results\\top_bigram_202509061639.csv\n",
      "[OK] 聚类总览图：analysis_results\\cluster_summary_202509061641.png\n",
      "[OK] 主题关键词表：analysis_results\\cluster_keywords_202509061639.csv\n",
      "[OK] 主题情感表：analysis_results\\cluster_sentiment_202509061639.csv\n",
      "\n",
      "词频TOP10：\n",
      "1. 没有  1066\n",
      "2. 智驾  1042\n",
      "3. 华为  860\n",
      "4. 小米  691\n",
      "5. 识别  651\n",
      "6. 知道  602\n",
      "7. 司机  542\n",
      "8. 开车  488\n",
      "9. 可能  479\n",
      "10. 安全  455\n",
      "\n",
      "完成。输出目录：analysis_results/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "B站弹幕分析（固定K=7版本）：词频 / 词云 / 聚类 / 情感\n",
    "- 输入：cleaned_danmu_results/combined_cleaned_danmu_*.{csv,txt}\n",
    "- 词性过滤 + 行业词典 + 强停用\n",
    "- TF-IDF: 1~3gram, min_df=5, max_df=0.5\n",
    "- 聚类：KMeans(K=7) 固定主题数，避免主题塌缩\n",
    "- 可视化：词云、主题分布饼图、情感堆叠柱 + 关键词表\n",
    "- 导出：词频、主题关键词表、主题情感表、Top bigram\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------------- 路径 & 参数 ----------------\n",
    "CLEANED_DIR = \"cleaned_danmu_results\"\n",
    "OUT_DIR     = \"analysis_results\"\n",
    "STOP_PATH   = \"cn_stopwords.txt\"     # 可留空\n",
    "USER_DICT   = \"user_dict.txt\"        # 可留空（建议添加行业词）\n",
    "\n",
    "# 允许的词性：名词/动词/英文/术语/组织/人名等\n",
    "KEEP_POS = {\"n\",\"nr\",\"ns\",\"nt\",\"nz\",\"vn\",\"v\",\"eng\",\"nw\",\"an\",\"i\",\"j\",\"ni\",\"nl\",\"ng\"}\n",
    "\n",
    "# 额外“通用弱信息词”停用（防主题被“智能/驾驶/辅助/系统/功能”等吸走）\n",
    "GENERIC_WEAK = {\n",
    "    \"智能\",\"驾驶\",\"辅助\",\"系统\",\"功能\",\"自动\",\"车辆\",\"设备\",\"技术\",\"模式\",\"信息\",\"体验\",\"感觉\",\"问题\",\"情况\",\n",
    "    \"方面\",\"进行\",\"实现\",\"处理\",\"比较\",\"可以\",\"不会\",\"不能\",\"应该\",\"已经\",\"还是\",\"就是\",\"什么\",\"还有\",\"然后\",\"但是\",\n",
    "    \"啊\",\"呀\",\"呢\",\"吧\",\"哦\",\"哈\",\"哈哈\",\"哈哈哈\",\"真的\",\"觉得\",\"感觉\",\"我们\",\"他们\",\"自己\",\"这个\",\"那个\",\"这样\",\"那样\"\n",
    "}\n",
    "\n",
    "# 行业词典建议：把“自动驾驶/端到端/FSD/ADS/NOA/激光雷达/Robotaxi/车路协同/感知/规划/控制/\n",
    "# 责任/法律/事故/保险/特斯拉/小鹏/理想/比亚迪/华为/萝卜快跑/NOH/NCA”等放到 user_dict.txt，一行一个词。\n",
    "\n",
    "# TF-IDF\n",
    "MAX_FEATURES = 5000\n",
    "MIN_DF       = 5\n",
    "MAX_DF       = 0.5\n",
    "NGRAM        = (1,3)\n",
    "\n",
    "# 聚类主题数（固定）\n",
    "K_FIXED      = 7\n",
    "\n",
    "# 词云\n",
    "WORDCLOUD_TOPN = 150\n",
    "\n",
    "# 短句情感（<4字判中性）\n",
    "SHORT_NEUTRAL_LEN = 4\n",
    "\n",
    "# ---------------- 字体 ----------------\n",
    "def setup_chinese_font():\n",
    "    cands = ['SimHei','Microsoft YaHei','SimSun','KaiTi','Noto Sans CJK SC','Source Han Sans SC','Arial Unicode MS']\n",
    "    picked = None\n",
    "    for n in cands:\n",
    "        if any(n in f.name for f in fm.fontManager.ttflist):\n",
    "            picked = n; break\n",
    "    if picked: plt.rcParams['font.family'] = picked\n",
    "    plt.rcParams['axes.unicode_minus'] = False\n",
    "    # for wordcloud\n",
    "    fp = None\n",
    "    if picked:\n",
    "        for f in fm.findSystemFonts():\n",
    "            if picked.lower() in os.path.basename(f).lower():\n",
    "                fp = f; break\n",
    "    if not fp:\n",
    "        fp = fm.findfont(fm.FontProperties(family='sans-serif'))\n",
    "    return fp\n",
    "\n",
    "WC_FONT = setup_chinese_font()\n",
    "\n",
    "# ---------------- 基础工具 ----------------\n",
    "CTRL_RE  = re.compile(r'[\\x00-\\x1F\\x7F-\\x9F]+')\n",
    "EMOJI_RE = re.compile(r'[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "\n",
    "def load_stopwords(path=STOP_PATH):\n",
    "    base = set()\n",
    "    if path and os.path.exists(path):\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            base = {x.strip() for x in f if x.strip()}\n",
    "    base |= set(list(\"，。、！？；：”“‘‘（）()[]【】—…- \"))\n",
    "    base |= GENERIC_WEAK\n",
    "    return base\n",
    "\n",
    "def maybe_load_user_dict():\n",
    "    if USER_DICT and os.path.exists(USER_DICT):\n",
    "        jieba.load_userdict(USER_DICT)\n",
    "        print(f\"[INFO] 已加载行业词典：{USER_DICT}\")\n",
    "\n",
    "def find_latest_cleaned_file():\n",
    "    csvs = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.csv\"))\n",
    "    txts = glob.glob(os.path.join(CLEANED_DIR, \"combined_cleaned_danmu_*.txt\"))\n",
    "    files = csvs + txts\n",
    "    return max(files, key=os.path.getmtime) if files else None\n",
    "\n",
    "def load_cleaned_lines(path):\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "        col = \"cleaned\" if \"cleaned\" in df.columns else df.columns[0]\n",
    "        lines = [str(x) for x in df[col].fillna(\"\").tolist()]\n",
    "    else:\n",
    "        with open(path,'r',encoding='utf-8') as f:\n",
    "            lines = [line.strip() for line in f]\n",
    "    cleaned, seen = [], set()\n",
    "    for s in lines:\n",
    "        s = CTRL_RE.sub(' ', s)\n",
    "        s = EMOJI_RE.sub('', s)\n",
    "        s = re.sub(r'\\s+',' ', s).strip()\n",
    "        if s and s not in seen:\n",
    "            seen.add(s); cleaned.append(s)\n",
    "    return cleaned\n",
    "\n",
    "# 分词：词性过滤 + 强停用\n",
    "def segment_docs(lines, stop):\n",
    "    all_words, docs, kept_lines = [], [], []\n",
    "    for s in lines:\n",
    "        if len(s) < 2:\n",
    "            continue\n",
    "        tokens = []\n",
    "        for w, flag in pseg.cut(s):\n",
    "            if (w not in stop) and (len(w) > 1) and (flag in KEEP_POS):\n",
    "                tokens.append(w)\n",
    "        if tokens:\n",
    "            all_words.extend(tokens)\n",
    "            docs.append(\" \".join(tokens))\n",
    "            kept_lines.append(s)\n",
    "    return all_words, docs, kept_lines\n",
    "\n",
    "def save_wordfreq(counter):\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"word_frequency_{ts}.csv\")\n",
    "    with open(out,'w',encoding='utf-8') as fw:\n",
    "        fw.write(\"rank,word,freq\\n\")\n",
    "        for i,(w,f) in enumerate(counter.most_common(),1):\n",
    "            fw.write(f\"{i},{w},{f}\\n\")\n",
    "    return out\n",
    "\n",
    "def draw_wordcloud(counter):\n",
    "    if not counter: return None\n",
    "    wc = WordCloud(width=1200, height=700, background_color='white', font_path=WC_FONT)\n",
    "    img = wc.generate_from_frequencies(dict(counter.most_common(WORDCLOUD_TOPN)))\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.imshow(img, interpolation='bilinear'); plt.axis('off'); plt.title('弹幕高频词云', fontsize=16)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"wordcloud_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close()\n",
    "    return out\n",
    "\n",
    "def top_ngrams(docs, n=2, topk=50):\n",
    "    c = Counter()\n",
    "    for d in docs:\n",
    "        toks = d.split()\n",
    "        for i in range(len(toks)-n+1):\n",
    "            c[\" \".join(toks[i:i+n])] += 1\n",
    "    return c.most_common(topk)\n",
    "\n",
    "# ---------------- KMeans（固定K=7） ----------------\n",
    "def kmeans_fixed(docs):\n",
    "    vec = TfidfVectorizer(max_features=MAX_FEATURES, ngram_range=NGRAM,\n",
    "                          min_df=MIN_DF, max_df=MAX_DF)\n",
    "    X = vec.fit_transform(docs)\n",
    "    k = min(K_FIXED, max(2, X.shape[0] // 30))  # 极少样本时自保\n",
    "    model = KMeans(n_clusters=k, n_init=10, random_state=42).fit(X)\n",
    "    return model, vec\n",
    "\n",
    "def extract_cluster_keywords(model, vectorizer, stop_extra, topn=10):\n",
    "    terms = (vectorizer.get_feature_names_out()\n",
    "             if hasattr(vectorizer, \"get_feature_names_out\")\n",
    "             else vectorizer.get_feature_names())\n",
    "    order = model.cluster_centers_.argsort()[:, ::-1]\n",
    "    keywords, names = [], []\n",
    "    for i in range(model.n_clusters):\n",
    "        keys = []\n",
    "        for j in order[i, :topn*3]:\n",
    "            if j < len(terms):\n",
    "                t = terms[j]\n",
    "                if all(tok not in stop_extra for tok in t.split()):\n",
    "                    keys.append(t)\n",
    "            if len(keys) >= topn:\n",
    "                break\n",
    "        keywords.append(keys[:topn])\n",
    "        names.append(\" \".join(keys[:2]) if keys else f\"主题{i+1}\")\n",
    "    return names, keywords\n",
    "\n",
    "def sentiment_ratio(texts):\n",
    "    if not texts: return (0,0,0)\n",
    "    pos = neu = neg = 0\n",
    "    for t in texts:\n",
    "        t = t.strip()\n",
    "        if len(t) < SHORT_NEUTRAL_LEN:\n",
    "            neu += 1; continue\n",
    "        s = SnowNLP(t).sentiments\n",
    "        if s > 0.6: pos += 1\n",
    "        elif s < 0.4: neg += 1\n",
    "        else: neu += 1\n",
    "    total = len(texts)\n",
    "    return pos/total, neu/total, neg/total\n",
    "\n",
    "def visualize_clusters(theme_names, theme_counts, sentiments, key_table):\n",
    "    if not theme_names: return None\n",
    "    fig = plt.figure(figsize=(16,12))\n",
    "    gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # 饼图\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    wedges, _, _ = ax1.pie(theme_counts, labels=None, autopct='%1.1f%%',\n",
    "                           startangle=90, pctdistance=0.8, labeldistance=1.4)\n",
    "    legend_labels = [f\"{nm}: {cnt}条 ({cnt/sum(theme_counts)*100:.1f}%)\"\n",
    "                     for nm, cnt in zip(theme_names, theme_counts)]\n",
    "    ax1.legend(legend_labels, loc='center left', bbox_to_anchor=(-0.32, 0))\n",
    "    ax1.set_title(\"主题分布\", fontsize=15)\n",
    "\n",
    "    # 情感柱\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    idx = np.arange(len(theme_names))\n",
    "    pos = [sentiments[n]['positive'] for n in theme_names]\n",
    "    neu = [sentiments[n]['neutral'] for n in theme_names]\n",
    "    neg = [sentiments[n]['negative'] for n in theme_names]\n",
    "    barw = 0.65\n",
    "    ax2.bar(idx, pos, width=barw, color='#4CAF50', label='积极')\n",
    "    ax2.bar(idx, neu, width=barw, bottom=pos, color='#2196F3', label='中性')\n",
    "    ax2.bar(idx, neg, width=barw, bottom=[i+j for i,j in zip(pos,neu)],\n",
    "            color='#F44336', label='消极')\n",
    "    ax2.set_xticks(idx)\n",
    "    ax2.set_xticklabels(theme_names, rotation=45, ha='right')\n",
    "    ax2.set_ylim(0,1)\n",
    "    ax2.set_ylabel(\"比例\"); ax2.set_title(\"情感分布\", fontsize=15)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.15,1))\n",
    "\n",
    "    # 关键词表\n",
    "    ax3 = fig.add_subplot(gs[1,:]); ax3.axis('off')\n",
    "    cols = [f\"关键词{i+1}\" for i in range(max(len(r) for r in key_table) if key_table else 10)]\n",
    "    data = [row + [\"\"]*(len(cols)-len(row)) for row in key_table]\n",
    "    table = ax3.table(cellText=data, rowLabels=theme_names, colLabels=cols, loc='center')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(10); table.scale(1,1.5)\n",
    "    ax3.set_title(\"主题关键词\", fontsize=15, y=0.98)\n",
    "\n",
    "    plt.subplots_adjust(top=0.93, bottom=0.08, left=0.08, right=0.95,\n",
    "                        hspace=0.6, wspace=0.35)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    out = os.path.join(OUT_DIR, f\"cluster_summary_{ts}.png\")\n",
    "    plt.savefig(out, dpi=300, bbox_inches='tight'); plt.close(fig)\n",
    "    return out\n",
    "\n",
    "# ---------------- 主流程 ----------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "    print(\"=\"*64)\n",
    "    print(\"弹幕文本分析（固定K=7版本）\")\n",
    "    print(\"=\"*64)\n",
    "\n",
    "    latest = find_latest_cleaned_file()\n",
    "    if not latest:\n",
    "        print(\"未找到清洗后的文件。\"); return\n",
    "    print(f\"[INFO] 分析文件：{latest}\")\n",
    "\n",
    "    maybe_load_user_dict()\n",
    "    stop = load_stopwords(STOP_PATH)\n",
    "    lines = load_cleaned_lines(latest)\n",
    "    print(f\"[INFO] 清洗后行数：{len(lines)}\")\n",
    "\n",
    "    jieba.initialize()\n",
    "    all_words, docs, kept_lines = segment_docs(lines, stop)\n",
    "    if not docs:\n",
    "        print(\"[WARN] 分词后为空。\"); return\n",
    "    print(f\"[INFO] 分词后文档数：{len(docs)} ；总词数：{len(all_words)}\")\n",
    "\n",
    "    # 词频/词云/Top bigram\n",
    "    counter = Counter(all_words)\n",
    "    freq_csv = save_wordfreq(counter); print(f\"[OK] 词频表：{freq_csv}\")\n",
    "    wc_path = draw_wordcloud(counter);\n",
    "    if wc_path: print(f\"[OK] 词云：{wc_path}\")\n",
    "    bigram = top_ngrams(docs, n=2, topk=50)\n",
    "    ts = time.strftime(\"%Y%m%d%H%M\")\n",
    "    bigram_csv = os.path.join(OUT_DIR, f\"top_bigram_{ts}.csv\")\n",
    "    pd.DataFrame(bigram, columns=[\"bigram\",\"count\"]).to_csv(bigram_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"[OK] Top bigram：{bigram_csv}\")\n",
    "\n",
    "    # 固定K聚类\n",
    "    if len(docs) >= 20:\n",
    "        model, vec = kmeans_fixed(docs)\n",
    "        labels = model.labels_\n",
    "        theme_names, key_table = extract_cluster_keywords(model, vec, GENERIC_WEAK, topn=10)\n",
    "\n",
    "        groups = {i: [] for i in range(model.n_clusters)}\n",
    "        for i, lbl in enumerate(labels):\n",
    "            groups[lbl].append(kept_lines[i])\n",
    "\n",
    "        theme_counts = [len(groups[i]) for i in range(model.n_clusters)]\n",
    "        sentiments = {theme_names[i]: dict(zip([\"positive\",\"neutral\",\"negative\"],\n",
    "                          sentiment_ratio(groups[i]))) for i in range(model.n_clusters)}\n",
    "\n",
    "        img = visualize_clusters(theme_names, theme_counts, sentiments, key_table)\n",
    "        if img: print(f\"[OK] 聚类总览图：{img}\")\n",
    "\n",
    "        # 导出 CSV\n",
    "        kw_csv = os.path.join(OUT_DIR, f\"cluster_keywords_{ts}.csv\")\n",
    "        pd.DataFrame({\"theme\": theme_names, **{f\"kw{i+1}\":[row[i] if i<len(row) else \"\" for row in key_table] for i in range(10)}})\\\n",
    "          .to_csv(kw_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] 主题关键词表：{kw_csv}\")\n",
    "\n",
    "        sent_csv = os.path.join(OUT_DIR, f\"cluster_sentiment_{ts}.csv\")\n",
    "        pd.DataFrame([{\"theme\": nm, **sentiments[nm], \"count\": cnt, \"percentage\": cnt/len(docs)}\n",
    "                      for nm, cnt in zip(theme_names, theme_counts)])\\\n",
    "          .to_csv(sent_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"[OK] 主题情感表：{sent_csv}\")\n",
    "    else:\n",
    "        print(\"[WARN] 文档数 <20，跳过聚类/情感。\")\n",
    "\n",
    "    # 摘要\n",
    "    print(\"\\n词频TOP10：\")\n",
    "    for i,(w,f) in enumerate(counter.most_common(10),1):\n",
    "        print(f\"{i}. {w}  {f}\")\n",
    "    print(\"\\n完成。输出目录：analysis_results/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-06T16:38:40.883837Z",
     "end_time": "2025-09-06T16:41:18.853092Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
